{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "7bd19c094dc44574d64e00d65102e6ba36e1076719a93266f721ddb81f822a22"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Extra lab 1 - Basics of Autograd in Python\n",
    "\n",
    "In this extra laboratory, we will learn a few concepts about autograd in PyTorch and how to build modules with a custom backward pass."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Building a computational graph and obtaining derivatives\n",
    "\n",
    "A computational graph in PT is automatically constructed by just applying some operations on one or more PT tensors.\n",
    "\n",
    "Let us reproduce the example from prof. Manzoni's lecture:\n",
    "\n",
    "![](img/compgraph.png)\n",
    "\n",
    "Input (leaf) tensors are indicated in yellow circles, gray circles indicate intermediate tensors, blue circles output tensors, operations are shown in black squares."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "source": [
    "We construct the tensor by specifying `requires_grad=True` in the constructor. If we don't do it, the gradient information won't be retained for the specific leaf tensor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.tensor([5.0], requires_grad=True)\n",
    "x_2 = torch.tensor([-2.1], requires_grad=True)\n",
    "print(x_1)\n",
    "print(x_2)"
   ]
  },
  {
   "source": [
    "By obtaining and printing `a`, we can see that the tensor has a specific gradient function attached."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = x_1 * x_2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = x_1.cos()\n",
    "y_1 = a + b\n",
    "print(y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = x_1.log()\n",
    "e = x_2 ** 3\n",
    "g = d + e\n",
    "y_2 = g - x_2\n",
    "print(y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.stack((y_1, y_2))\n",
    "print(f)"
   ]
  },
  {
   "source": [
    "We ask Python to calculate the gradient with the `backward` method.\n",
    "\n",
    "Note: `backward()` may be called only on singleton tensors!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "source": [
    "We then call `backward()` on the two scalars composing it. We also call `retain_graph=True` on the first try since each time a vanilla `backward()` is called, PT deletes the underlying computational graph for efficiency reasons."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f[0].backward(retain_graph=True) # equivalent to y_1.backward()\n",
    "f[1].backward()"
   ]
  },
  {
   "source": [
    "Let us analyze the gradients:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.grad"
   ]
  },
  {
   "source": [
    "Indeed, the grad is stored (again, for efficiency reasons) only on the leaf tensors. You may use [hooks](https://www.youtube.com/watch?v=syLFCVYua6Q) to retain also intermediate grads."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_1.grad, \"\\n\")\n",
    "print(x_2.grad)"
   ]
  },
  {
   "source": [
    "Let's do it the quick way:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = torch.stack([\n",
    "    x_1 * x_2 + x_1.cos(),\n",
    "    x_1.log() + x_2 ** 3 - x_2\n",
    "])\n",
    "ff[0].backward(retain_graph=True)\n",
    "ff[1].backward()\n",
    "print(ff)\n",
    "print(x_1.grad, \"\\n\")\n",
    "print(x_2.grad)"
   ]
  },
  {
   "source": [
    "Before, we had this\n",
    "\n",
    "```\n",
    "tensor([-0.9411]) \n",
    "\n",
    "tensor([17.2300])\n",
    "```\n",
    "\n",
    "now, this\n",
    "\n",
    "```\n",
    "tensor([-1.8822]) \n",
    "\n",
    "tensor([34.4600])\n",
    "```\n",
    "\n",
    "**Q**: who knows what happened?\n",
    "*Hint*: it has nothing to do with the fact that we did it _the quick way_."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Building a custom non-parametric module\n",
    "\n",
    "Basically, we want to create a module which is not controlled by any parameter, be it trainable or non-trainable.\n",
    "\n",
    "As an example, we might have the Leaky ReLU, an activation function which can be used in place of the more-known ReLU.\n",
    "\n",
    "$\\text{Leaky_ReLU} = \\max\\{0.01\\cdot x, x\\}$\n",
    "\n",
    "![](https://i1.wp.com/clay-atlas.com/wp-content/uploads/2019/10/image-37.png?resize=640%2C480&ssl=1)\n",
    "\n",
    "We could do it with the basic PyTorch Tensor methods, like we did at the end of Lab2. Suppose though that, for any reason, we did not have an automatic gradient calculation: we would need to build an autograd Function to implement our Leaky ReLU."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "An autograd Function inherits from `torch.autograd.Function` and has two compulsory methods: `forward` and `backward`, whose meaning should be obvious to all.\n",
    "\n",
    "Both functions have a compulsory first argument which is the **context**, `ctx` for brevity.\n",
    "From the context we can infer informations about the entities involved in the calculation of the gradient.\n",
    "The context is built upon calling the `forward` method, so that, during the `backward` call, we can obtain the info such what tensors have been used in `forward` and whether a tensor requires or not the grad.\n",
    "\n",
    "Moreover, the backward method needs an additional argument, `output_grad`, which conveys information about the gradient which is _entering_ the Function (be mindful, we're running _backward_, so a gradient _enters the function_ upstream w.r.t. the forward pass)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU_Fun(torch.autograd.Function):\n",
    "    @staticmethod # mind the decorator\n",
    "    def forward(ctx, input_):\n",
    "        ctx.save_for_backward(input_) # the parameters that will be involved in the gradient\n",
    "        return torch.max(input_, input_ * 0.01)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_, = ctx.saved_tensors # these are the variables which we need to backpropagate the gradient to (only the input)\n",
    "        # the gradient is 1 for positive x's, 0.01 for negative x's\n",
    "        grad_input = torch.ones_like(input_)\n",
    "        grad_input[input_<0] = 0.01\n",
    "        # now, we need to rescale for the grad_output\n",
    "        grad_input *= grad_output\n",
    "        '''\n",
    "        a valid alternative (maybe better performing?):\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input_<0] *= 0.01\n",
    "        '''\n",
    "        return grad_input\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = LeakyReLU_Fun.apply\n",
    "x = torch.linspace(-5,5,11, requires_grad=True)\n",
    "y = fun(x)\n",
    "z = y.sum()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return LeakyReLU_Fun.apply(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeakyReLU()(x)"
   ]
  },
  {
   "source": [
    "## Building a custom parametric module\n",
    "\n",
    "We wish to extend our Leaky ReLU module to the Parametric ReLU: $\\text{Param_ReLU} = \\max\\{\\alpha\\cdot x, x\\}, x \\in [0,1)$.\n",
    "\n",
    "![](https://pytorch.org/docs/stable/_images/PReLU.png)\n",
    "\n",
    "Parametric ReLU with $\\alpha=0.25$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamReLU_Fun(torch.autograd.Function):\n",
    "    @staticmethod # mind the decorator\n",
    "    def forward(ctx, input_, alpha:float):\n",
    "        assert alpha >= 0 and alpha < 1, f\"alpha should be >= 0 and < 1. Found {alpha}.\"\n",
    "        ctx.save_for_backward(input_) # the parameters that will be involved in the gradient\n",
    "        ctx.alpha = alpha # note that we don't use self.alpha\n",
    "        return torch.max(input_, input_ * alpha)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_, = ctx.saved_tensors # these are the variables which we need to backpropagate the gradient to (only the input)\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input_<0] *= ctx.alpha\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamReLU(torch.nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return ParamReLU_Fun.apply(X, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prelu = ParamReLU(0.25)\n",
    "x = torch.linspace(-5,5,11, requires_grad=True)\n",
    "y = prelu(x)\n",
    "z = y.sum()\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "source": [
    "## Building a custom parametric module, with trainable parameters\n",
    "\n",
    "What if our $\\alpha$ within the parametric ReLU was a trainable parameter? I.e., what if the optimizer could upldate values of $\\alpha$ during training?\n",
    "\n",
    "In this case, we will not have a single parameter $\\alpha$, but a vector $\\mathbf{a}$ of the same size of the input of the function.\n",
    "\n",
    "Moreover, we're not enforcing anymore a condition on $\\mathbf{a}$, so we must extend our Parametric ReLU formula to encompass also the condition in which $\\alpha<0$ or $\\alpha>1$. The formula becomes:\n",
    "\n",
    "$\\text{Parametric_ReLU}(x) = \\max(0, x) + \\alpha \\min(0, x)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamReLU_Trainable_Fun(torch.autograd.Function):\n",
    "    @staticmethod # mind the decorator\n",
    "    def forward(ctx, input_:torch.Tensor, alpha:torch.Tensor):\n",
    "        # we are not enforcing anymore the condition on alpha\n",
    "        ctx.save_for_backward(input_, alpha) # the parameters that will be involved in the gradient\n",
    "        zeros = torch.zeros_like(input_)\n",
    "        return torch.max(input_, zeros) + alpha * torch.min(input_, zeros)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_, alpha, = ctx.saved_tensors # these are the variables which we need to backpropagate the gradient to (only the input)\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input_<0] *= alpha.expand_as(input_)[input_<0]\n",
    "        \n",
    "        # gradient of alpha - note that the funciton param_relu(a) is constant for positive input_ -> zero derivative\n",
    "        grad_alpha = grad_output.clone()\n",
    "        grad_alpha[input_<0] *= input_[input_<0]\n",
    "        grad_alpha[input_>=0] = 0\n",
    "        return grad_input, grad_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the function for various \"anomalous\" levels of alpha\n",
    "prelu_fun = ParamReLU_Trainable_Fun.apply\n",
    "y1 = prelu_fun(x, torch.full_like(x, -0.25))\n",
    "y2 = prelu_fun(x, torch.full_like(x, 1.75))\n",
    "y3 = prelu_fun(x, torch.full_like(x, -1.25))\n",
    "plt.plot(x.detach().numpy(), y1.detach().numpy())\n",
    "plt.plot(x.detach().numpy(), y2.detach().numpy())\n",
    "plt.plot(x.detach().numpy(), y3.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamReLU_Trainable(torch.nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.alpha = torch.nn.Parameter(torch.Tensor(in_features))\n",
    "        self.alpha.data.uniform_(0, 1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return ParamReLU_Trainable_Fun.apply(X, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prelu = ParamReLU_Trainable(x.shape)\n",
    "print(prelu.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = prelu(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y.sum() * 2\n",
    "z.backward()\n",
    "print(prelu.alpha.grad)\n",
    "print(x.grad)"
   ]
  },
  {
   "source": [
    "Let us put this inside our MLP and see how things work out..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts import train\n",
    "from scripts import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(28*28, 16),\n",
    "            ParamReLU_Trainable(16),\n",
    "\n",
    "            torch.nn.BatchNorm1d(num_features=16),\n",
    "            torch.nn.Linear(16, 32),\n",
    "            ParamReLU_Trainable(32),\n",
    "\n",
    "            torch.nn.BatchNorm1d(num_features=32),\n",
    "            torch.nn.Linear(32, 24),\n",
    "            ParamReLU_Trainable(24),\n",
    "\n",
    "            torch.nn.BatchNorm1d(num_features=24),\n",
    "            torch.nn.Linear(24, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP()"
   ]
  },
  {
   "source": [
    "First, let us inspect alphas"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_alphas(net):\n",
    "    for name, param in net.named_parameters():\n",
    "        if \"alpha\" in name:\n",
    "            print(name, \"\\n\", param, \"\\n\")\n",
    "inspect_alphas(net)"
   ]
  },
  {
   "source": [
    "Then, train our network..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(net.parameters())\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "num_epochs = 5\n",
    "trainloader, testloader, _, _ = mnist.get_data()\n",
    "train.train_model(net, trainloader, loss, optim, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.test_model(net, testloader)"
   ]
  },
  {
   "source": [
    "Actually, our model is not performing bad at all.\n",
    "\n",
    "Let us check the values of the $\\alpha$s after training:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_alphas(net)"
   ]
  }
 ]
}