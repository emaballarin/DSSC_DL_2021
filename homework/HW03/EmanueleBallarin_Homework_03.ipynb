{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Deep Learning Homework \\#03\n","### Deep Learning Course $\\in$ DSSC @ UniTS (Spring 2021)  \n","\n","#### Submitted by [Emanuele Ballarin](mailto:emanuele@ballarin.cc)  "]},{"cell_type":"markdown","metadata":{},"source":["### Preliminaries:"]},{"cell_type":"markdown","metadata":{},"source":["#### Imports:\n","\n","We start off by importing all the libraries, modules, classes and functions we are going to use *today*..."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Type hints\n","from typing import Union, Optional, List\n","from torch import Tensor\n","\n","# Just to force-load MKL (if available)\n","import numpy as np\n","\n","# Functions on floats and ints\n","from math import sqrt as math_sqrt\n","\n","# Neural networks and friends\n","import torch as th\n","from torch.nn import Sequential, BatchNorm1d, Linear, LogSoftmax\n","import torch.nn.functional as F\n","\n","# Bespoke Modules / Functions / Optimizers\n","from ebtorch.nn import Mish, mishlayer_init\n","from madgrad.madgrad import MADGRAD as MadGrad\n","\n","# Model summarization\n","from torchinfo import summary\n","\n","# Dataset handling for PyTorch\n","import os\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import MNIST\n","from torchvision.transforms import ToTensor, Normalize, Compose, Lambda\n","\n","# Plotting the loss function / accuracy\n","from matplotlib import pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["### Request 1:\n","\n","Implement *L1 norm* regularization as a custom loss function;  "]},{"cell_type":"markdown","metadata":{},"source":["#### Defining the *ElasticLoss*\n","\n","We define here an *elastic net regularization* loss adapter for any *PyTorch* loss function and model. The requested $L_1$ regularization loss can be easily obtained as a special case of the following."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def elastic_loss(output: Tensor, target: Tensor, model, base_loss_fn, l1_coeff, l2_coeff, device, **kwargs) -> Tensor:\n","    \"\"\"\n","    Linearly-combined L1 and L2 penalty loss adapter. Supports any valid output vs. target loss\n","    function or criterion. Also known as 'elastic net' regularizer. Reduces to pure L1 (LASSO)\n","    or L2 (Ridge, Tichonov) regularizer for adequate choice of parameters.\n","\n","    Parameters\n","    ----------\n","    output : Tensor\n","        The class-probability or prediction batch tensor (output of a model) for a given input batch.\n","    target : Tensor\n","        The correct class(es) or prediction(s) batch for such input batch.\n","    model : Any PyTorch-compatible object supporting the .parameters() method\n","        Model whose parameters need to be considered as L1 and/or L2 penalty of.\n","    base_loss_fn : Any PyTorch-compatible loss function\n","        Base loss function to be penalized.\n","    l1_coeff : Any Tensor-broadcastable\n","        Coefficient of the LASSO penalty.\n","    l2_coeff : Any Tensor-broadcastable\n","        Coefficient of the Ridge/Tichonov penalty.\n","    device : Any Torch-compatible device (usually 'cpu', 'cuda' or 'cuda<number>')\n","        The same device the model will be running on.\n","\n","    Returns\n","    -------\n","    Tensor\n","        The value of the loss function.\n","    \"\"\"\n","\n","    # Compute base loss\n","    loss = base_loss_fn(output, target, **kwargs)\n","\n","    # Compute model-parameters L1 and L2 \"entrywise\" (a.k.a. vector-equivalent) norms\n","    l1total: Tensor = th.tensor(0, dtype=th.float).to(device)\n","    l2total: Tensor = th.tensor(0, dtype=th.float).to(device)\n","\n","    for p in model.parameters():\n","        l1total.add_(other=th.linalg.norm(p.flatten(), ord=1))\n","        l2total.add_(other=(th.linalg.norm(p.flatten(), ord=2).pow_(exponent=2)))    # Accum. squared inplace\n","\n","    # Return loss\n","    return loss.add_(other=l1total, alpha=l1_coeff).add_(other=l2total, alpha=l2_coeff)"]},{"cell_type":"markdown","metadata":{},"source":["### Request 3 (out-of-order!):\n","\n","We [have seen](https://github.com/ansuini/DSSC_DL_2021/blob/main/labs/02-sgd-training.ipynb) how to implement the *Quadratic Loss* for multinomial classification problems. Read the [paper from Demirkaya et al.](https://intra.ece.ucr.edu/~oymak/multiclass.pdf) (in which the Quadratic Loss is introduced along with its issues) and try implementing *Correct Class Quadratic Loss (CCQL)* in PyTorch as well."]},{"cell_type":"markdown","metadata":{},"source":["#### Defining the *Correct Class Quadratic Loss (CCQL)*\n","\n","We define here the *Correct Class Quadratic Loss (CCQL)* as an auto-differentiable custom *PyTorch* function."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def CCQLLoss(output: Tensor, target: Tensor, nclasses: int, w: Optional[Union[float, int]] = None, reduction: str = \"mean\") -> Tensor:\n","    \"\"\"\n","    The Correct Class Quadratic Loss. It is useful to train a classification\n","    problem with `C` classes, when emphasis on correct-class accuracy needs to\n","    be tunable with respect to overall accuracy.\n","    The criterion has been introduced in [Demirkaya et al., 2020].\n","\n","    .. [Demirkaya et al., 2020]: http://repository.bilkent.edu.tr/bitstream/handle/11693/54981/Exploring_the_role_of_loss_functions_in_multiclass_classification.pdf;jsessionid=034C8E24236741F33021DD62076126E2?sequence=1\n","\n","    Parameters\n","    ----------\n","    output : Tensor\n","        The class-probability batch tensor (output of a model) for a given input batch.\n","    target : Tensor\n","        The correct class(es) batch for such input batch.\n","    nclasses : int\n","        The overall number of different classes for the problem.\n","    w : Union[float, int], optional\n","        The weight of the correct-class sub-loss, usually in [0, +infty]. The default, None\n","        sets the weight to its optimal value, given the number of classes. Also\n","        introduced in [Demirkaya et al., 2020].\n","    reduction : str, optional\n","        The reduction to apply over the input batch. One of 'mean' and 'sum'.\n","        Defaults to 'mean'.\n","\n","    Returns\n","    -------\n","    Tensor\n","        The value of the CCQL loss, reduced over the batch.\n","\n","    Raises\n","    ------\n","    RuntimeError\n","        In case an invalid reduction os specified.\n","    \"\"\"\n","\n","    # Validate 'reduction' argument\n","    if reduction != \"sum\" and reduction != \"mean\":\n","        raise RuntimeError(\"reduction must be either 'sum' or 'mean'\")\n","    \n","    oh_targets = F.one_hot(target, num_classes=nclasses).float()\n","\n","    lql = F.mse_loss(output, oh_targets, reduction=reduction)   # 2 * Lql; halving deferred;\n","                                                                # reduction over BOTH batch and classes\n","\n","    # Set optimal w, if None\n","    if w is None:\n","        w = math_sqrt(nclasses - 1.0) - 1.0\n","\n","    ccl = th.pow(1.0 - (output * oh_targets).sum(dim=1), 2) # target-th element of predictions via\n","                                                            # dot product with one-hot target, and\n","                                                            # squared elementwise\n","\n","    # Reduce CCL\n","    if reduction == \"mean\":\n","        ccl = ccl.mean()\n","        lql.mul_(nclasses)    # undo mean-reduction over classes\n","    elif reduction == \"sum\":\n","        ccl = ccl.sum()\n","\n","    return 0.5 * (lql + w * ccl)"]},{"cell_type":"markdown","metadata":{},"source":["### An *intermezzo*\n","\n","#### Defining the *training / testing* machinery"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# MNIST DataLoader(s) builder\n","\n","\n","def spawn_mnist_loaders(\n","    data_root=\"datasets/\",\n","    batch_size_train=256,\n","    batch_size_test=512,\n","    cuda_accel=False,\n","    **kwargs\n","):\n","\n","    os.makedirs(data_root, exist_ok=True)\n","\n","    transforms = Compose(\n","        [\n","            ToTensor(),\n","            Normalize((0.1307,), (0.3081,)),  # usual normalization constants for MNIST\n","            Lambda(lambda x: th.flatten(x)),\n","        ]\n","    )\n","\n","    trainset = MNIST(data_root, train=True, transform=transforms, download=True)\n","    testset = MNIST(data_root, train=False, transform=transforms, download=True)\n","\n","    cuda_args = {}\n","    if cuda_accel:\n","        cuda_args = {\"num_workers\": 1, \"pin_memory\": True}\n","\n","    trainloader = DataLoader(\n","        trainset, batch_size=batch_size_train, shuffle=True, **cuda_args\n","    )\n","    testloader = DataLoader(\n","        testset, batch_size=batch_size_test, shuffle=False, **cuda_args\n","    )\n","    tontrloader = DataLoader(   # tontr == test on train\n","        trainset, batch_size=batch_size_test, shuffle=False, **cuda_args\n","    )\n","\n","    return trainloader, testloader, tontrloader"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Train / Test loop(s): inner part\n","\n","def train_epoch(\n","    model, device, train_loader, loss_fn, optimizer, epoch, print_every_nep, inner_scheduler=None, quiet=False,\n","):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = loss_fn(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if inner_scheduler is not None:\n","            inner_scheduler.step()\n","        if not quiet and batch_idx % print_every_nep == 0:\n","            print(\n","                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n","                    epoch,\n","                    batch_idx * len(data),\n","                    len(train_loader.dataset),\n","                    100.0 * batch_idx / len(train_loader),\n","                    loss.item(),\n","                )\n","            )\n","\n","\n","def test(model, device, test_loader, loss_fn, quiet=False):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with th.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += loss_fn(\n","                output, target, reduction=\"sum\"\n","            ).item()  # sum up batch loss\n","            pred = output.argmax(\n","                dim=1, keepdim=True\n","            )  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    ltlds = len(test_loader.dataset)\n","\n","    test_loss /= ltlds\n","    \n","    if not quiet:\n","        print(\n","            \"Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n","                test_loss,\n","                correct,\n","                ltlds,\n","                100.0 * correct / ltlds,\n","            )\n","        )\n","    \n","    return test_loss, correct / ltlds"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Setting the device\n","# NOTE: The MADGRAD optimizer works only on GPU; ensure you have access to one ;)\n","\n","device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Hyperparameters & co.\n","\n","minibatch_size_train: int = 128 # I know it's somehow high; I just want a little more stability\n","                                # i.e. I want to exploit the regularizing effect of large batch sizes\n","minibatch_size_test: int = 512\n","\n","nrepochs = 100  # Will be stopped early!"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Test and Train dataloaders\n","\n","train_loader, test_loader, test_on_train_loader = spawn_mnist_loaders(\n","    batch_size_train=minibatch_size_train,\n","    batch_size_test=minibatch_size_test,\n","    cuda_accel=bool(device == \"cuda\"),\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# THE MODEL\n","\n","# An easily-manageable, compact 3L-MLP with some added Mish flavour to it!\n","lsizes = [28*28, int(28*28*0.75), int(28*28*0.75), 10]\n","\n","model = Sequential(\n","\n","    # POST-INPUT BLOCK:\n","    Linear(in_features=lsizes[0], out_features=lsizes[1], bias=True),\n","    Mish(),\n","\n","    # HIDDEN BLOCK:\n","    BatchNorm1d(num_features=lsizes[1], affine=True),   # It's harmless at worst\n","    Linear(in_features=lsizes[1], out_features=lsizes[2], bias=True),\n","    Mish(),\n","\n","    # PRE-OUTPUT BLOCK:\n","    BatchNorm1d(num_features=lsizes[2], affine=True),   # It's harmless at worst\n","    Linear(in_features=lsizes[2], out_features=lsizes[3], bias=True),\n","    LogSoftmax(dim=1)\n","\n","        ).to(device)\n","\n","if device == \"cpu\":\n","    raise RuntimeError(\"The MADGRAD optimizer won't work without a GPU. You may want to use RAdam instead! ;)\")\n","\n","# The new kid of the post-Adam optimizers family. Looks like AdaGrad + Momentum\n","# Personal, anecdotal opinion: very favourable for convergence; needs some care to avoid overfitting.\n","optimizer = MadGrad(model.parameters(), lr=0.0002, weight_decay=10e-6)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Initialize weights of the Mish-activated model in a way that fosters convergence\n","# (no opinion given on whether it favours also generalizability; usure about it)\n","\n","for layr in model:\n","    mishlayer_init(layr)"]},{"cell_type":"markdown","metadata":{},"source":["#### Model summary"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":"=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─Linear: 1-1                            461,580\n├─Mish: 1-2                              --\n├─BatchNorm1d: 1-3                       1,176\n├─Linear: 1-4                            346,332\n├─Mish: 1-5                              --\n├─BatchNorm1d: 1-6                       1,176\n├─Linear: 1-7                            5,890\n├─LogSoftmax: 1-8                        --\n=================================================================\nTotal params: 816,154\nTrainable params: 816,154\nNon-trainable params: 0\n================================================================="},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["summary(model)"]},{"cell_type":"markdown","metadata":{},"source":["### Request 2a:\n","\n","Implement *early stopping* according to the $E_{\\text{opt}}$ specification."]},{"cell_type":"markdown","metadata":{},"source":["We will do so also using the just-implemented $L_{1}$ regularization loss.  \n","\n","Additionally, the criterion is implemented with *risk-averse gracing*, i.e. the criterion triggers a halt in training only if a pre-set number of epochs have passed; however, such additional condition should never result in a worse model compared to the situation in which no *grace* had been applied."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TRAINING...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 30.704525\n","Train Epoch: 1 [3840/60000 (6%)]\tLoss: 24.737648\n","Train Epoch: 1 [7680/60000 (13%)]\tLoss: 20.740026\n","Train Epoch: 1 [11520/60000 (19%)]\tLoss: 17.332554\n","Train Epoch: 1 [15360/60000 (26%)]\tLoss: 14.461641\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 12.211861\n","Train Epoch: 1 [23040/60000 (38%)]\tLoss: 10.423887\n","Train Epoch: 1 [26880/60000 (45%)]\tLoss: 8.837670\n","Train Epoch: 1 [30720/60000 (51%)]\tLoss: 7.701161\n","Train Epoch: 1 [34560/60000 (58%)]\tLoss: 6.975154\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 6.348234\n","Train Epoch: 1 [42240/60000 (70%)]\tLoss: 5.822669\n","Train Epoch: 1 [46080/60000 (77%)]\tLoss: 5.340216\n","Train Epoch: 1 [49920/60000 (83%)]\tLoss: 4.897836\n","Train Epoch: 1 [53760/60000 (90%)]\tLoss: 4.546145\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 4.375381\n","\n","ON TRAINING SET:\n","Average loss: 0.2046, Accuracy: 56742/60000 (95%)\n","\n","ON TEST SET:\n","Average loss: 0.2150, Accuracy: 9396/10000 (94%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 4.084240\n","Train Epoch: 2 [3840/60000 (6%)]\tLoss: 4.005355\n","Train Epoch: 2 [7680/60000 (13%)]\tLoss: 3.817745\n","Train Epoch: 2 [11520/60000 (19%)]\tLoss: 3.610757\n","Train Epoch: 2 [15360/60000 (26%)]\tLoss: 3.375965\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 3.307833\n","Train Epoch: 2 [23040/60000 (38%)]\tLoss: 3.378753\n","Train Epoch: 2 [26880/60000 (45%)]\tLoss: 3.226986\n","Train Epoch: 2 [30720/60000 (51%)]\tLoss: 3.091142\n","Train Epoch: 2 [34560/60000 (58%)]\tLoss: 3.068086\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 3.011193\n","Train Epoch: 2 [42240/60000 (70%)]\tLoss: 2.941469\n","Train Epoch: 2 [46080/60000 (77%)]\tLoss: 2.926279\n","Train Epoch: 2 [49920/60000 (83%)]\tLoss: 2.920853\n","Train Epoch: 2 [53760/60000 (90%)]\tLoss: 2.829407\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.852183\n","\n","ON TRAINING SET:\n","Average loss: 0.2094, Accuracy: 56627/60000 (94%)\n","\n","ON TEST SET:\n","Average loss: 0.2087, Accuracy: 9415/10000 (94%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.774872\n","Train Epoch: 3 [3840/60000 (6%)]\tLoss: 2.698013\n","Train Epoch: 3 [7680/60000 (13%)]\tLoss: 2.659183\n","Train Epoch: 3 [11520/60000 (19%)]\tLoss: 2.584097\n","Train Epoch: 3 [15360/60000 (26%)]\tLoss: 2.704056\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.608545\n","Train Epoch: 3 [23040/60000 (38%)]\tLoss: 2.569454\n","Train Epoch: 3 [26880/60000 (45%)]\tLoss: 2.478830\n","Train Epoch: 3 [30720/60000 (51%)]\tLoss: 2.485075\n","Train Epoch: 3 [34560/60000 (58%)]\tLoss: 2.486971\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.433170\n","Train Epoch: 3 [42240/60000 (70%)]\tLoss: 2.294269\n","Train Epoch: 3 [46080/60000 (77%)]\tLoss: 2.382549\n","Train Epoch: 3 [49920/60000 (83%)]\tLoss: 2.369254\n","Train Epoch: 3 [53760/60000 (90%)]\tLoss: 2.418791\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.360312\n","\n","ON TRAINING SET:\n","Average loss: 0.1516, Accuracy: 57600/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 0.1580, Accuracy: 9549/10000 (95%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.336377\n","Train Epoch: 4 [3840/60000 (6%)]\tLoss: 2.208331\n","Train Epoch: 4 [7680/60000 (13%)]\tLoss: 2.248824\n","Train Epoch: 4 [11520/60000 (19%)]\tLoss: 2.251632\n","Train Epoch: 4 [15360/60000 (26%)]\tLoss: 2.186656\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.145989\n","Train Epoch: 4 [23040/60000 (38%)]\tLoss: 2.081495\n","Train Epoch: 4 [26880/60000 (45%)]\tLoss: 2.095391\n","Train Epoch: 4 [30720/60000 (51%)]\tLoss: 2.047714\n","Train Epoch: 4 [34560/60000 (58%)]\tLoss: 2.041052\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.233298\n","Train Epoch: 4 [42240/60000 (70%)]\tLoss: 2.072645\n","Train Epoch: 4 [46080/60000 (77%)]\tLoss: 2.003678\n","Train Epoch: 4 [49920/60000 (83%)]\tLoss: 2.018095\n","Train Epoch: 4 [53760/60000 (90%)]\tLoss: 1.993058\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 1.928032\n","\n","ON TRAINING SET:\n","Average loss: 0.1434, Accuracy: 57802/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 0.1545, Accuracy: 9607/10000 (96%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.101098\n","Train Epoch: 5 [3840/60000 (6%)]\tLoss: 1.960890\n","Train Epoch: 5 [7680/60000 (13%)]\tLoss: 2.004443\n","Train Epoch: 5 [11520/60000 (19%)]\tLoss: 1.917226\n","Train Epoch: 5 [15360/60000 (26%)]\tLoss: 1.819068\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 1.766409\n","Train Epoch: 5 [23040/60000 (38%)]\tLoss: 1.845281\n","Train Epoch: 5 [26880/60000 (45%)]\tLoss: 1.783473\n","Train Epoch: 5 [30720/60000 (51%)]\tLoss: 1.956403\n","Train Epoch: 5 [34560/60000 (58%)]\tLoss: 1.923358\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.763244\n","Train Epoch: 5 [42240/60000 (70%)]\tLoss: 1.764721\n","Train Epoch: 5 [46080/60000 (77%)]\tLoss: 1.713365\n","Train Epoch: 5 [49920/60000 (83%)]\tLoss: 1.714304\n","Train Epoch: 5 [53760/60000 (90%)]\tLoss: 1.711384\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.708822\n","\n","ON TRAINING SET:\n","Average loss: 0.1713, Accuracy: 57155/60000 (95%)\n","\n","ON TEST SET:\n","Average loss: 0.1781, Accuracy: 9516/10000 (95%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.624897\n","Train Epoch: 6 [3840/60000 (6%)]\tLoss: 1.596800\n","Train Epoch: 6 [7680/60000 (13%)]\tLoss: 1.594311\n","Train Epoch: 6 [11520/60000 (19%)]\tLoss: 1.632913\n","Train Epoch: 6 [15360/60000 (26%)]\tLoss: 1.623003\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.549485\n","Train Epoch: 6 [23040/60000 (38%)]\tLoss: 1.521499\n","Train Epoch: 6 [26880/60000 (45%)]\tLoss: 1.483691\n","Train Epoch: 6 [30720/60000 (51%)]\tLoss: 1.463013\n","Train Epoch: 6 [34560/60000 (58%)]\tLoss: 1.508350\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.542426\n","Train Epoch: 6 [42240/60000 (70%)]\tLoss: 1.524360\n","Train Epoch: 6 [46080/60000 (77%)]\tLoss: 1.459505\n","Train Epoch: 6 [49920/60000 (83%)]\tLoss: 1.513950\n","Train Epoch: 6 [53760/60000 (90%)]\tLoss: 1.472206\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.387000\n","\n","ON TRAINING SET:\n","Average loss: 0.1328, Accuracy: 57835/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 0.1441, Accuracy: 9597/10000 (96%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.377197\n","Train Epoch: 7 [3840/60000 (6%)]\tLoss: 1.470539\n","Train Epoch: 7 [7680/60000 (13%)]\tLoss: 1.344845\n","Train Epoch: 7 [11520/60000 (19%)]\tLoss: 1.364721\n","Train Epoch: 7 [15360/60000 (26%)]\tLoss: 1.288426\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.385105\n","Train Epoch: 7 [23040/60000 (38%)]\tLoss: 1.322107\n","Train Epoch: 7 [26880/60000 (45%)]\tLoss: 1.284510\n","Train Epoch: 7 [30720/60000 (51%)]\tLoss: 1.248553\n","Train Epoch: 7 [34560/60000 (58%)]\tLoss: 1.292886\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.244820\n","Train Epoch: 7 [42240/60000 (70%)]\tLoss: 1.296306\n","Train Epoch: 7 [46080/60000 (77%)]\tLoss: 1.182709\n","Train Epoch: 7 [49920/60000 (83%)]\tLoss: 1.205678\n","Train Epoch: 7 [53760/60000 (90%)]\tLoss: 1.214873\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.174214\n","\n","ON TRAINING SET:\n","Average loss: 0.1434, Accuracy: 57679/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 0.1518, Accuracy: 9576/10000 (96%)\n","\n","\n","\n"]}],"source":["lossfn = lambda x, y, **kwargs: elastic_loss(output=x, target=y, model=model, base_loss_fn=F.nll_loss, l1_coeff=0.001, l2_coeff=0.0, device=device, **kwargs)\n","\n","# Eopt stopping (outer preparation)\n","grace_epoch = 5\n","ceopt = np.inf\n","\n","for epoch in range(1, nrepochs + 1):\n","    print(\"TRAINING...\")\n","    train_epoch(\n","        model, device, train_loader, lossfn, optimizer, epoch, print_every_nep=30, inner_scheduler=None, quiet=False,\n","    )\n","    print(\"\\nON TRAINING SET:\")\n","    _ = test(model, device, test_on_train_loader, lossfn, quiet=False)\n","    print(\"\\nON TEST SET:\")\n","    testloss, _ = test(model, device, test_loader, lossfn, quiet=False)\n","    print(\"\\n\\n\")\n","\n","    # Eopt stopping (per-epoch)\n","    if testloss > ceopt:\n","        if epoch > grace_epoch:\n","            break\n","        else:\n","            # Do not save the model if update is only due to epoch < grace_epoch\n","            ceopt = testloss\n","    else:\n","        ceopt = testloss\n","        # Always ave the model if testloss < ceopt\n","        th.save(model, \"./eopt_model.pt\")\n","\n","# Load the best model\n","model = th.load(\"./eopt_model.pt\")"]},{"cell_type":"markdown","metadata":{},"source":["### Request 2b:\n","\n","Implement *early stopping* in one of the other, additional specifications."]},{"cell_type":"markdown","metadata":{},"source":["We will implement the $GL_{\\alpha}$ criterion, and we will do so by also using the just-implemented $CCQL$ loss.  \n","\n","Additionally, the criterion is implemented with *risk-averse gracing*, i.e. the criterion triggers a halt in training only if a pre-set number of epochs have passed; however, such additional condition should never result in a worse model compared to the situation in which no *grace* had been applied."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# But first, reset model and optimizer instances\n","\n","def weight_reset(m):\n","    reset_parameters = getattr(m, \"reset_parameters\", None)\n","    if callable(reset_parameters):\n","        m.reset_parameters()\n","\n","model.apply(weight_reset)\n","\n","optimizer = MadGrad(model.parameters(), lr=0.0002, weight_decay=10e-6)\n","\n","for layr in model:\n","    mishlayer_init(layr)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TRAINING...\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 68.453087\n","Train Epoch: 1 [3840/60000 (6%)]\tLoss: 41.454453\n","Train Epoch: 1 [7680/60000 (13%)]\tLoss: 37.893993\n","Train Epoch: 1 [11520/60000 (19%)]\tLoss: 37.773579\n","Train Epoch: 1 [15360/60000 (26%)]\tLoss: 37.348087\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 37.165184\n","Train Epoch: 1 [23040/60000 (38%)]\tLoss: 37.153824\n","Train Epoch: 1 [26880/60000 (45%)]\tLoss: 37.296307\n","Train Epoch: 1 [30720/60000 (51%)]\tLoss: 37.304657\n","Train Epoch: 1 [34560/60000 (58%)]\tLoss: 37.140083\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 37.007721\n","Train Epoch: 1 [42240/60000 (70%)]\tLoss: 36.989956\n","Train Epoch: 1 [46080/60000 (77%)]\tLoss: 37.061058\n","Train Epoch: 1 [49920/60000 (83%)]\tLoss: 37.022251\n","Train Epoch: 1 [53760/60000 (90%)]\tLoss: 36.977104\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 37.049873\n","\n","ON TRAINING SET:\n","Average loss: 36.8515, Accuracy: 58533/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 36.8962, Accuracy: 9691/10000 (97%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 36.771652\n","Train Epoch: 2 [3840/60000 (6%)]\tLoss: 36.773563\n","Train Epoch: 2 [7680/60000 (13%)]\tLoss: 36.819435\n","Train Epoch: 2 [11520/60000 (19%)]\tLoss: 36.865391\n","Train Epoch: 2 [15360/60000 (26%)]\tLoss: 37.008430\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 36.879105\n","Train Epoch: 2 [23040/60000 (38%)]\tLoss: 37.001339\n","Train Epoch: 2 [26880/60000 (45%)]\tLoss: 36.787086\n","Train Epoch: 2 [30720/60000 (51%)]\tLoss: 36.876881\n","Train Epoch: 2 [34560/60000 (58%)]\tLoss: 36.814648\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 36.857178\n","Train Epoch: 2 [42240/60000 (70%)]\tLoss: 36.751915\n","Train Epoch: 2 [46080/60000 (77%)]\tLoss: 36.797993\n","Train Epoch: 2 [49920/60000 (83%)]\tLoss: 36.722103\n","Train Epoch: 2 [53760/60000 (90%)]\tLoss: 36.863838\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 36.770546\n","\n","ON TRAINING SET:\n","Average loss: 36.7070, Accuracy: 59001/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 36.7662, Accuracy: 9742/10000 (97%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 36.757656\n","Train Epoch: 3 [3840/60000 (6%)]\tLoss: 36.798214\n","Train Epoch: 3 [7680/60000 (13%)]\tLoss: 36.746315\n","Train Epoch: 3 [11520/60000 (19%)]\tLoss: 36.766594\n","Train Epoch: 3 [15360/60000 (26%)]\tLoss: 36.707275\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 36.737057\n","Train Epoch: 3 [23040/60000 (38%)]\tLoss: 36.669128\n","Train Epoch: 3 [26880/60000 (45%)]\tLoss: 36.706444\n","Train Epoch: 3 [30720/60000 (51%)]\tLoss: 36.646877\n","Train Epoch: 3 [34560/60000 (58%)]\tLoss: 36.681515\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 36.698879\n","Train Epoch: 3 [42240/60000 (70%)]\tLoss: 36.673538\n","Train Epoch: 3 [46080/60000 (77%)]\tLoss: 36.586563\n","Train Epoch: 3 [49920/60000 (83%)]\tLoss: 36.740276\n","Train Epoch: 3 [53760/60000 (90%)]\tLoss: 36.684525\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 36.621056\n","\n","ON TRAINING SET:\n","Average loss: 36.6421, Accuracy: 59292/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 36.7007, Accuracy: 9784/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 36.639961\n","Train Epoch: 4 [3840/60000 (6%)]\tLoss: 36.636951\n","Train Epoch: 4 [7680/60000 (13%)]\tLoss: 36.646843\n","Train Epoch: 4 [11520/60000 (19%)]\tLoss: 36.619911\n","Train Epoch: 4 [15360/60000 (26%)]\tLoss: 36.696522\n","Train Epoch: 4 [19200/60000 (32%)]\tLoss: 36.722660\n","Train Epoch: 4 [23040/60000 (38%)]\tLoss: 36.635971\n","Train Epoch: 4 [26880/60000 (45%)]\tLoss: 36.613182\n","Train Epoch: 4 [30720/60000 (51%)]\tLoss: 36.636890\n","Train Epoch: 4 [34560/60000 (58%)]\tLoss: 36.798912\n","Train Epoch: 4 [38400/60000 (64%)]\tLoss: 36.594940\n","Train Epoch: 4 [42240/60000 (70%)]\tLoss: 36.577328\n","Train Epoch: 4 [46080/60000 (77%)]\tLoss: 36.680202\n","Train Epoch: 4 [49920/60000 (83%)]\tLoss: 36.628437\n","Train Epoch: 4 [53760/60000 (90%)]\tLoss: 36.596962\n","Train Epoch: 4 [57600/60000 (96%)]\tLoss: 36.624794\n","\n","ON TRAINING SET:\n","Average loss: 36.5573, Accuracy: 59497/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 36.6236, Accuracy: 9811/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 36.565445\n","Train Epoch: 5 [3840/60000 (6%)]\tLoss: 36.539768\n","Train Epoch: 5 [7680/60000 (13%)]\tLoss: 36.635719\n","Train Epoch: 5 [11520/60000 (19%)]\tLoss: 36.580563\n","Train Epoch: 5 [15360/60000 (26%)]\tLoss: 36.663147\n","Train Epoch: 5 [19200/60000 (32%)]\tLoss: 36.614750\n","Train Epoch: 5 [23040/60000 (38%)]\tLoss: 36.618805\n","Train Epoch: 5 [26880/60000 (45%)]\tLoss: 36.494724\n","Train Epoch: 5 [30720/60000 (51%)]\tLoss: 36.605717\n","Train Epoch: 5 [34560/60000 (58%)]\tLoss: 36.620094\n","Train Epoch: 5 [38400/60000 (64%)]\tLoss: 36.626068\n","Train Epoch: 5 [42240/60000 (70%)]\tLoss: 36.596596\n","Train Epoch: 5 [46080/60000 (77%)]\tLoss: 36.598724\n","Train Epoch: 5 [49920/60000 (83%)]\tLoss: 36.639748\n","Train Epoch: 5 [53760/60000 (90%)]\tLoss: 36.666759\n","Train Epoch: 5 [57600/60000 (96%)]\tLoss: 36.659229\n","\n","ON TRAINING SET:\n","Average loss: 36.5417, Accuracy: 59582/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 36.6126, Accuracy: 9821/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 36.541019\n","Train Epoch: 6 [3840/60000 (6%)]\tLoss: 36.525517\n","Train Epoch: 6 [7680/60000 (13%)]\tLoss: 36.577927\n","Train Epoch: 6 [11520/60000 (19%)]\tLoss: 36.531288\n","Train Epoch: 6 [15360/60000 (26%)]\tLoss: 36.577095\n","Train Epoch: 6 [19200/60000 (32%)]\tLoss: 36.557079\n","Train Epoch: 6 [23040/60000 (38%)]\tLoss: 36.591263\n","Train Epoch: 6 [26880/60000 (45%)]\tLoss: 36.543686\n","Train Epoch: 6 [30720/60000 (51%)]\tLoss: 36.588203\n","Train Epoch: 6 [34560/60000 (58%)]\tLoss: 36.593895\n","Train Epoch: 6 [38400/60000 (64%)]\tLoss: 36.559166\n","Train Epoch: 6 [42240/60000 (70%)]\tLoss: 36.535839\n","Train Epoch: 6 [46080/60000 (77%)]\tLoss: 36.581104\n","Train Epoch: 6 [49920/60000 (83%)]\tLoss: 36.520550\n","Train Epoch: 6 [53760/60000 (90%)]\tLoss: 36.566940\n","Train Epoch: 6 [57600/60000 (96%)]\tLoss: 36.548546\n","\n","ON TRAINING SET:\n","Average loss: 36.4918, Accuracy: 59679/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 36.5660, Accuracy: 9834/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 36.540859\n","Train Epoch: 7 [3840/60000 (6%)]\tLoss: 36.543266\n","Train Epoch: 7 [7680/60000 (13%)]\tLoss: 36.512341\n","Train Epoch: 7 [11520/60000 (19%)]\tLoss: 36.560387\n","Train Epoch: 7 [15360/60000 (26%)]\tLoss: 36.514233\n","Train Epoch: 7 [19200/60000 (32%)]\tLoss: 36.611050\n","Train Epoch: 7 [23040/60000 (38%)]\tLoss: 36.535789\n","Train Epoch: 7 [26880/60000 (45%)]\tLoss: 36.524097\n","Train Epoch: 7 [30720/60000 (51%)]\tLoss: 36.481598\n","Train Epoch: 7 [34560/60000 (58%)]\tLoss: 36.540985\n","Train Epoch: 7 [38400/60000 (64%)]\tLoss: 36.534557\n","Train Epoch: 7 [42240/60000 (70%)]\tLoss: 36.526321\n","Train Epoch: 7 [46080/60000 (77%)]\tLoss: 36.657585\n","Train Epoch: 7 [49920/60000 (83%)]\tLoss: 36.598679\n","Train Epoch: 7 [53760/60000 (90%)]\tLoss: 36.537285\n","Train Epoch: 7 [57600/60000 (96%)]\tLoss: 36.538803\n","\n","ON TRAINING SET:\n","Average loss: 36.4793, Accuracy: 59739/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.5585, Accuracy: 9827/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 36.568821\n","Train Epoch: 8 [3840/60000 (6%)]\tLoss: 36.492344\n","Train Epoch: 8 [7680/60000 (13%)]\tLoss: 36.537151\n","Train Epoch: 8 [11520/60000 (19%)]\tLoss: 36.528202\n","Train Epoch: 8 [15360/60000 (26%)]\tLoss: 36.530716\n","Train Epoch: 8 [19200/60000 (32%)]\tLoss: 36.496700\n","Train Epoch: 8 [23040/60000 (38%)]\tLoss: 36.465466\n","Train Epoch: 8 [26880/60000 (45%)]\tLoss: 36.534756\n","Train Epoch: 8 [30720/60000 (51%)]\tLoss: 36.486717\n","Train Epoch: 8 [34560/60000 (58%)]\tLoss: 36.513088\n","Train Epoch: 8 [38400/60000 (64%)]\tLoss: 36.543045\n","Train Epoch: 8 [42240/60000 (70%)]\tLoss: 36.518478\n","Train Epoch: 8 [46080/60000 (77%)]\tLoss: 36.544834\n","Train Epoch: 8 [49920/60000 (83%)]\tLoss: 36.476074\n","Train Epoch: 8 [53760/60000 (90%)]\tLoss: 36.542095\n","Train Epoch: 8 [57600/60000 (96%)]\tLoss: 36.542850\n","\n","ON TRAINING SET:\n","Average loss: 36.4814, Accuracy: 59801/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.5629, Accuracy: 9835/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 36.551994\n","Train Epoch: 9 [3840/60000 (6%)]\tLoss: 36.478611\n","Train Epoch: 9 [7680/60000 (13%)]\tLoss: 36.512264\n","Train Epoch: 9 [11520/60000 (19%)]\tLoss: 36.488945\n","Train Epoch: 9 [15360/60000 (26%)]\tLoss: 36.476540\n","Train Epoch: 9 [19200/60000 (32%)]\tLoss: 36.502689\n","Train Epoch: 9 [23040/60000 (38%)]\tLoss: 36.527397\n","Train Epoch: 9 [26880/60000 (45%)]\tLoss: 36.535702\n","Train Epoch: 9 [30720/60000 (51%)]\tLoss: 36.545895\n","Train Epoch: 9 [34560/60000 (58%)]\tLoss: 36.574493\n","Train Epoch: 9 [38400/60000 (64%)]\tLoss: 36.527889\n","Train Epoch: 9 [42240/60000 (70%)]\tLoss: 36.555725\n","Train Epoch: 9 [46080/60000 (77%)]\tLoss: 36.488747\n","Train Epoch: 9 [49920/60000 (83%)]\tLoss: 36.487396\n","Train Epoch: 9 [53760/60000 (90%)]\tLoss: 36.566265\n","Train Epoch: 9 [57600/60000 (96%)]\tLoss: 36.566795\n","\n","ON TRAINING SET:\n","Average loss: 36.4513, Accuracy: 59844/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.5329, Accuracy: 9835/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 36.501564\n","Train Epoch: 10 [3840/60000 (6%)]\tLoss: 36.484726\n","Train Epoch: 10 [7680/60000 (13%)]\tLoss: 36.507664\n","Train Epoch: 10 [11520/60000 (19%)]\tLoss: 36.463116\n","Train Epoch: 10 [15360/60000 (26%)]\tLoss: 36.510456\n","Train Epoch: 10 [19200/60000 (32%)]\tLoss: 36.527126\n","Train Epoch: 10 [23040/60000 (38%)]\tLoss: 36.482269\n","Train Epoch: 10 [26880/60000 (45%)]\tLoss: 36.533432\n","Train Epoch: 10 [30720/60000 (51%)]\tLoss: 36.490616\n","Train Epoch: 10 [34560/60000 (58%)]\tLoss: 36.492874\n","Train Epoch: 10 [38400/60000 (64%)]\tLoss: 36.560493\n","Train Epoch: 10 [42240/60000 (70%)]\tLoss: 36.500031\n","Train Epoch: 10 [46080/60000 (77%)]\tLoss: 36.524807\n","Train Epoch: 10 [49920/60000 (83%)]\tLoss: 36.454765\n","Train Epoch: 10 [53760/60000 (90%)]\tLoss: 36.474777\n","Train Epoch: 10 [57600/60000 (96%)]\tLoss: 36.511467\n","\n","ON TRAINING SET:\n","Average loss: 36.4377, Accuracy: 59845/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.5222, Accuracy: 9845/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 11 [0/60000 (0%)]\tLoss: 36.467533\n","Train Epoch: 11 [3840/60000 (6%)]\tLoss: 36.455334\n","Train Epoch: 11 [7680/60000 (13%)]\tLoss: 36.458721\n","Train Epoch: 11 [11520/60000 (19%)]\tLoss: 36.481350\n","Train Epoch: 11 [15360/60000 (26%)]\tLoss: 36.467644\n","Train Epoch: 11 [19200/60000 (32%)]\tLoss: 36.462315\n","Train Epoch: 11 [23040/60000 (38%)]\tLoss: 36.459457\n","Train Epoch: 11 [26880/60000 (45%)]\tLoss: 36.498344\n","Train Epoch: 11 [30720/60000 (51%)]\tLoss: 36.483589\n","Train Epoch: 11 [34560/60000 (58%)]\tLoss: 36.478085\n","Train Epoch: 11 [38400/60000 (64%)]\tLoss: 36.478569\n","Train Epoch: 11 [42240/60000 (70%)]\tLoss: 36.488979\n","Train Epoch: 11 [46080/60000 (77%)]\tLoss: 36.448879\n","Train Epoch: 11 [49920/60000 (83%)]\tLoss: 36.496704\n","Train Epoch: 11 [53760/60000 (90%)]\tLoss: 36.480003\n","Train Epoch: 11 [57600/60000 (96%)]\tLoss: 36.504704\n","\n","ON TRAINING SET:\n","Average loss: 36.4402, Accuracy: 59889/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.5266, Accuracy: 9849/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 12 [0/60000 (0%)]\tLoss: 36.428326\n","Train Epoch: 12 [3840/60000 (6%)]\tLoss: 36.438835\n","Train Epoch: 12 [7680/60000 (13%)]\tLoss: 36.441040\n","Train Epoch: 12 [11520/60000 (19%)]\tLoss: 36.505314\n","Train Epoch: 12 [15360/60000 (26%)]\tLoss: 36.448696\n","Train Epoch: 12 [19200/60000 (32%)]\tLoss: 36.446159\n","Train Epoch: 12 [23040/60000 (38%)]\tLoss: 36.442425\n","Train Epoch: 12 [26880/60000 (45%)]\tLoss: 36.418705\n","Train Epoch: 12 [30720/60000 (51%)]\tLoss: 36.448444\n","Train Epoch: 12 [34560/60000 (58%)]\tLoss: 36.440094\n","Train Epoch: 12 [38400/60000 (64%)]\tLoss: 36.465130\n","Train Epoch: 12 [42240/60000 (70%)]\tLoss: 36.459648\n","Train Epoch: 12 [46080/60000 (77%)]\tLoss: 36.486950\n","Train Epoch: 12 [49920/60000 (83%)]\tLoss: 36.450253\n","Train Epoch: 12 [53760/60000 (90%)]\tLoss: 36.460686\n","Train Epoch: 12 [57600/60000 (96%)]\tLoss: 36.473286\n","\n","ON TRAINING SET:\n","Average loss: 36.4247, Accuracy: 59901/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.5146, Accuracy: 9846/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 13 [0/60000 (0%)]\tLoss: 36.446434\n","Train Epoch: 13 [3840/60000 (6%)]\tLoss: 36.490116\n","Train Epoch: 13 [7680/60000 (13%)]\tLoss: 36.461544\n","Train Epoch: 13 [11520/60000 (19%)]\tLoss: 36.461830\n","Train Epoch: 13 [15360/60000 (26%)]\tLoss: 36.435081\n","Train Epoch: 13 [19200/60000 (32%)]\tLoss: 36.453423\n","Train Epoch: 13 [23040/60000 (38%)]\tLoss: 36.455231\n","Train Epoch: 13 [26880/60000 (45%)]\tLoss: 36.516960\n","Train Epoch: 13 [30720/60000 (51%)]\tLoss: 36.463459\n","Train Epoch: 13 [34560/60000 (58%)]\tLoss: 36.461040\n","Train Epoch: 13 [38400/60000 (64%)]\tLoss: 36.458557\n","Train Epoch: 13 [42240/60000 (70%)]\tLoss: 36.433731\n","Train Epoch: 13 [46080/60000 (77%)]\tLoss: 36.472851\n","Train Epoch: 13 [49920/60000 (83%)]\tLoss: 36.437069\n","Train Epoch: 13 [53760/60000 (90%)]\tLoss: 36.499115\n","Train Epoch: 13 [57600/60000 (96%)]\tLoss: 36.475742\n","\n","ON TRAINING SET:\n","Average loss: 36.4334, Accuracy: 59934/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.5221, Accuracy: 9851/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 14 [0/60000 (0%)]\tLoss: 36.491425\n","Train Epoch: 14 [3840/60000 (6%)]\tLoss: 36.496895\n","Train Epoch: 14 [7680/60000 (13%)]\tLoss: 36.450047\n","Train Epoch: 14 [11520/60000 (19%)]\tLoss: 36.457642\n","Train Epoch: 14 [15360/60000 (26%)]\tLoss: 36.445377\n","Train Epoch: 14 [19200/60000 (32%)]\tLoss: 36.419788\n","Train Epoch: 14 [23040/60000 (38%)]\tLoss: 36.434052\n","Train Epoch: 14 [26880/60000 (45%)]\tLoss: 36.423706\n","Train Epoch: 14 [30720/60000 (51%)]\tLoss: 36.454048\n","Train Epoch: 14 [34560/60000 (58%)]\tLoss: 36.437729\n","Train Epoch: 14 [38400/60000 (64%)]\tLoss: 36.436859\n","Train Epoch: 14 [42240/60000 (70%)]\tLoss: 36.408157\n","Train Epoch: 14 [46080/60000 (77%)]\tLoss: 36.480625\n","Train Epoch: 14 [49920/60000 (83%)]\tLoss: 36.455040\n","Train Epoch: 14 [53760/60000 (90%)]\tLoss: 36.458778\n","Train Epoch: 14 [57600/60000 (96%)]\tLoss: 36.447784\n","\n","ON TRAINING SET:\n","Average loss: 36.4124, Accuracy: 59937/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.5029, Accuracy: 9862/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 15 [0/60000 (0%)]\tLoss: 36.475212\n","Train Epoch: 15 [3840/60000 (6%)]\tLoss: 36.418564\n","Train Epoch: 15 [7680/60000 (13%)]\tLoss: 36.420193\n","Train Epoch: 15 [11520/60000 (19%)]\tLoss: 36.425545\n","Train Epoch: 15 [15360/60000 (26%)]\tLoss: 36.447708\n","Train Epoch: 15 [19200/60000 (32%)]\tLoss: 36.434067\n","Train Epoch: 15 [23040/60000 (38%)]\tLoss: 36.459698\n","Train Epoch: 15 [26880/60000 (45%)]\tLoss: 36.444237\n","Train Epoch: 15 [30720/60000 (51%)]\tLoss: 36.419960\n","Train Epoch: 15 [34560/60000 (58%)]\tLoss: 36.435230\n","Train Epoch: 15 [38400/60000 (64%)]\tLoss: 36.446522\n","Train Epoch: 15 [42240/60000 (70%)]\tLoss: 36.421478\n","Train Epoch: 15 [46080/60000 (77%)]\tLoss: 36.429077\n","Train Epoch: 15 [49920/60000 (83%)]\tLoss: 36.449600\n","Train Epoch: 15 [53760/60000 (90%)]\tLoss: 36.464188\n","Train Epoch: 15 [57600/60000 (96%)]\tLoss: 36.450447\n","\n","ON TRAINING SET:\n","Average loss: 36.4007, Accuracy: 59958/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4930, Accuracy: 9852/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 16 [0/60000 (0%)]\tLoss: 36.434212\n","Train Epoch: 16 [3840/60000 (6%)]\tLoss: 36.442402\n","Train Epoch: 16 [7680/60000 (13%)]\tLoss: 36.414627\n","Train Epoch: 16 [11520/60000 (19%)]\tLoss: 36.459576\n","Train Epoch: 16 [15360/60000 (26%)]\tLoss: 36.439888\n","Train Epoch: 16 [19200/60000 (32%)]\tLoss: 36.412949\n","Train Epoch: 16 [23040/60000 (38%)]\tLoss: 36.446510\n","Train Epoch: 16 [26880/60000 (45%)]\tLoss: 36.450855\n","Train Epoch: 16 [30720/60000 (51%)]\tLoss: 36.431698\n","Train Epoch: 16 [34560/60000 (58%)]\tLoss: 36.416607\n","Train Epoch: 16 [38400/60000 (64%)]\tLoss: 36.446579\n","Train Epoch: 16 [42240/60000 (70%)]\tLoss: 36.436790\n","Train Epoch: 16 [46080/60000 (77%)]\tLoss: 36.452492\n","Train Epoch: 16 [49920/60000 (83%)]\tLoss: 36.446777\n","Train Epoch: 16 [53760/60000 (90%)]\tLoss: 36.402256\n","Train Epoch: 16 [57600/60000 (96%)]\tLoss: 36.428726\n","\n","ON TRAINING SET:\n","Average loss: 36.4143, Accuracy: 59960/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.5067, Accuracy: 9850/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 17 [0/60000 (0%)]\tLoss: 36.449909\n","Train Epoch: 17 [3840/60000 (6%)]\tLoss: 36.406082\n","Train Epoch: 17 [7680/60000 (13%)]\tLoss: 36.425888\n","Train Epoch: 17 [11520/60000 (19%)]\tLoss: 36.410706\n","Train Epoch: 17 [15360/60000 (26%)]\tLoss: 36.435204\n","Train Epoch: 17 [19200/60000 (32%)]\tLoss: 36.412304\n","Train Epoch: 17 [23040/60000 (38%)]\tLoss: 36.422131\n","Train Epoch: 17 [26880/60000 (45%)]\tLoss: 36.427277\n","Train Epoch: 17 [30720/60000 (51%)]\tLoss: 36.429176\n","Train Epoch: 17 [34560/60000 (58%)]\tLoss: 36.433983\n","Train Epoch: 17 [38400/60000 (64%)]\tLoss: 36.436176\n","Train Epoch: 17 [42240/60000 (70%)]\tLoss: 36.434929\n","Train Epoch: 17 [46080/60000 (77%)]\tLoss: 36.418201\n","Train Epoch: 17 [49920/60000 (83%)]\tLoss: 36.417011\n","Train Epoch: 17 [53760/60000 (90%)]\tLoss: 36.423195\n","Train Epoch: 17 [57600/60000 (96%)]\tLoss: 36.460560\n","\n","ON TRAINING SET:\n","Average loss: 36.4023, Accuracy: 59966/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4966, Accuracy: 9860/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 18 [0/60000 (0%)]\tLoss: 36.430183\n","Train Epoch: 18 [3840/60000 (6%)]\tLoss: 36.451752\n","Train Epoch: 18 [7680/60000 (13%)]\tLoss: 36.429867\n","Train Epoch: 18 [11520/60000 (19%)]\tLoss: 36.418308\n","Train Epoch: 18 [15360/60000 (26%)]\tLoss: 36.412697\n","Train Epoch: 18 [19200/60000 (32%)]\tLoss: 36.431648\n","Train Epoch: 18 [23040/60000 (38%)]\tLoss: 36.425014\n","Train Epoch: 18 [26880/60000 (45%)]\tLoss: 36.404140\n","Train Epoch: 18 [30720/60000 (51%)]\tLoss: 36.432365\n","Train Epoch: 18 [34560/60000 (58%)]\tLoss: 36.405773\n","Train Epoch: 18 [38400/60000 (64%)]\tLoss: 36.411533\n","Train Epoch: 18 [42240/60000 (70%)]\tLoss: 36.405060\n","Train Epoch: 18 [46080/60000 (77%)]\tLoss: 36.409348\n","Train Epoch: 18 [49920/60000 (83%)]\tLoss: 36.475277\n","Train Epoch: 18 [53760/60000 (90%)]\tLoss: 36.404198\n","Train Epoch: 18 [57600/60000 (96%)]\tLoss: 36.430912\n","\n","ON TRAINING SET:\n","Average loss: 36.3849, Accuracy: 59976/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4783, Accuracy: 9856/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 19 [0/60000 (0%)]\tLoss: 36.430073\n","Train Epoch: 19 [3840/60000 (6%)]\tLoss: 36.461308\n","Train Epoch: 19 [7680/60000 (13%)]\tLoss: 36.442959\n","Train Epoch: 19 [11520/60000 (19%)]\tLoss: 36.397808\n","Train Epoch: 19 [15360/60000 (26%)]\tLoss: 36.436626\n","Train Epoch: 19 [19200/60000 (32%)]\tLoss: 36.416443\n","Train Epoch: 19 [23040/60000 (38%)]\tLoss: 36.437813\n","Train Epoch: 19 [26880/60000 (45%)]\tLoss: 36.405254\n","Train Epoch: 19 [30720/60000 (51%)]\tLoss: 36.432896\n","Train Epoch: 19 [34560/60000 (58%)]\tLoss: 36.431919\n","Train Epoch: 19 [38400/60000 (64%)]\tLoss: 36.451878\n","Train Epoch: 19 [42240/60000 (70%)]\tLoss: 36.419437\n","Train Epoch: 19 [46080/60000 (77%)]\tLoss: 36.424088\n","Train Epoch: 19 [49920/60000 (83%)]\tLoss: 36.419071\n","Train Epoch: 19 [53760/60000 (90%)]\tLoss: 36.415291\n","Train Epoch: 19 [57600/60000 (96%)]\tLoss: 36.397648\n","\n","ON TRAINING SET:\n","Average loss: 36.3937, Accuracy: 59977/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4871, Accuracy: 9861/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 20 [0/60000 (0%)]\tLoss: 36.443310\n","Train Epoch: 20 [3840/60000 (6%)]\tLoss: 36.390793\n","Train Epoch: 20 [7680/60000 (13%)]\tLoss: 36.408546\n","Train Epoch: 20 [11520/60000 (19%)]\tLoss: 36.419235\n","Train Epoch: 20 [15360/60000 (26%)]\tLoss: 36.424126\n","Train Epoch: 20 [19200/60000 (32%)]\tLoss: 36.407204\n","Train Epoch: 20 [23040/60000 (38%)]\tLoss: 36.404114\n","Train Epoch: 20 [26880/60000 (45%)]\tLoss: 36.441887\n","Train Epoch: 20 [30720/60000 (51%)]\tLoss: 36.410496\n","Train Epoch: 20 [34560/60000 (58%)]\tLoss: 36.398354\n","Train Epoch: 20 [38400/60000 (64%)]\tLoss: 36.399006\n","Train Epoch: 20 [42240/60000 (70%)]\tLoss: 36.416405\n","Train Epoch: 20 [46080/60000 (77%)]\tLoss: 36.417881\n","Train Epoch: 20 [49920/60000 (83%)]\tLoss: 36.418995\n","Train Epoch: 20 [53760/60000 (90%)]\tLoss: 36.431644\n","Train Epoch: 20 [57600/60000 (96%)]\tLoss: 36.385536\n","\n","ON TRAINING SET:\n","Average loss: 36.3858, Accuracy: 59981/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4816, Accuracy: 9846/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 21 [0/60000 (0%)]\tLoss: 36.397934\n","Train Epoch: 21 [3840/60000 (6%)]\tLoss: 36.450481\n","Train Epoch: 21 [7680/60000 (13%)]\tLoss: 36.444923\n","Train Epoch: 21 [11520/60000 (19%)]\tLoss: 36.442982\n","Train Epoch: 21 [15360/60000 (26%)]\tLoss: 36.422169\n","Train Epoch: 21 [19200/60000 (32%)]\tLoss: 36.403885\n","Train Epoch: 21 [23040/60000 (38%)]\tLoss: 36.402805\n","Train Epoch: 21 [26880/60000 (45%)]\tLoss: 36.428127\n","Train Epoch: 21 [30720/60000 (51%)]\tLoss: 36.438210\n","Train Epoch: 21 [34560/60000 (58%)]\tLoss: 36.423786\n","Train Epoch: 21 [38400/60000 (64%)]\tLoss: 36.428982\n","Train Epoch: 21 [42240/60000 (70%)]\tLoss: 36.426292\n","Train Epoch: 21 [46080/60000 (77%)]\tLoss: 36.410332\n","Train Epoch: 21 [49920/60000 (83%)]\tLoss: 36.419952\n","Train Epoch: 21 [53760/60000 (90%)]\tLoss: 36.447006\n","Train Epoch: 21 [57600/60000 (96%)]\tLoss: 36.419117\n","\n","ON TRAINING SET:\n","Average loss: 36.3907, Accuracy: 59979/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4851, Accuracy: 9859/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 22 [0/60000 (0%)]\tLoss: 36.415920\n","Train Epoch: 22 [3840/60000 (6%)]\tLoss: 36.412109\n","Train Epoch: 22 [7680/60000 (13%)]\tLoss: 36.425858\n","Train Epoch: 22 [11520/60000 (19%)]\tLoss: 36.442661\n","Train Epoch: 22 [15360/60000 (26%)]\tLoss: 36.436451\n","Train Epoch: 22 [19200/60000 (32%)]\tLoss: 36.402412\n","Train Epoch: 22 [23040/60000 (38%)]\tLoss: 36.401207\n","Train Epoch: 22 [26880/60000 (45%)]\tLoss: 36.396759\n","Train Epoch: 22 [30720/60000 (51%)]\tLoss: 36.398438\n","Train Epoch: 22 [34560/60000 (58%)]\tLoss: 36.398933\n","Train Epoch: 22 [38400/60000 (64%)]\tLoss: 36.400360\n","Train Epoch: 22 [42240/60000 (70%)]\tLoss: 36.397552\n","Train Epoch: 22 [46080/60000 (77%)]\tLoss: 36.418598\n","Train Epoch: 22 [49920/60000 (83%)]\tLoss: 36.428524\n","Train Epoch: 22 [53760/60000 (90%)]\tLoss: 36.413662\n","Train Epoch: 22 [57600/60000 (96%)]\tLoss: 36.405037\n","\n","ON TRAINING SET:\n","Average loss: 36.3786, Accuracy: 59988/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4742, Accuracy: 9864/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 23 [0/60000 (0%)]\tLoss: 36.402206\n","Train Epoch: 23 [3840/60000 (6%)]\tLoss: 36.400822\n","Train Epoch: 23 [7680/60000 (13%)]\tLoss: 36.426350\n","Train Epoch: 23 [11520/60000 (19%)]\tLoss: 36.400429\n","Train Epoch: 23 [15360/60000 (26%)]\tLoss: 36.404419\n","Train Epoch: 23 [19200/60000 (32%)]\tLoss: 36.380222\n","Train Epoch: 23 [23040/60000 (38%)]\tLoss: 36.393166\n","Train Epoch: 23 [26880/60000 (45%)]\tLoss: 36.409344\n","Train Epoch: 23 [30720/60000 (51%)]\tLoss: 36.427643\n","Train Epoch: 23 [34560/60000 (58%)]\tLoss: 36.407063\n","Train Epoch: 23 [38400/60000 (64%)]\tLoss: 36.406151\n","Train Epoch: 23 [42240/60000 (70%)]\tLoss: 36.401913\n","Train Epoch: 23 [46080/60000 (77%)]\tLoss: 36.409027\n","Train Epoch: 23 [49920/60000 (83%)]\tLoss: 36.420601\n","Train Epoch: 23 [53760/60000 (90%)]\tLoss: 36.407993\n","Train Epoch: 23 [57600/60000 (96%)]\tLoss: 36.415592\n","\n","ON TRAINING SET:\n","Average loss: 36.3715, Accuracy: 59991/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4675, Accuracy: 9865/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 24 [0/60000 (0%)]\tLoss: 36.393009\n","Train Epoch: 24 [3840/60000 (6%)]\tLoss: 36.391090\n","Train Epoch: 24 [7680/60000 (13%)]\tLoss: 36.425117\n","Train Epoch: 24 [11520/60000 (19%)]\tLoss: 36.444328\n","Train Epoch: 24 [15360/60000 (26%)]\tLoss: 36.411972\n","Train Epoch: 24 [19200/60000 (32%)]\tLoss: 36.387779\n","Train Epoch: 24 [23040/60000 (38%)]\tLoss: 36.393925\n","Train Epoch: 24 [26880/60000 (45%)]\tLoss: 36.408180\n","Train Epoch: 24 [30720/60000 (51%)]\tLoss: 36.422710\n","Train Epoch: 24 [34560/60000 (58%)]\tLoss: 36.410828\n","Train Epoch: 24 [38400/60000 (64%)]\tLoss: 36.399651\n","Train Epoch: 24 [42240/60000 (70%)]\tLoss: 36.379692\n","Train Epoch: 24 [46080/60000 (77%)]\tLoss: 36.391670\n","Train Epoch: 24 [49920/60000 (83%)]\tLoss: 36.400642\n","Train Epoch: 24 [53760/60000 (90%)]\tLoss: 36.409836\n","Train Epoch: 24 [57600/60000 (96%)]\tLoss: 36.434013\n","\n","ON TRAINING SET:\n","Average loss: 36.3792, Accuracy: 59988/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4753, Accuracy: 9861/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 25 [0/60000 (0%)]\tLoss: 36.392330\n","Train Epoch: 25 [3840/60000 (6%)]\tLoss: 36.398216\n","Train Epoch: 25 [7680/60000 (13%)]\tLoss: 36.386570\n","Train Epoch: 25 [11520/60000 (19%)]\tLoss: 36.396877\n","Train Epoch: 25 [15360/60000 (26%)]\tLoss: 36.407684\n","Train Epoch: 25 [19200/60000 (32%)]\tLoss: 36.407200\n","Train Epoch: 25 [23040/60000 (38%)]\tLoss: 36.407162\n","Train Epoch: 25 [26880/60000 (45%)]\tLoss: 36.404617\n","Train Epoch: 25 [30720/60000 (51%)]\tLoss: 36.403786\n","Train Epoch: 25 [34560/60000 (58%)]\tLoss: 36.388031\n","Train Epoch: 25 [38400/60000 (64%)]\tLoss: 36.417900\n","Train Epoch: 25 [42240/60000 (70%)]\tLoss: 36.418678\n","Train Epoch: 25 [46080/60000 (77%)]\tLoss: 36.409180\n","Train Epoch: 25 [49920/60000 (83%)]\tLoss: 36.421253\n","Train Epoch: 25 [53760/60000 (90%)]\tLoss: 36.391571\n","Train Epoch: 25 [57600/60000 (96%)]\tLoss: 36.385124\n","\n","ON TRAINING SET:\n","Average loss: 36.3718, Accuracy: 59991/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4672, Accuracy: 9858/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 26 [0/60000 (0%)]\tLoss: 36.418633\n","Train Epoch: 26 [3840/60000 (6%)]\tLoss: 36.417126\n","Train Epoch: 26 [7680/60000 (13%)]\tLoss: 36.406143\n","Train Epoch: 26 [11520/60000 (19%)]\tLoss: 36.393955\n","Train Epoch: 26 [15360/60000 (26%)]\tLoss: 36.390377\n","Train Epoch: 26 [19200/60000 (32%)]\tLoss: 36.406582\n","Train Epoch: 26 [23040/60000 (38%)]\tLoss: 36.394005\n","Train Epoch: 26 [26880/60000 (45%)]\tLoss: 36.396049\n","Train Epoch: 26 [30720/60000 (51%)]\tLoss: 36.405689\n","Train Epoch: 26 [34560/60000 (58%)]\tLoss: 36.410133\n","Train Epoch: 26 [38400/60000 (64%)]\tLoss: 36.412174\n","Train Epoch: 26 [42240/60000 (70%)]\tLoss: 36.392403\n","Train Epoch: 26 [46080/60000 (77%)]\tLoss: 36.394287\n","Train Epoch: 26 [49920/60000 (83%)]\tLoss: 36.405563\n","Train Epoch: 26 [53760/60000 (90%)]\tLoss: 36.389076\n","Train Epoch: 26 [57600/60000 (96%)]\tLoss: 36.393600\n","\n","ON TRAINING SET:\n","Average loss: 36.3754, Accuracy: 59994/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4715, Accuracy: 9859/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 27 [0/60000 (0%)]\tLoss: 36.389141\n","Train Epoch: 27 [3840/60000 (6%)]\tLoss: 36.377090\n","Train Epoch: 27 [7680/60000 (13%)]\tLoss: 36.385002\n","Train Epoch: 27 [11520/60000 (19%)]\tLoss: 36.373711\n","Train Epoch: 27 [15360/60000 (26%)]\tLoss: 36.390339\n","Train Epoch: 27 [19200/60000 (32%)]\tLoss: 36.400356\n","Train Epoch: 27 [23040/60000 (38%)]\tLoss: 36.380863\n","Train Epoch: 27 [26880/60000 (45%)]\tLoss: 36.417915\n","Train Epoch: 27 [30720/60000 (51%)]\tLoss: 36.392693\n","Train Epoch: 27 [34560/60000 (58%)]\tLoss: 36.386837\n","Train Epoch: 27 [38400/60000 (64%)]\tLoss: 36.381916\n","Train Epoch: 27 [42240/60000 (70%)]\tLoss: 36.407898\n","Train Epoch: 27 [46080/60000 (77%)]\tLoss: 36.400887\n","Train Epoch: 27 [49920/60000 (83%)]\tLoss: 36.398716\n","Train Epoch: 27 [53760/60000 (90%)]\tLoss: 36.384174\n","Train Epoch: 27 [57600/60000 (96%)]\tLoss: 36.384739\n","\n","ON TRAINING SET:\n","Average loss: 36.3654, Accuracy: 59995/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4618, Accuracy: 9858/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 28 [0/60000 (0%)]\tLoss: 36.385872\n","Train Epoch: 28 [3840/60000 (6%)]\tLoss: 36.395920\n","Train Epoch: 28 [7680/60000 (13%)]\tLoss: 36.388206\n","Train Epoch: 28 [11520/60000 (19%)]\tLoss: 36.379272\n","Train Epoch: 28 [15360/60000 (26%)]\tLoss: 36.389961\n","Train Epoch: 28 [19200/60000 (32%)]\tLoss: 36.386684\n","Train Epoch: 28 [23040/60000 (38%)]\tLoss: 36.386951\n","Train Epoch: 28 [26880/60000 (45%)]\tLoss: 36.376034\n","Train Epoch: 28 [30720/60000 (51%)]\tLoss: 36.390385\n","Train Epoch: 28 [34560/60000 (58%)]\tLoss: 36.370914\n","Train Epoch: 28 [38400/60000 (64%)]\tLoss: 36.405521\n","Train Epoch: 28 [42240/60000 (70%)]\tLoss: 36.412376\n","Train Epoch: 28 [46080/60000 (77%)]\tLoss: 36.376312\n","Train Epoch: 28 [49920/60000 (83%)]\tLoss: 36.390339\n","Train Epoch: 28 [53760/60000 (90%)]\tLoss: 36.392586\n","Train Epoch: 28 [57600/60000 (96%)]\tLoss: 36.409981\n","\n","ON TRAINING SET:\n","Average loss: 36.3672, Accuracy: 59996/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4625, Accuracy: 9856/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 29 [0/60000 (0%)]\tLoss: 36.367386\n","Train Epoch: 29 [3840/60000 (6%)]\tLoss: 36.379017\n","Train Epoch: 29 [7680/60000 (13%)]\tLoss: 36.386189\n","Train Epoch: 29 [11520/60000 (19%)]\tLoss: 36.404396\n","Train Epoch: 29 [15360/60000 (26%)]\tLoss: 36.388271\n","Train Epoch: 29 [19200/60000 (32%)]\tLoss: 36.371754\n","Train Epoch: 29 [23040/60000 (38%)]\tLoss: 36.420036\n","Train Epoch: 29 [26880/60000 (45%)]\tLoss: 36.406940\n","Train Epoch: 29 [30720/60000 (51%)]\tLoss: 36.374222\n","Train Epoch: 29 [34560/60000 (58%)]\tLoss: 36.369621\n","Train Epoch: 29 [38400/60000 (64%)]\tLoss: 36.374611\n","Train Epoch: 29 [42240/60000 (70%)]\tLoss: 36.411732\n","Train Epoch: 29 [46080/60000 (77%)]\tLoss: 36.396732\n","Train Epoch: 29 [49920/60000 (83%)]\tLoss: 36.373692\n","Train Epoch: 29 [53760/60000 (90%)]\tLoss: 36.405186\n","Train Epoch: 29 [57600/60000 (96%)]\tLoss: 36.379780\n","\n","ON TRAINING SET:\n","Average loss: 36.3714, Accuracy: 59996/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4669, Accuracy: 9855/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 30 [0/60000 (0%)]\tLoss: 36.387772\n","Train Epoch: 30 [3840/60000 (6%)]\tLoss: 36.384190\n","Train Epoch: 30 [7680/60000 (13%)]\tLoss: 36.369823\n","Train Epoch: 30 [11520/60000 (19%)]\tLoss: 36.400387\n","Train Epoch: 30 [15360/60000 (26%)]\tLoss: 36.375900\n","Train Epoch: 30 [19200/60000 (32%)]\tLoss: 36.395096\n","Train Epoch: 30 [23040/60000 (38%)]\tLoss: 36.401749\n","Train Epoch: 30 [26880/60000 (45%)]\tLoss: 36.378651\n","Train Epoch: 30 [30720/60000 (51%)]\tLoss: 36.390694\n","Train Epoch: 30 [34560/60000 (58%)]\tLoss: 36.370075\n","Train Epoch: 30 [38400/60000 (64%)]\tLoss: 36.376587\n","Train Epoch: 30 [42240/60000 (70%)]\tLoss: 36.373653\n","Train Epoch: 30 [46080/60000 (77%)]\tLoss: 36.392700\n","Train Epoch: 30 [49920/60000 (83%)]\tLoss: 36.386482\n","Train Epoch: 30 [53760/60000 (90%)]\tLoss: 36.405529\n","Train Epoch: 30 [57600/60000 (96%)]\tLoss: 36.395069\n","\n","ON TRAINING SET:\n","Average loss: 36.3707, Accuracy: 59999/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4656, Accuracy: 9862/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 31 [0/60000 (0%)]\tLoss: 36.389992\n","Train Epoch: 31 [3840/60000 (6%)]\tLoss: 36.378849\n","Train Epoch: 31 [7680/60000 (13%)]\tLoss: 36.402687\n","Train Epoch: 31 [11520/60000 (19%)]\tLoss: 36.387589\n","Train Epoch: 31 [15360/60000 (26%)]\tLoss: 36.381954\n","Train Epoch: 31 [19200/60000 (32%)]\tLoss: 36.396179\n","Train Epoch: 31 [23040/60000 (38%)]\tLoss: 36.386753\n","Train Epoch: 31 [26880/60000 (45%)]\tLoss: 36.394737\n","Train Epoch: 31 [30720/60000 (51%)]\tLoss: 36.382080\n","Train Epoch: 31 [34560/60000 (58%)]\tLoss: 36.390671\n","Train Epoch: 31 [38400/60000 (64%)]\tLoss: 36.396641\n","Train Epoch: 31 [42240/60000 (70%)]\tLoss: 36.371300\n","Train Epoch: 31 [46080/60000 (77%)]\tLoss: 36.382751\n","Train Epoch: 31 [49920/60000 (83%)]\tLoss: 36.382851\n","Train Epoch: 31 [53760/60000 (90%)]\tLoss: 36.370476\n","Train Epoch: 31 [57600/60000 (96%)]\tLoss: 36.381851\n","\n","ON TRAINING SET:\n","Average loss: 36.3594, Accuracy: 59996/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4557, Accuracy: 9856/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 32 [0/60000 (0%)]\tLoss: 36.386585\n","Train Epoch: 32 [3840/60000 (6%)]\tLoss: 36.386845\n","Train Epoch: 32 [7680/60000 (13%)]\tLoss: 36.388821\n","Train Epoch: 32 [11520/60000 (19%)]\tLoss: 36.378017\n","Train Epoch: 32 [15360/60000 (26%)]\tLoss: 36.378223\n","Train Epoch: 32 [19200/60000 (32%)]\tLoss: 36.404121\n","Train Epoch: 32 [23040/60000 (38%)]\tLoss: 36.402695\n","Train Epoch: 32 [26880/60000 (45%)]\tLoss: 36.389236\n","Train Epoch: 32 [30720/60000 (51%)]\tLoss: 36.408882\n","Train Epoch: 32 [34560/60000 (58%)]\tLoss: 36.384418\n","Train Epoch: 32 [38400/60000 (64%)]\tLoss: 36.399063\n","Train Epoch: 32 [42240/60000 (70%)]\tLoss: 36.383099\n","Train Epoch: 32 [46080/60000 (77%)]\tLoss: 36.389427\n","Train Epoch: 32 [49920/60000 (83%)]\tLoss: 36.372280\n","Train Epoch: 32 [53760/60000 (90%)]\tLoss: 36.393711\n","Train Epoch: 32 [57600/60000 (96%)]\tLoss: 36.387604\n","\n","ON TRAINING SET:\n","Average loss: 36.3677, Accuracy: 59998/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4628, Accuracy: 9855/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 33 [0/60000 (0%)]\tLoss: 36.384369\n","Train Epoch: 33 [3840/60000 (6%)]\tLoss: 36.370262\n","Train Epoch: 33 [7680/60000 (13%)]\tLoss: 36.369232\n","Train Epoch: 33 [11520/60000 (19%)]\tLoss: 36.381569\n","Train Epoch: 33 [15360/60000 (26%)]\tLoss: 36.389805\n","Train Epoch: 33 [19200/60000 (32%)]\tLoss: 36.404678\n","Train Epoch: 33 [23040/60000 (38%)]\tLoss: 36.381042\n","Train Epoch: 33 [26880/60000 (45%)]\tLoss: 36.394379\n","Train Epoch: 33 [30720/60000 (51%)]\tLoss: 36.400421\n","Train Epoch: 33 [34560/60000 (58%)]\tLoss: 36.407055\n","Train Epoch: 33 [38400/60000 (64%)]\tLoss: 36.375977\n","Train Epoch: 33 [42240/60000 (70%)]\tLoss: 36.384258\n","Train Epoch: 33 [46080/60000 (77%)]\tLoss: 36.385761\n","Train Epoch: 33 [49920/60000 (83%)]\tLoss: 36.386036\n","Train Epoch: 33 [53760/60000 (90%)]\tLoss: 36.391735\n","Train Epoch: 33 [57600/60000 (96%)]\tLoss: 36.368202\n","\n","ON TRAINING SET:\n","Average loss: 36.3698, Accuracy: 59997/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4648, Accuracy: 9866/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 34 [0/60000 (0%)]\tLoss: 36.392937\n","Train Epoch: 34 [3840/60000 (6%)]\tLoss: 36.388424\n","Train Epoch: 34 [7680/60000 (13%)]\tLoss: 36.395470\n","Train Epoch: 34 [11520/60000 (19%)]\tLoss: 36.360397\n","Train Epoch: 34 [15360/60000 (26%)]\tLoss: 36.381157\n","Train Epoch: 34 [19200/60000 (32%)]\tLoss: 36.378506\n","Train Epoch: 34 [23040/60000 (38%)]\tLoss: 36.378113\n","Train Epoch: 34 [26880/60000 (45%)]\tLoss: 36.390873\n","Train Epoch: 34 [30720/60000 (51%)]\tLoss: 36.396500\n","Train Epoch: 34 [34560/60000 (58%)]\tLoss: 36.382603\n","Train Epoch: 34 [38400/60000 (64%)]\tLoss: 36.393227\n","Train Epoch: 34 [42240/60000 (70%)]\tLoss: 36.408550\n","Train Epoch: 34 [46080/60000 (77%)]\tLoss: 36.374584\n","Train Epoch: 34 [49920/60000 (83%)]\tLoss: 36.391396\n","Train Epoch: 34 [53760/60000 (90%)]\tLoss: 36.382774\n","Train Epoch: 34 [57600/60000 (96%)]\tLoss: 36.360054\n","\n","ON TRAINING SET:\n","Average loss: 36.3760, Accuracy: 59997/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4690, Accuracy: 9864/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 35 [0/60000 (0%)]\tLoss: 36.415077\n","Train Epoch: 35 [3840/60000 (6%)]\tLoss: 36.380638\n","Train Epoch: 35 [7680/60000 (13%)]\tLoss: 36.384827\n","Train Epoch: 35 [11520/60000 (19%)]\tLoss: 36.372604\n","Train Epoch: 35 [15360/60000 (26%)]\tLoss: 36.373260\n","Train Epoch: 35 [19200/60000 (32%)]\tLoss: 36.369225\n","Train Epoch: 35 [23040/60000 (38%)]\tLoss: 36.368706\n","Train Epoch: 35 [26880/60000 (45%)]\tLoss: 36.396584\n","Train Epoch: 35 [30720/60000 (51%)]\tLoss: 36.370533\n","Train Epoch: 35 [34560/60000 (58%)]\tLoss: 36.391567\n","Train Epoch: 35 [38400/60000 (64%)]\tLoss: 36.369766\n","Train Epoch: 35 [42240/60000 (70%)]\tLoss: 36.400520\n","Train Epoch: 35 [46080/60000 (77%)]\tLoss: 36.378792\n","Train Epoch: 35 [49920/60000 (83%)]\tLoss: 36.416756\n","Train Epoch: 35 [53760/60000 (90%)]\tLoss: 36.369984\n","Train Epoch: 35 [57600/60000 (96%)]\tLoss: 36.385807\n","\n","ON TRAINING SET:\n","Average loss: 36.3640, Accuracy: 59999/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4593, Accuracy: 9860/10000 (99%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 36 [0/60000 (0%)]\tLoss: 36.379009\n","Train Epoch: 36 [3840/60000 (6%)]\tLoss: 36.361706\n","Train Epoch: 36 [7680/60000 (13%)]\tLoss: 36.375061\n","Train Epoch: 36 [11520/60000 (19%)]\tLoss: 36.371662\n","Train Epoch: 36 [15360/60000 (26%)]\tLoss: 36.371319\n","Train Epoch: 36 [19200/60000 (32%)]\tLoss: 36.391693\n","Train Epoch: 36 [23040/60000 (38%)]\tLoss: 36.373146\n","Train Epoch: 36 [26880/60000 (45%)]\tLoss: 36.362579\n","Train Epoch: 36 [30720/60000 (51%)]\tLoss: 36.373810\n","Train Epoch: 36 [34560/60000 (58%)]\tLoss: 36.382668\n","Train Epoch: 36 [38400/60000 (64%)]\tLoss: 36.405064\n","Train Epoch: 36 [42240/60000 (70%)]\tLoss: 36.369766\n","Train Epoch: 36 [46080/60000 (77%)]\tLoss: 36.394157\n","Train Epoch: 36 [49920/60000 (83%)]\tLoss: 36.368134\n","Train Epoch: 36 [53760/60000 (90%)]\tLoss: 36.377251\n","Train Epoch: 36 [57600/60000 (96%)]\tLoss: 36.384411\n","\n","ON TRAINING SET:\n","Average loss: 36.3645, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 36.4569, Accuracy: 9869/10000 (99%)\n","\n","\n","\n","Yay! 🥳\n"]}],"source":["# Actual training\n","\n","lossfn = lambda x, y, **kwargs: CCQLLoss(x, y, 10, **kwargs)\n","\n","# GL-alpha stopping (outer preparation)\n","gl_alpha_value = 0.01\n","grace_epoch = 5\n","ceopt = np.inf\n","\n","for epoch in range(1, nrepochs + 1):\n","    print(\"TRAINING...\")\n","    train_epoch(\n","        model, device, train_loader, lossfn, optimizer, epoch, print_every_nep=30, inner_scheduler=None, quiet=False,\n","    )\n","    print(\"\\nON TRAINING SET:\")\n","    _, trainacc = test(model, device, test_on_train_loader, lossfn, quiet=False)\n","    print(\"\\nON TEST SET:\")\n","    testloss, _ = test(model, device, test_loader, lossfn, quiet=False)\n","    print(\"\\n\\n\")\n","\n","    # GL-alpha stopping (per-epoch)\n","    if testloss < ceopt:\n","        # Always set to the minimum testloss so far\n","        ceopt = testloss\n","\n","    curr_gl = testloss/ceopt - 1.0\n","\n","    if curr_gl > gl_alpha_value:\n","        if epoch > grace_epoch:\n","            break\n","    else:\n","        th.save(model, \"./glalpha_model_model.pt\")\n","    \n","    if trainacc == 1.0:\n","        # Crazily enough, this \"little thing\" may go ahead and overfit all of MNIST.\n","        # If so, acknowledge that and call it a day! Good job CCQL! :)\n","        #\n","        # P.S.: This exercise has been done AFTER the other specifically asking\n","        #       to overfit MNIST.\n","        print(\"Yay! \\U0001F973\")    # It's the Unicode for 🥳\n","        th.save(model, \"./glalpha_model_model.pt\")\n","        break\n","\n","# Load the best model\n","model = th.load(\"./glalpha_model_model.pt\")"]},{"cell_type":"markdown","metadata":{},"source":["### Request 2:"]},{"cell_type":"markdown","metadata":{},"source":["#### Extra\n","\n","The following is an implementation of an essential, but easy-to-use, class for the purpose of tracking running-metrics along so-called *epochs strips* (or *training strips*), as defined in [Prechelt, 97](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf).  \n","\n","Though of no use in the preceding (no *early stopping* criterion making use of *strips* has been implemented), it is shared here tor completeness."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["realnum = Union[float, int]\n","\n","class EasyStrip:\n","    def __init__(self, strip_size: Optional[realnum] = None) -> None:\n","        if strip_size is not None:\n","            queue_size: int = int(strip_size)\n","        self.strip_size: Optional[int] = strip_size\n","        self.strip_list: List[float] = []\n","    \n","    def append(self, element) -> None:\n","        if self.strip_size is not None:\n","            while len(self.strip_list) >= self.strip_size:\n","                _ = self.strip_list.pop()\n","        self.strip_list.insert(0, element)  # Executes only when len(self.strip_list) < self.strip_size\n","\n","    def make_empty(self) -> None:\n","        self.strip_list = []\n","\n","    def resize(self, strip_size: Optional[realnum] = None) -> None:\n","        if strip_size is not None:\n","            strip_size: int = int(strip_size)\n","        self.strip_size = strip_size\n","    \n","    def reset(self) -> None:\n","        self.make_empty()\n","        self.resize(strip_size=None)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.8 64-bit ('RDDL': conda)","name":"python388jvsc74a57bd0eb8633c4d4e251251708d3c7ece77ee33d393b5bf4628cd3b0e51f052595f5d6"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}