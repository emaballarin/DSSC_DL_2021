{"cells":[{"cell_type":"markdown","metadata":{"id":"WZnV3RNgU0Mi"},"source":["# Deep Learning Homework \\#03\n","### Deep Learning Course $\\in$ DSSC @ UniTS (Spring 2021)  \n","\n","#### Submitted by [Emanuele Ballarin](mailto:emanuele@ballarin.cc)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ra6EQvDLUxqG"},"outputs":[],"source":["# Type hints\n","from torch import Tensor\n","#from typing import Union, Optional\n","\n","# Just to force-load MKL (if available)\n","import numpy as np\n","\n","# Mathematical functions\n","from math import sqrt as msqrt\n","\n","# Neural networks and friends\n","import torch as th\n","from torch.nn import Sequential, BatchNorm1d, Linear, LogSoftmax, Dropout\n","import torch.nn.functional as F\n","\n","# Optimization and scheduling\n","from torch.optim.lr_scheduler import StepLR, MultiStepLR\n","\n","# Bespoke Modules / Functions / Optimizers\n","from ebtorch.logging import AverageMeter\n","from ebtorch.nn import Mish, mishlayer_init\n","from ebtorch.optim import Lookahead\n","from madgrad.madgrad import MADGRAD as MadGrad\n","\n","# Model summarization\n","from torchinfo import summary\n","\n","# Dataset handling for PyTorch\n","import os\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import MNIST\n","from torchvision.transforms import ToTensor, Normalize, Compose, Lambda"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANKugUNmUyFo"},"outputs":[],"source":["# MNIST DataLoader(s) builder\n","\n","def spawn_mnist_loaders(\n","    data_root=\"datasets/\",\n","    batch_size_train=256,\n","    batch_size_test=512,\n","    cuda_accel=False,\n","    **kwargs\n","):\n","\n","    os.makedirs(data_root, exist_ok=True)\n","\n","    transforms = Compose(\n","        [\n","            ToTensor(),\n","            Normalize((0.1307,), (0.3081,)),  # usual normalization constants for MNIST\n","            Lambda(lambda x: th.flatten(x)),\n","        ]\n","    )\n","\n","    trainset = MNIST(data_root, train=True, transform=transforms, download=True)\n","    testset = MNIST(data_root, train=False, transform=transforms, download=True)\n","\n","    cuda_args = {}\n","    if cuda_accel:\n","        cuda_args = {\"num_workers\": 1, \"pin_memory\": True}\n","\n","    trainloader = DataLoader(\n","        trainset, batch_size=batch_size_train, shuffle=True, **cuda_args\n","    )\n","    testloader = DataLoader(\n","        testset, batch_size=batch_size_test, shuffle=False, **cuda_args\n","    )\n","    tontrloader = DataLoader(   # tontr == test on train\n","        trainset, batch_size=batch_size_test, shuffle=False, **cuda_args\n","    )\n","\n","    return trainloader, testloader, tontrloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0xgs3EZBUyJ4"},"outputs":[],"source":["# Train / Test tooling\n","\n","train_acc_avgmeter = AverageMeter(\"Training Loss\")\n","\n","def train_epoch(\n","    model, device, train_loader, loss_fn, optimizer, epoch, print_every_nep, inner_scheduler=None, quiet=False,\n","):\n","    train_acc_avgmeter.reset()\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = loss_fn(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if inner_scheduler is not None:\n","            inner_scheduler.step()\n","        \n","        train_acc_avgmeter.update(loss.item())\n","\n","        if not quiet and batch_idx % print_every_nep == 0:\n","            print(\n","                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tAvg. loss: {:.6f}\".format(\n","                    epoch,\n","                    batch_idx * len(data),\n","                    len(train_loader.dataset),\n","                    100.0 * batch_idx / len(train_loader),\n","                    train_acc_avgmeter.avg\n","                )\n","            )\n","\n","\n","def test(model, device, test_loader, loss_fn, quiet=False):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with th.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += loss_fn(\n","                output, target, reduction=\"sum\"\n","            ).item()  # sum up batch loss\n","            pred = output.argmax(\n","                dim=1, keepdim=True\n","            )  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    ltlds = len(test_loader.dataset)\n","\n","    test_loss /= ltlds\n","    \n","    if not quiet:\n","        print(\n","            \"Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n","                test_loss,\n","                correct,\n","                ltlds,\n","                100.0 * correct / ltlds,\n","            )\n","        )\n","    \n","    return test_loss, correct / ltlds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2aclwpGUyOP"},"outputs":[],"source":["device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PtJNzN33cX_e"},"outputs":[],"source":["# Hyperparameters & co.\n","\n","minibatch_size_train: int = 512 # I know it's high; I just want a \"little\" more stability\n","minibatch_size_test: int = 512\n","\n","nrepochs = 12\n","\n","lossfn = F.nll_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N8YTl9NTUySq"},"outputs":[],"source":["train_loader, test_loader, test_on_train_loader = spawn_mnist_loaders(\n","    batch_size_train=minibatch_size_train,\n","    batch_size_test=minibatch_size_test,\n","    cuda_accel=bool(device == \"cuda\"),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = Sequential(\n","    # -> Input is here <-\n","\n","    # POST-INPUT BLOCK:\n","    Linear(in_features=28*28, out_features=1500, bias=True),    # Hyperfeaturize ~2*input\n","    Mish(),\n","\n","    # HIDDEN BLOCK:\n","    BatchNorm1d(num_features=1500, affine=True),\n","    Linear(in_features=1500, out_features=500, bias=True),      # Compress ~0.75*input\n","    Mish(),\n","\n","    # PRE-OUTPUT BLOCK:\n","    BatchNorm1d(num_features=500, affine=True),\n","    Linear(in_features=500, out_features=10, bias=True),        # To output\n","    LogSoftmax(dim=1)\n","\n","    # -> Output is here <-\n","        ).to(device)\n","\n","base_optimizer = MadGrad(model.parameters(), lr=0.00017)\n","optimizer      = Lookahead(base_optimizer, la_steps=4)\n","scheduler      = MultiStepLR(optimizer, milestones=[10, 11], gamma=0.4)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize weights and biases in the proper way ;)\n","for layr in model:\n","    mishlayer_init(layr)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["summary(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in range(1, nrepochs + 1):\n","\n","    # Training\n","    print(\"TRAINING...\")\n","    train_epoch(\n","        model, device, train_loader, lossfn, optimizer, epoch, print_every_nep=15, inner_scheduler=None, quiet=False,\n","    )\n","\n","    # Tweaks for the Lookahead optimizer (before testing)\n","    if isinstance(optimizer, Lookahead):\n","        optimizer._backup_and_load_cache()\n","\n","    # Testing: on training and testing set\n","    print(\"\\nON TRAINING SET:\")\n","    _ = test(model, device, test_on_train_loader, lossfn, quiet=False)\n","    print(\"\\nON TEST SET:\")\n","    _ = test(model, device, test_loader, lossfn, quiet=False)\n","    print(\"\\n\\n\")\n","\n","    # Tweaks for the Lookahead optimizer (after testing)\n","    if isinstance(optimizer, Lookahead):\n","        optimizer._clear_and_load_backup()\n","    \n","    # Scheduling step (outer)\n","    scheduler.step()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"softmax_mnist.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('RDDL': conda)","name":"python388jvsc74a57bd0eb8633c4d4e251251708d3c7ece77ee33d393b5bf4628cd3b0e51f052595f5d6"},"language_info":{"name":"python","version":""}},"nbformat":4,"nbformat_minor":0}