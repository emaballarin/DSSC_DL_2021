{"cells": [{"cell_type": "markdown", "metadata": {"id": "WZnV3RNgU0Mi"}, "source": ["# Training a *1-layer SoftMax classifier* on the *MNIST* dataset\n", "\n", "In this brief experiment, we will test the accuracy achieved by a simple *logistic regression model* (a.k.a. *SoftMax classifier*) on the MNIST dataset. Layer weights are determined via *vanilla-SGD*-optimized error backpropagation.  \n", "Given such tight bounds, all remaining hyperparameters can be tuned as to maximize accuracy."]}, {"cell_type": "code", "execution_count": 1, "metadata": {"id": "Ra6EQvDLUxqG"}, "outputs": [], "source": ["# Data download:\n", "import os\n", "\n", "# NNets & co.:\n", "import numpy as np\n", "import torch as th\n", "import torch.nn.functional as F\n", "from torch.optim.lr_scheduler import StepLR\n", "\n", "# Data(set) handling\n", "from torch.utils.data import DataLoader\n", "from torchvision.datasets import MNIST\n", "from torchvision.transforms import ToTensor, Normalize, Compose, Lambda\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"id": "ANKugUNmUyFo"}, "outputs": [], "source": ["# MNIST DataLoader(s) builder\n", "\n", "\n", "def spawn_mnist_loaders(\n", "    data_root=\"datasets/\",\n", "    batch_size_train=256,\n", "    batch_size_test=512,\n", "    cuda_accel=False,\n", "    **kwargs\n", "):\n", "\n", "    os.makedirs(data_root, exist_ok=True)\n", "\n", "    transforms = Compose(\n", "        [\n", "            ToTensor(),\n", "            Normalize((0.1307,), (0.3081,)),  # usual magic constants for MNIST\n", "            Lambda(lambda x: th.flatten(x)),\n", "        ]\n", "    )\n", "\n", "    trainset = MNIST(data_root, train=True, transform=transforms, download=True)\n", "    testset = MNIST(data_root, train=False, transform=transforms, download=True)\n", "\n", "    cuda_args = {}\n", "    if cuda_accel:\n", "        cuda_args = {\"num_workers\": 1, \"pin_memory\": True}\n", "\n", "    trainloader = DataLoader(\n", "        trainset, batch_size=batch_size_train, shuffle=True, **cuda_args\n", "    )\n", "    testloader = DataLoader(\n", "        trainset, batch_size=batch_size_test, shuffle=False, **cuda_args\n", "    )\n", "\n", "    return trainloader, testloader\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"id": "ADD28-fDUyHw"}, "outputs": [], "source": ["# Parameterized SoftMax Classifier\n", "\n", "\n", "class SMC(th.nn.Module):\n", "    def __init__(self, fin: int, fout: int):\n", "        super().__init__()\n", "        self.layer1 = th.nn.Linear(in_features=fin, out_features=fout)\n", "\n", "    def forward(self, X):\n", "        out = self.layer1(X)\n", "        out = F.log_softmax(out, dim=1)\n", "        return out\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"id": "0xgs3EZBUyJ4"}, "outputs": [], "source": ["# Train / Test tooling\n", "\n", "\n", "def train_epoch(\n", "    model, device, train_loader, loss_fn, optimizer, epoch, print_every_nep\n", "):\n", "    model.train()\n", "    for batch_idx, (data, target) in enumerate(train_loader):\n", "        data, target = data.to(device), target.to(device)\n", "        optimizer.zero_grad()\n", "        output = model(data)\n", "        loss = loss_fn(output, target)\n", "        loss.backward()\n", "        optimizer.step()\n", "        if batch_idx % print_every_nep == 0:\n", "            print(\n", "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n", "                    epoch,\n", "                    batch_idx * len(data),\n", "                    len(train_loader.dataset),\n", "                    100.0 * batch_idx / len(train_loader),\n", "                    loss.item(),\n", "                )\n", "            )\n", "\n", "\n", "def test(model, device, test_loader, loss_fn):\n", "    model.eval()\n", "    test_loss = 0\n", "    correct = 0\n", "    with th.no_grad():\n", "        for data, target in test_loader:\n", "            data, target = data.to(device), target.to(device)\n", "            output = model(data)\n", "            test_loss += loss_fn(\n", "                output, target, reduction=\"sum\"\n", "            ).item()  # sum up batch loss\n", "            pred = output.argmax(\n", "                dim=1, keepdim=True\n", "            )  # get the index of the max log-probability\n", "            correct += pred.eq(target.view_as(pred)).sum().item()\n", "\n", "    test_loss /= len(test_loader.dataset)\n", "\n", "    print(\n", "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n", "            test_loss,\n", "            correct,\n", "            len(test_loader.dataset),\n", "            100.0 * correct / len(test_loader.dataset),\n", "        )\n", "    )\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"id": "i2aclwpGUyOP"}, "outputs": [], "source": ["device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"id": "PtJNzN33cX_e"}, "outputs": [], "source": ["# Hyperparameters & co.\n", "\n", "minibatch_size_train: int = 32  # (cfr. Masters & Luschi, 2018)\n", "minibatch_size_test: int = 512\n", "\n", "nrepochs = 20\n", "\n", "lossfn = F.nll_loss\n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"id": "N8YTl9NTUySq"}, "outputs": [], "source": ["train_loader, test_loader = spawn_mnist_loaders(\n", "    batch_size_train=minibatch_size_test,\n", "    batch_size_test=minibatch_size_test,\n", "    cuda_accel=True,\n", ")\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"id": "A5QVDGBfUyW7"}, "outputs": [], "source": ["model = SMC(28 * 28, 10).to(device)\n", "optimizer = th.optim.SGD(model.parameters(), lr=0.7)\n", "scheduler = StepLR(optimizer, step_size=1, gamma=0.75)  # or a lot of patience :)\n"]}, {"cell_type": "code", "execution_count": 9, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Cda0zuTDUyZQ", "outputId": "27fbb0b9-0bc2-453b-a082-66a35674e9f0"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.408727\n", "Train Epoch: 1 [5120/60000 (8%)]\tLoss: 1.599636\n", "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.097477\n", "Train Epoch: 1 [15360/60000 (25%)]\tLoss: 1.587394\n", "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.427219\n", "Train Epoch: 1 [25600/60000 (42%)]\tLoss: 0.690546\n", "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 3.272240\n", "Train Epoch: 1 [35840/60000 (59%)]\tLoss: 0.916544\n", "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 2.618241\n", "Train Epoch: 1 [46080/60000 (76%)]\tLoss: 0.598979\n", "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.626734\n", "Train Epoch: 1 [56320/60000 (93%)]\tLoss: 0.543361\n", "\n", "Test set: Average loss: 2.1848, Accuracy: 45735/60000 (76%)\n", "\n", "\n", "Test set: Average loss: 2.1848, Accuracy: 45735/60000 (76%)\n", "\n", "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.336122\n", "Train Epoch: 2 [5120/60000 (8%)]\tLoss: 0.508835\n", "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.386824\n", "Train Epoch: 2 [15360/60000 (25%)]\tLoss: 0.520472\n", "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.927284\n", "Train Epoch: 2 [25600/60000 (42%)]\tLoss: 0.590420\n", "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.517405\n", "Train Epoch: 2 [35840/60000 (59%)]\tLoss: 0.337604\n", "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.552233\n", "Train Epoch: 2 [46080/60000 (76%)]\tLoss: 0.428812\n", "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.631749\n", "Train Epoch: 2 [56320/60000 (93%)]\tLoss: 2.473611\n", "\n", "Test set: Average loss: 0.6192, Accuracy: 52009/60000 (87%)\n", "\n", "\n", "Test set: Average loss: 0.6192, Accuracy: 52009/60000 (87%)\n", "\n", "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.642502\n", "Train Epoch: 3 [5120/60000 (8%)]\tLoss: 0.352148\n", "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.464355\n", "Train Epoch: 3 [15360/60000 (25%)]\tLoss: 0.446786\n", "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.315201\n", "Train Epoch: 3 [25600/60000 (42%)]\tLoss: 0.343800\n", "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.433786\n", "Train Epoch: 3 [35840/60000 (59%)]\tLoss: 0.346691\n", "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.421714\n", "Train Epoch: 3 [46080/60000 (76%)]\tLoss: 0.310368\n", "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.657718\n", "Train Epoch: 3 [56320/60000 (93%)]\tLoss: 0.373162\n", "\n", "Test set: Average loss: 0.3450, Accuracy: 54925/60000 (92%)\n", "\n", "\n", "Test set: Average loss: 0.3450, Accuracy: 54925/60000 (92%)\n", "\n", "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.426312\n", "Train Epoch: 4 [5120/60000 (8%)]\tLoss: 0.309970\n", "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.278036\n", "Train Epoch: 4 [15360/60000 (25%)]\tLoss: 0.257540\n", "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.271585\n", "Train Epoch: 4 [25600/60000 (42%)]\tLoss: 0.371356\n", "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.329072\n", "Train Epoch: 4 [35840/60000 (59%)]\tLoss: 0.316906\n", "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.340257\n", "Train Epoch: 4 [46080/60000 (76%)]\tLoss: 0.235375\n", "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.250413\n", "Train Epoch: 4 [56320/60000 (93%)]\tLoss: 0.393616\n", "\n", "Test set: Average loss: 0.2966, Accuracy: 55227/60000 (92%)\n", "\n", "\n", "Test set: Average loss: 0.2966, Accuracy: 55227/60000 (92%)\n", "\n", "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.252253\n", "Train Epoch: 5 [5120/60000 (8%)]\tLoss: 0.300670\n", "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.320572\n", "Train Epoch: 5 [15360/60000 (25%)]\tLoss: 0.211737\n", "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.282587\n", "Train Epoch: 5 [25600/60000 (42%)]\tLoss: 0.234704\n", "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.313939\n", "Train Epoch: 5 [35840/60000 (59%)]\tLoss: 0.257795\n", "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.308993\n", "Train Epoch: 5 [46080/60000 (76%)]\tLoss: 0.274697\n", "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.331116\n", "Train Epoch: 5 [56320/60000 (93%)]\tLoss: 0.246911\n", "\n", "Test set: Average loss: 0.2672, Accuracy: 55666/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2672, Accuracy: 55666/60000 (93%)\n", "\n", "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.335880\n", "Train Epoch: 6 [5120/60000 (8%)]\tLoss: 0.280157\n", "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.249837\n", "Train Epoch: 6 [15360/60000 (25%)]\tLoss: 0.234991\n", "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.237107\n", "Train Epoch: 6 [25600/60000 (42%)]\tLoss: 0.335038\n", "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.247825\n", "Train Epoch: 6 [35840/60000 (59%)]\tLoss: 0.280874\n", "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.191396\n", "Train Epoch: 6 [46080/60000 (76%)]\tLoss: 0.327989\n", "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.305349\n", "Train Epoch: 6 [56320/60000 (93%)]\tLoss: 0.215920\n", "\n", "Test set: Average loss: 0.2704, Accuracy: 55585/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2704, Accuracy: 55585/60000 (93%)\n", "\n", "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.251642\n", "Train Epoch: 7 [5120/60000 (8%)]\tLoss: 0.213513\n", "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.251808\n", "Train Epoch: 7 [15360/60000 (25%)]\tLoss: 0.213969\n", "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.390113\n", "Train Epoch: 7 [25600/60000 (42%)]\tLoss: 0.286337\n", "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.221979\n", "Train Epoch: 7 [35840/60000 (59%)]\tLoss: 0.293472\n", "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.272849\n", "Train Epoch: 7 [46080/60000 (76%)]\tLoss: 0.288520\n", "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.310335\n", "Train Epoch: 7 [56320/60000 (93%)]\tLoss: 0.244193\n", "\n", "Test set: Average loss: 0.2563, Accuracy: 55803/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2563, Accuracy: 55803/60000 (93%)\n", "\n", "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.222585\n", "Train Epoch: 8 [5120/60000 (8%)]\tLoss: 0.279018\n", "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.294199\n", "Train Epoch: 8 [15360/60000 (25%)]\tLoss: 0.226113\n", "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.274955\n", "Train Epoch: 8 [25600/60000 (42%)]\tLoss: 0.235353\n", "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.273470\n", "Train Epoch: 8 [35840/60000 (59%)]\tLoss: 0.242228\n", "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.284284\n", "Train Epoch: 8 [46080/60000 (76%)]\tLoss: 0.177724\n", "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.240366\n", "Train Epoch: 8 [56320/60000 (93%)]\tLoss: 0.246822\n", "\n", "Test set: Average loss: 0.2554, Accuracy: 55855/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2554, Accuracy: 55855/60000 (93%)\n", "\n", "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.334525\n", "Train Epoch: 9 [5120/60000 (8%)]\tLoss: 0.327787\n", "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.241783\n", "Train Epoch: 9 [15360/60000 (25%)]\tLoss: 0.216689\n", "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.225229\n", "Train Epoch: 9 [25600/60000 (42%)]\tLoss: 0.224827\n", "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.284185\n", "Train Epoch: 9 [35840/60000 (59%)]\tLoss: 0.261631\n", "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.284478\n", "Train Epoch: 9 [46080/60000 (76%)]\tLoss: 0.199262\n", "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.360386\n", "Train Epoch: 9 [56320/60000 (93%)]\tLoss: 0.234994\n", "\n", "Test set: Average loss: 0.2526, Accuracy: 55861/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2526, Accuracy: 55861/60000 (93%)\n", "\n", "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.281763\n", "Train Epoch: 10 [5120/60000 (8%)]\tLoss: 0.201551\n", "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.254850\n", "Train Epoch: 10 [15360/60000 (25%)]\tLoss: 0.217167\n", "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.238605\n", "Train Epoch: 10 [25600/60000 (42%)]\tLoss: 0.251810\n", "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.202196\n", "Train Epoch: 10 [35840/60000 (59%)]\tLoss: 0.298837\n", "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.250671\n", "Train Epoch: 10 [46080/60000 (76%)]\tLoss: 0.297911\n", "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.239781\n", "Train Epoch: 10 [56320/60000 (93%)]\tLoss: 0.211061\n", "\n", "Test set: Average loss: 0.2521, Accuracy: 55883/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2521, Accuracy: 55883/60000 (93%)\n", "\n", "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.203200\n", "Train Epoch: 11 [5120/60000 (8%)]\tLoss: 0.235896\n", "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.197390\n", "Train Epoch: 11 [15360/60000 (25%)]\tLoss: 0.222265\n", "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.235139\n", "Train Epoch: 11 [25600/60000 (42%)]\tLoss: 0.229837\n", "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.225266\n", "Train Epoch: 11 [35840/60000 (59%)]\tLoss: 0.283380\n", "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.274482\n", "Train Epoch: 11 [46080/60000 (76%)]\tLoss: 0.239856\n", "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.265640\n", "Train Epoch: 11 [56320/60000 (93%)]\tLoss: 0.265853\n", "\n", "Test set: Average loss: 0.2501, Accuracy: 55895/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2501, Accuracy: 55895/60000 (93%)\n", "\n", "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.302785\n", "Train Epoch: 12 [5120/60000 (8%)]\tLoss: 0.207004\n", "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.329819\n", "Train Epoch: 12 [15360/60000 (25%)]\tLoss: 0.189696\n", "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.248807\n", "Train Epoch: 12 [25600/60000 (42%)]\tLoss: 0.229621\n", "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.196794\n", "Train Epoch: 12 [35840/60000 (59%)]\tLoss: 0.285670\n", "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.191989\n", "Train Epoch: 12 [46080/60000 (76%)]\tLoss: 0.290510\n", "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.284765\n", "Train Epoch: 12 [56320/60000 (93%)]\tLoss: 0.239598\n", "\n", "Test set: Average loss: 0.2498, Accuracy: 55940/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2498, Accuracy: 55940/60000 (93%)\n", "\n", "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.296801\n", "Train Epoch: 13 [5120/60000 (8%)]\tLoss: 0.236547\n", "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.262679\n", "Train Epoch: 13 [15360/60000 (25%)]\tLoss: 0.288431\n", "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.193326\n", "Train Epoch: 13 [25600/60000 (42%)]\tLoss: 0.291113\n", "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.265122\n", "Train Epoch: 13 [35840/60000 (59%)]\tLoss: 0.290005\n", "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.181895\n", "Train Epoch: 13 [46080/60000 (76%)]\tLoss: 0.259161\n", "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.354097\n", "Train Epoch: 13 [56320/60000 (93%)]\tLoss: 0.219549\n", "\n", "Test set: Average loss: 0.2493, Accuracy: 55906/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2493, Accuracy: 55906/60000 (93%)\n", "\n", "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.240383\n", "Train Epoch: 14 [5120/60000 (8%)]\tLoss: 0.272087\n", "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.236147\n", "Train Epoch: 14 [15360/60000 (25%)]\tLoss: 0.290487\n", "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.315364\n", "Train Epoch: 14 [25600/60000 (42%)]\tLoss: 0.210752\n", "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.232842\n", "Train Epoch: 14 [35840/60000 (59%)]\tLoss: 0.289602\n", "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.245984\n", "Train Epoch: 14 [46080/60000 (76%)]\tLoss: 0.280898\n", "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.220486\n", "Train Epoch: 14 [56320/60000 (93%)]\tLoss: 0.334575\n", "\n", "Test set: Average loss: 0.2487, Accuracy: 55937/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2487, Accuracy: 55937/60000 (93%)\n", "\n", "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.245430\n", "Train Epoch: 15 [5120/60000 (8%)]\tLoss: 0.218481\n", "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.286211\n", "Train Epoch: 15 [15360/60000 (25%)]\tLoss: 0.273744\n", "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.261384\n", "Train Epoch: 15 [25600/60000 (42%)]\tLoss: 0.241235\n", "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.275530\n", "Train Epoch: 15 [35840/60000 (59%)]\tLoss: 0.212370\n", "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.246870\n", "Train Epoch: 15 [46080/60000 (76%)]\tLoss: 0.240476\n", "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.247391\n", "Train Epoch: 15 [56320/60000 (93%)]\tLoss: 0.252921\n", "\n", "Test set: Average loss: 0.2486, Accuracy: 55944/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2486, Accuracy: 55944/60000 (93%)\n", "\n", "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.260347\n", "Train Epoch: 16 [5120/60000 (8%)]\tLoss: 0.280913\n", "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.196765\n", "Train Epoch: 16 [15360/60000 (25%)]\tLoss: 0.224422\n", "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.291463\n", "Train Epoch: 16 [25600/60000 (42%)]\tLoss: 0.223319\n", "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.234738\n", "Train Epoch: 16 [35840/60000 (59%)]\tLoss: 0.272523\n", "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.243524\n", "Train Epoch: 16 [46080/60000 (76%)]\tLoss: 0.282271\n", "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.220864\n", "Train Epoch: 16 [56320/60000 (93%)]\tLoss: 0.346046\n", "\n", "Test set: Average loss: 0.2485, Accuracy: 55947/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2485, Accuracy: 55947/60000 (93%)\n", "\n", "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.240008\n", "Train Epoch: 17 [5120/60000 (8%)]\tLoss: 0.240274\n", "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.331140\n", "Train Epoch: 17 [15360/60000 (25%)]\tLoss: 0.257364\n", "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.233215\n", "Train Epoch: 17 [25600/60000 (42%)]\tLoss: 0.208424\n", "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.335331\n", "Train Epoch: 17 [35840/60000 (59%)]\tLoss: 0.305398\n", "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.212644\n", "Train Epoch: 17 [46080/60000 (76%)]\tLoss: 0.196147\n", "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.207975\n", "Train Epoch: 17 [56320/60000 (93%)]\tLoss: 0.212033\n", "\n", "Test set: Average loss: 0.2483, Accuracy: 55953/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2483, Accuracy: 55953/60000 (93%)\n", "\n", "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.202168\n", "Train Epoch: 18 [5120/60000 (8%)]\tLoss: 0.310644\n", "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 0.255969\n", "Train Epoch: 18 [15360/60000 (25%)]\tLoss: 0.252786\n", "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 0.302990\n", "Train Epoch: 18 [25600/60000 (42%)]\tLoss: 0.209136\n", "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 0.330426\n", "Train Epoch: 18 [35840/60000 (59%)]\tLoss: 0.245553\n", "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 0.235219\n", "Train Epoch: 18 [46080/60000 (76%)]\tLoss: 0.257457\n", "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.249078\n", "Train Epoch: 18 [56320/60000 (93%)]\tLoss: 0.270562\n", "\n", "Test set: Average loss: 0.2483, Accuracy: 55956/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2483, Accuracy: 55956/60000 (93%)\n", "\n", "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.289760\n", "Train Epoch: 19 [5120/60000 (8%)]\tLoss: 0.238685\n", "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 0.249098\n", "Train Epoch: 19 [15360/60000 (25%)]\tLoss: 0.218893\n", "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 0.295245\n", "Train Epoch: 19 [25600/60000 (42%)]\tLoss: 0.335269\n", "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 0.275119\n", "Train Epoch: 19 [35840/60000 (59%)]\tLoss: 0.279259\n", "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 0.330670\n", "Train Epoch: 19 [46080/60000 (76%)]\tLoss: 0.256646\n", "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.172941\n", "Train Epoch: 19 [56320/60000 (93%)]\tLoss: 0.265512\n", "\n", "Test set: Average loss: 0.2482, Accuracy: 55948/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2482, Accuracy: 55948/60000 (93%)\n", "\n", "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.216848\n", "Train Epoch: 20 [5120/60000 (8%)]\tLoss: 0.173934\n", "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 0.227009\n", "Train Epoch: 20 [15360/60000 (25%)]\tLoss: 0.288362\n", "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 0.214005\n", "Train Epoch: 20 [25600/60000 (42%)]\tLoss: 0.326166\n", "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 0.207580\n", "Train Epoch: 20 [35840/60000 (59%)]\tLoss: 0.243645\n", "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 0.236824\n", "Train Epoch: 20 [46080/60000 (76%)]\tLoss: 0.272087\n", "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.250909\n", "Train Epoch: 20 [56320/60000 (93%)]\tLoss: 0.191350\n", "\n", "Test set: Average loss: 0.2482, Accuracy: 55951/60000 (93%)\n", "\n", "\n", "Test set: Average loss: 0.2482, Accuracy: 55951/60000 (93%)\n", "\n"]}], "source": ["for epoch in range(1, nrepochs + 1):\n", "    train_epoch(\n", "        model, device, train_loader, lossfn, optimizer, epoch, print_every_nep=10\n", "    )\n", "    test(model, device, test_loader, lossfn)\n", "    test(model, device, test_loader, lossfn)\n", "    scheduler.step()\n"]}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": [], "name": "softmax_mnist.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3.8.8 64-bit ('RDDL': virtualenv)", "name": "python388jvsc74a57bd0eb8633c4d4e251251708d3c7ece77ee33d393b5bf4628cd3b0e51f052595f5d6"}, "language_info": {"version": "3.8.8-final"}}, "nbformat": 4, "nbformat_minor": 0}