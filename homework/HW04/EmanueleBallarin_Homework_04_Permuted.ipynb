{"cells":[{"cell_type":"markdown","metadata":{"id":"WZnV3RNgU0Mi"},"source":["# Deep Learning Homework \\#03\n","### Deep Learning Course $\\in$ DSSC @ UniTS (Spring 2021)  \n","\n","#### Submitted by [Emanuele Ballarin](mailto:emanuele@ballarin.cc)  "]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Ra6EQvDLUxqG"},"outputs":[],"source":["# Type hints\n","from torch import Tensor\n","#from typing import Union, Optional\n","\n","# Just to force-load MKL (if available)\n","import numpy as np\n","\n","# Mathematical functions\n","from math import sqrt as msqrt\n","\n","# Neural networks and friends\n","import torch as th\n","from torch.nn import Sequential, BatchNorm1d, Linear, LogSoftmax, Dropout\n","import torch.nn.functional as F\n","\n","# Optimization and scheduling\n","from torch.optim.lr_scheduler import StepLR, MultiStepLR\n","\n","# Bespoke Modules / Functions / Optimizers\n","from ebtorch.logging import AverageMeter\n","from ebtorch.nn import Mish, mishlayer_init\n","from ebtorch.optim import Lookahead\n","from madgrad.madgrad import MADGRAD as MadGrad\n","\n","# Model summarization\n","from torchinfo import summary\n","\n","# Dataset handling for PyTorch\n","import os\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import MNIST\n","from torchvision.transforms import ToTensor, Normalize, Compose, Lambda"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ANKugUNmUyFo"},"outputs":[],"source":["# MNIST DataLoader(s) builder\n","\n","def spawn_mnist_loaders(\n","    data_root=\"datasets/\",\n","    batch_size_train=256,\n","    batch_size_test=512,\n","    cuda_accel=False,\n","    **kwargs\n","):\n","\n","    os.makedirs(data_root, exist_ok=True)\n","\n","    transforms = Compose(\n","        [\n","            ToTensor(),\n","            Normalize((0.1307,), (0.3081,)),  # usual normalization constants for MNIST\n","            Lambda(lambda x: th.flatten(x)),\n","        ]\n","    )\n","\n","    trainset = MNIST(data_root, train=True, transform=transforms, download=True)\n","    testset = MNIST(data_root, train=False, transform=transforms, download=True)\n","\n","    # Permute trainset.targets\n","    idx = th.randperm(trainset.targets.nelement())\n","    trainset.targets = trainset.targets.view(-1)[idx].view(trainset.targets.size())\n","\n","    cuda_args = {}\n","    if cuda_accel:\n","        cuda_args = {\"num_workers\": 1, \"pin_memory\": True}\n","\n","    trainloader = DataLoader(\n","        trainset, batch_size=batch_size_train, shuffle=True, **cuda_args\n","    )\n","    testloader = DataLoader(\n","        testset, batch_size=batch_size_test, shuffle=False, **cuda_args\n","    )\n","    tontrloader = DataLoader(   # tontr == test on train\n","        trainset, batch_size=batch_size_test, shuffle=False, **cuda_args\n","    )\n","\n","    return trainloader, testloader, tontrloader"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"0xgs3EZBUyJ4"},"outputs":[],"source":["# Train / Test tooling\n","\n","train_acc_avgmeter = AverageMeter(\"Training Loss\")\n","\n","def train_epoch(\n","    model, device, train_loader, loss_fn, optimizer, epoch, print_every_nep, inner_scheduler=None, quiet=False,\n","):\n","    train_acc_avgmeter.reset()\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = loss_fn(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if inner_scheduler is not None:\n","            inner_scheduler.step()\n","        \n","        train_acc_avgmeter.update(loss.item())\n","\n","        if not quiet and batch_idx % print_every_nep == 0:\n","            print(\n","                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tAvg. loss: {:.6f}\".format(\n","                    epoch,\n","                    batch_idx * len(data),\n","                    len(train_loader.dataset),\n","                    100.0 * batch_idx / len(train_loader),\n","                    train_acc_avgmeter.avg\n","                )\n","            )\n","\n","\n","def test(model, device, test_loader, loss_fn, quiet=False):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with th.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += loss_fn(\n","                output, target, reduction=\"sum\"\n","            ).item()  # sum up batch loss\n","            pred = output.argmax(\n","                dim=1, keepdim=True\n","            )  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    ltlds = len(test_loader.dataset)\n","\n","    test_loss /= ltlds\n","    \n","    if not quiet:\n","        print(\n","            \"Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n","                test_loss,\n","                correct,\n","                ltlds,\n","                100.0 * correct / ltlds,\n","            )\n","        )\n","    \n","    return test_loss, correct / ltlds"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"i2aclwpGUyOP"},"outputs":[],"source":["device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"PtJNzN33cX_e"},"outputs":[],"source":["# Hyperparameters & co.\n","\n","minibatch_size_train: int = 1024 # I know it's high; I just want a \"little\" more stability\n","minibatch_size_test: int = 512\n","\n","nrepochs = 90\n","\n","lossfn = F.nll_loss"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"N8YTl9NTUySq"},"outputs":[],"source":["train_loader, test_loader, test_on_train_loader = spawn_mnist_loaders(\n","    batch_size_train=minibatch_size_train,\n","    batch_size_test=minibatch_size_test,\n","    cuda_accel=bool(device == \"cuda\"),\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["model = Sequential(\n","    # -> Input is here <-\n","\n","    # POST-INPUT BLOCK:\n","    Linear(in_features=28*28, out_features=1500, bias=True),    # Hyperfeaturize ~2*input\n","    Mish(),\n","\n","    # HIDDEN BLOCK:\n","    BatchNorm1d(num_features=1500, affine=True),\n","    Linear(in_features=1500, out_features=500, bias=True),      # Compress ~0.75*input\n","    Mish(),\n","\n","    # PRE-OUTPUT BLOCK:\n","    BatchNorm1d(num_features=500, affine=True),\n","    Linear(in_features=500, out_features=10, bias=True),        # To output\n","    LogSoftmax(dim=1)\n","\n","    # -> Output is here <-\n","        ).to(device)\n","\n","base_optimizer = MadGrad(model.parameters(), lr=0.00017)\n","optimizer      = Lookahead(base_optimizer, la_steps=4)\n","scheduler      = MultiStepLR(optimizer, milestones=[], gamma=0.4)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Initialize weights and biases in the proper way ;)\n","for layr in model:\n","    mishlayer_init(layr)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":"=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─Linear: 1-1                            1,177,500\n├─Mish: 1-2                              --\n├─BatchNorm1d: 1-3                       3,000\n├─Linear: 1-4                            750,500\n├─Mish: 1-5                              --\n├─BatchNorm1d: 1-6                       1,000\n├─Linear: 1-7                            5,010\n├─LogSoftmax: 1-8                        --\n=================================================================\nTotal params: 1,937,010\nTrainable params: 1,937,010\nNon-trainable params: 0\n================================================================="},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["summary(model)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TRAINING...\n","Train Epoch: 1 [0/60000 (0%)]\tAvg. loss: 3.152756\n","Train Epoch: 1 [15360/60000 (25%)]\tAvg. loss: 2.866908\n","Train Epoch: 1 [30720/60000 (51%)]\tAvg. loss: 2.843758\n","Train Epoch: 1 [46080/60000 (76%)]\tAvg. loss: 2.827503\n","\n","ON TRAINING SET:\n","Average loss: 2.5975, Accuracy: 7187/60000 (12%)\n","\n","ON TEST SET:\n","Average loss: 2.6009, Accuracy: 872/10000 (9%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 2 [0/60000 (0%)]\tAvg. loss: 2.562488\n","Train Epoch: 2 [15360/60000 (25%)]\tAvg. loss: 2.562849\n","Train Epoch: 2 [30720/60000 (51%)]\tAvg. loss: 2.550872\n","Train Epoch: 2 [46080/60000 (76%)]\tAvg. loss: 2.543620\n","\n","ON TRAINING SET:\n","Average loss: 2.3525, Accuracy: 10478/60000 (17%)\n","\n","ON TEST SET:\n","Average loss: 2.4654, Accuracy: 1070/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 3 [0/60000 (0%)]\tAvg. loss: 2.357925\n","Train Epoch: 3 [15360/60000 (25%)]\tAvg. loss: 2.343006\n","Train Epoch: 3 [30720/60000 (51%)]\tAvg. loss: 2.341059\n","Train Epoch: 3 [46080/60000 (76%)]\tAvg. loss: 2.337425\n","\n","ON TRAINING SET:\n","Average loss: 2.1716, Accuracy: 13312/60000 (22%)\n","\n","ON TEST SET:\n","Average loss: 2.5433, Accuracy: 680/10000 (7%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 4 [0/60000 (0%)]\tAvg. loss: 2.142822\n","Train Epoch: 4 [15360/60000 (25%)]\tAvg. loss: 2.181716\n","Train Epoch: 4 [30720/60000 (51%)]\tAvg. loss: 2.187627\n","Train Epoch: 4 [46080/60000 (76%)]\tAvg. loss: 2.196380\n","\n","ON TRAINING SET:\n","Average loss: 2.0795, Accuracy: 15719/60000 (26%)\n","\n","ON TEST SET:\n","Average loss: 2.4101, Accuracy: 1267/10000 (13%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 5 [0/60000 (0%)]\tAvg. loss: 2.060993\n","Train Epoch: 5 [15360/60000 (25%)]\tAvg. loss: 2.096416\n","Train Epoch: 5 [30720/60000 (51%)]\tAvg. loss: 2.104429\n","Train Epoch: 5 [46080/60000 (76%)]\tAvg. loss: 2.117726\n","\n","ON TRAINING SET:\n","Average loss: 2.0027, Accuracy: 17817/60000 (30%)\n","\n","ON TEST SET:\n","Average loss: 2.5028, Accuracy: 909/10000 (9%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 6 [0/60000 (0%)]\tAvg. loss: 2.001839\n","Train Epoch: 6 [15360/60000 (25%)]\tAvg. loss: 2.007267\n","Train Epoch: 6 [30720/60000 (51%)]\tAvg. loss: 2.022994\n","Train Epoch: 6 [46080/60000 (76%)]\tAvg. loss: 2.044111\n","\n","ON TRAINING SET:\n","Average loss: 1.9159, Accuracy: 20327/60000 (34%)\n","\n","ON TEST SET:\n","Average loss: 2.5179, Accuracy: 1158/10000 (12%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 7 [0/60000 (0%)]\tAvg. loss: 1.902422\n","Train Epoch: 7 [15360/60000 (25%)]\tAvg. loss: 1.928183\n","Train Epoch: 7 [30720/60000 (51%)]\tAvg. loss: 1.944648\n","Train Epoch: 7 [46080/60000 (76%)]\tAvg. loss: 1.964142\n","\n","ON TRAINING SET:\n","Average loss: 1.8215, Accuracy: 23019/60000 (38%)\n","\n","ON TEST SET:\n","Average loss: 2.5672, Accuracy: 930/10000 (9%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 8 [0/60000 (0%)]\tAvg. loss: 1.838342\n","Train Epoch: 8 [15360/60000 (25%)]\tAvg. loss: 1.840209\n","Train Epoch: 8 [30720/60000 (51%)]\tAvg. loss: 1.855471\n","Train Epoch: 8 [46080/60000 (76%)]\tAvg. loss: 1.880998\n","\n","ON TRAINING SET:\n","Average loss: 1.7299, Accuracy: 25110/60000 (42%)\n","\n","ON TEST SET:\n","Average loss: 2.6896, Accuracy: 891/10000 (9%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 9 [0/60000 (0%)]\tAvg. loss: 1.691318\n","Train Epoch: 9 [15360/60000 (25%)]\tAvg. loss: 1.743944\n","Train Epoch: 9 [30720/60000 (51%)]\tAvg. loss: 1.759636\n","Train Epoch: 9 [46080/60000 (76%)]\tAvg. loss: 1.785754\n","\n","ON TRAINING SET:\n","Average loss: 1.6153, Accuracy: 28385/60000 (47%)\n","\n","ON TEST SET:\n","Average loss: 2.6818, Accuracy: 978/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 10 [0/60000 (0%)]\tAvg. loss: 1.644393\n","Train Epoch: 10 [15360/60000 (25%)]\tAvg. loss: 1.640812\n","Train Epoch: 10 [30720/60000 (51%)]\tAvg. loss: 1.660918\n","Train Epoch: 10 [46080/60000 (76%)]\tAvg. loss: 1.685464\n","\n","ON TRAINING SET:\n","Average loss: 1.5105, Accuracy: 30872/60000 (51%)\n","\n","ON TEST SET:\n","Average loss: 2.7822, Accuracy: 918/10000 (9%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 11 [0/60000 (0%)]\tAvg. loss: 1.519551\n","Train Epoch: 11 [15360/60000 (25%)]\tAvg. loss: 1.525129\n","Train Epoch: 11 [30720/60000 (51%)]\tAvg. loss: 1.551849\n","Train Epoch: 11 [46080/60000 (76%)]\tAvg. loss: 1.577243\n","\n","ON TRAINING SET:\n","Average loss: 1.3920, Accuracy: 33619/60000 (56%)\n","\n","ON TEST SET:\n","Average loss: 2.8536, Accuracy: 997/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 12 [0/60000 (0%)]\tAvg. loss: 1.404137\n","Train Epoch: 12 [15360/60000 (25%)]\tAvg. loss: 1.401325\n","Train Epoch: 12 [30720/60000 (51%)]\tAvg. loss: 1.433957\n","Train Epoch: 12 [46080/60000 (76%)]\tAvg. loss: 1.466584\n","\n","ON TRAINING SET:\n","Average loss: 1.2853, Accuracy: 35976/60000 (60%)\n","\n","ON TEST SET:\n","Average loss: 2.9214, Accuracy: 1023/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 13 [0/60000 (0%)]\tAvg. loss: 1.270396\n","Train Epoch: 13 [15360/60000 (25%)]\tAvg. loss: 1.290682\n","Train Epoch: 13 [30720/60000 (51%)]\tAvg. loss: 1.315837\n","Train Epoch: 13 [46080/60000 (76%)]\tAvg. loss: 1.349033\n","\n","ON TRAINING SET:\n","Average loss: 1.1675, Accuracy: 39060/60000 (65%)\n","\n","ON TEST SET:\n","Average loss: 3.0886, Accuracy: 988/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 14 [0/60000 (0%)]\tAvg. loss: 1.152161\n","Train Epoch: 14 [15360/60000 (25%)]\tAvg. loss: 1.180820\n","Train Epoch: 14 [30720/60000 (51%)]\tAvg. loss: 1.198237\n","Train Epoch: 14 [46080/60000 (76%)]\tAvg. loss: 1.229164\n","\n","ON TRAINING SET:\n","Average loss: 1.0400, Accuracy: 41780/60000 (70%)\n","\n","ON TEST SET:\n","Average loss: 3.1647, Accuracy: 985/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 15 [0/60000 (0%)]\tAvg. loss: 1.022247\n","Train Epoch: 15 [15360/60000 (25%)]\tAvg. loss: 1.061771\n","Train Epoch: 15 [30720/60000 (51%)]\tAvg. loss: 1.084955\n","Train Epoch: 15 [46080/60000 (76%)]\tAvg. loss: 1.112402\n","\n","ON TRAINING SET:\n","Average loss: 0.9386, Accuracy: 43758/60000 (73%)\n","\n","ON TEST SET:\n","Average loss: 3.1884, Accuracy: 1148/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 16 [0/60000 (0%)]\tAvg. loss: 0.921243\n","Train Epoch: 16 [15360/60000 (25%)]\tAvg. loss: 0.948580\n","Train Epoch: 16 [30720/60000 (51%)]\tAvg. loss: 0.971736\n","Train Epoch: 16 [46080/60000 (76%)]\tAvg. loss: 1.000486\n","\n","ON TRAINING SET:\n","Average loss: 0.8266, Accuracy: 46030/60000 (77%)\n","\n","ON TEST SET:\n","Average loss: 3.4774, Accuracy: 984/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 17 [0/60000 (0%)]\tAvg. loss: 0.799618\n","Train Epoch: 17 [15360/60000 (25%)]\tAvg. loss: 0.856920\n","Train Epoch: 17 [30720/60000 (51%)]\tAvg. loss: 0.878942\n","Train Epoch: 17 [46080/60000 (76%)]\tAvg. loss: 0.899547\n","\n","ON TRAINING SET:\n","Average loss: 0.7258, Accuracy: 48405/60000 (81%)\n","\n","ON TEST SET:\n","Average loss: 3.5988, Accuracy: 913/10000 (9%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 18 [0/60000 (0%)]\tAvg. loss: 0.728390\n","Train Epoch: 18 [15360/60000 (25%)]\tAvg. loss: 0.750384\n","Train Epoch: 18 [30720/60000 (51%)]\tAvg. loss: 0.771681\n","Train Epoch: 18 [46080/60000 (76%)]\tAvg. loss: 0.793528\n","\n","ON TRAINING SET:\n","Average loss: 0.6346, Accuracy: 49892/60000 (83%)\n","\n","ON TEST SET:\n","Average loss: 3.6850, Accuracy: 992/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 19 [0/60000 (0%)]\tAvg. loss: 0.638963\n","Train Epoch: 19 [15360/60000 (25%)]\tAvg. loss: 0.668537\n","Train Epoch: 19 [30720/60000 (51%)]\tAvg. loss: 0.682797\n","Train Epoch: 19 [46080/60000 (76%)]\tAvg. loss: 0.700273\n","\n","ON TRAINING SET:\n","Average loss: 0.5692, Accuracy: 50881/60000 (85%)\n","\n","ON TEST SET:\n","Average loss: 3.8416, Accuracy: 1106/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 20 [0/60000 (0%)]\tAvg. loss: 0.538471\n","Train Epoch: 20 [15360/60000 (25%)]\tAvg. loss: 0.579909\n","Train Epoch: 20 [30720/60000 (51%)]\tAvg. loss: 0.595036\n","Train Epoch: 20 [46080/60000 (76%)]\tAvg. loss: 0.614952\n","\n","ON TRAINING SET:\n","Average loss: 0.4902, Accuracy: 52473/60000 (87%)\n","\n","ON TEST SET:\n","Average loss: 4.0804, Accuracy: 1092/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 21 [0/60000 (0%)]\tAvg. loss: 0.477283\n","Train Epoch: 21 [15360/60000 (25%)]\tAvg. loss: 0.511696\n","Train Epoch: 21 [30720/60000 (51%)]\tAvg. loss: 0.523520\n","Train Epoch: 21 [46080/60000 (76%)]\tAvg. loss: 0.535075\n","\n","ON TRAINING SET:\n","Average loss: 0.4195, Accuracy: 53703/60000 (90%)\n","\n","ON TEST SET:\n","Average loss: 4.1173, Accuracy: 1087/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 22 [0/60000 (0%)]\tAvg. loss: 0.411568\n","Train Epoch: 22 [15360/60000 (25%)]\tAvg. loss: 0.445528\n","Train Epoch: 22 [30720/60000 (51%)]\tAvg. loss: 0.457205\n","Train Epoch: 22 [46080/60000 (76%)]\tAvg. loss: 0.466024\n","\n","ON TRAINING SET:\n","Average loss: 0.3678, Accuracy: 54525/60000 (91%)\n","\n","ON TEST SET:\n","Average loss: 4.3120, Accuracy: 1030/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 23 [0/60000 (0%)]\tAvg. loss: 0.390651\n","Train Epoch: 23 [15360/60000 (25%)]\tAvg. loss: 0.397465\n","Train Epoch: 23 [30720/60000 (51%)]\tAvg. loss: 0.406149\n","Train Epoch: 23 [46080/60000 (76%)]\tAvg. loss: 0.414209\n","\n","ON TRAINING SET:\n","Average loss: 0.3353, Accuracy: 54970/60000 (92%)\n","\n","ON TEST SET:\n","Average loss: 4.4310, Accuracy: 1129/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 24 [0/60000 (0%)]\tAvg. loss: 0.339600\n","Train Epoch: 24 [15360/60000 (25%)]\tAvg. loss: 0.348739\n","Train Epoch: 24 [30720/60000 (51%)]\tAvg. loss: 0.357302\n","Train Epoch: 24 [46080/60000 (76%)]\tAvg. loss: 0.365271\n","\n","ON TRAINING SET:\n","Average loss: 0.2889, Accuracy: 55730/60000 (93%)\n","\n","ON TEST SET:\n","Average loss: 4.6013, Accuracy: 1028/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 25 [0/60000 (0%)]\tAvg. loss: 0.268049\n","Train Epoch: 25 [15360/60000 (25%)]\tAvg. loss: 0.309913\n","Train Epoch: 25 [30720/60000 (51%)]\tAvg. loss: 0.315345\n","Train Epoch: 25 [46080/60000 (76%)]\tAvg. loss: 0.320121\n","\n","ON TRAINING SET:\n","Average loss: 0.2532, Accuracy: 56173/60000 (94%)\n","\n","ON TEST SET:\n","Average loss: 4.6835, Accuracy: 1004/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 26 [0/60000 (0%)]\tAvg. loss: 0.263928\n","Train Epoch: 26 [15360/60000 (25%)]\tAvg. loss: 0.266694\n","Train Epoch: 26 [30720/60000 (51%)]\tAvg. loss: 0.272674\n","Train Epoch: 26 [46080/60000 (76%)]\tAvg. loss: 0.281431\n","\n","ON TRAINING SET:\n","Average loss: 0.2319, Accuracy: 56434/60000 (94%)\n","\n","ON TEST SET:\n","Average loss: 4.7495, Accuracy: 1067/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 27 [0/60000 (0%)]\tAvg. loss: 0.236221\n","Train Epoch: 27 [15360/60000 (25%)]\tAvg. loss: 0.254504\n","Train Epoch: 27 [30720/60000 (51%)]\tAvg. loss: 0.257885\n","Train Epoch: 27 [46080/60000 (76%)]\tAvg. loss: 0.261416\n","\n","ON TRAINING SET:\n","Average loss: 0.2125, Accuracy: 56742/60000 (95%)\n","\n","ON TEST SET:\n","Average loss: 4.9518, Accuracy: 957/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 28 [0/60000 (0%)]\tAvg. loss: 0.223919\n","Train Epoch: 28 [15360/60000 (25%)]\tAvg. loss: 0.225779\n","Train Epoch: 28 [30720/60000 (51%)]\tAvg. loss: 0.230490\n","Train Epoch: 28 [46080/60000 (76%)]\tAvg. loss: 0.237161\n","\n","ON TRAINING SET:\n","Average loss: 0.1955, Accuracy: 56965/60000 (95%)\n","\n","ON TEST SET:\n","Average loss: 5.0124, Accuracy: 1052/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 29 [0/60000 (0%)]\tAvg. loss: 0.206469\n","Train Epoch: 29 [15360/60000 (25%)]\tAvg. loss: 0.205120\n","Train Epoch: 29 [30720/60000 (51%)]\tAvg. loss: 0.211038\n","Train Epoch: 29 [46080/60000 (76%)]\tAvg. loss: 0.214468\n","\n","ON TRAINING SET:\n","Average loss: 0.1738, Accuracy: 57250/60000 (95%)\n","\n","ON TEST SET:\n","Average loss: 5.1724, Accuracy: 934/10000 (9%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 30 [0/60000 (0%)]\tAvg. loss: 0.164900\n","Train Epoch: 30 [15360/60000 (25%)]\tAvg. loss: 0.182505\n","Train Epoch: 30 [30720/60000 (51%)]\tAvg. loss: 0.191306\n","Train Epoch: 30 [46080/60000 (76%)]\tAvg. loss: 0.192575\n","\n","ON TRAINING SET:\n","Average loss: 0.1563, Accuracy: 57531/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 5.1450, Accuracy: 1079/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 31 [0/60000 (0%)]\tAvg. loss: 0.157617\n","Train Epoch: 31 [15360/60000 (25%)]\tAvg. loss: 0.173499\n","Train Epoch: 31 [30720/60000 (51%)]\tAvg. loss: 0.175565\n","Train Epoch: 31 [46080/60000 (76%)]\tAvg. loss: 0.178830\n","\n","ON TRAINING SET:\n","Average loss: 0.1514, Accuracy: 57491/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 5.2721, Accuracy: 1033/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 32 [0/60000 (0%)]\tAvg. loss: 0.191540\n","Train Epoch: 32 [15360/60000 (25%)]\tAvg. loss: 0.168826\n","Train Epoch: 32 [30720/60000 (51%)]\tAvg. loss: 0.168234\n","Train Epoch: 32 [46080/60000 (76%)]\tAvg. loss: 0.172101\n","\n","ON TRAINING SET:\n","Average loss: 0.1471, Accuracy: 57591/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 5.3591, Accuracy: 970/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 33 [0/60000 (0%)]\tAvg. loss: 0.134640\n","Train Epoch: 33 [15360/60000 (25%)]\tAvg. loss: 0.158208\n","Train Epoch: 33 [30720/60000 (51%)]\tAvg. loss: 0.162228\n","Train Epoch: 33 [46080/60000 (76%)]\tAvg. loss: 0.165590\n","\n","ON TRAINING SET:\n","Average loss: 0.1382, Accuracy: 57721/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 5.3381, Accuracy: 1053/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 34 [0/60000 (0%)]\tAvg. loss: 0.150675\n","Train Epoch: 34 [15360/60000 (25%)]\tAvg. loss: 0.147562\n","Train Epoch: 34 [30720/60000 (51%)]\tAvg. loss: 0.153213\n","Train Epoch: 34 [46080/60000 (76%)]\tAvg. loss: 0.154057\n","\n","ON TRAINING SET:\n","Average loss: 0.1354, Accuracy: 57809/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 5.5146, Accuracy: 945/10000 (9%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 35 [0/60000 (0%)]\tAvg. loss: 0.141243\n","Train Epoch: 35 [15360/60000 (25%)]\tAvg. loss: 0.132375\n","Train Epoch: 35 [30720/60000 (51%)]\tAvg. loss: 0.138874\n","Train Epoch: 35 [46080/60000 (76%)]\tAvg. loss: 0.146198\n","\n","ON TRAINING SET:\n","Average loss: 0.1219, Accuracy: 57993/60000 (97%)\n","\n","ON TEST SET:\n","Average loss: 5.6248, Accuracy: 990/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 36 [0/60000 (0%)]\tAvg. loss: 0.138785\n","Train Epoch: 36 [15360/60000 (25%)]\tAvg. loss: 0.133427\n","Train Epoch: 36 [30720/60000 (51%)]\tAvg. loss: 0.139002\n","Train Epoch: 36 [46080/60000 (76%)]\tAvg. loss: 0.142068\n","\n","ON TRAINING SET:\n","Average loss: 0.1221, Accuracy: 58012/60000 (97%)\n","\n","ON TEST SET:\n","Average loss: 5.5795, Accuracy: 1030/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 37 [0/60000 (0%)]\tAvg. loss: 0.126432\n","Train Epoch: 37 [15360/60000 (25%)]\tAvg. loss: 0.131187\n","Train Epoch: 37 [30720/60000 (51%)]\tAvg. loss: 0.136608\n","Train Epoch: 37 [46080/60000 (76%)]\tAvg. loss: 0.136021\n","\n","ON TRAINING SET:\n","Average loss: 0.1106, Accuracy: 58215/60000 (97%)\n","\n","ON TEST SET:\n","Average loss: 5.4962, Accuracy: 1107/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 38 [0/60000 (0%)]\tAvg. loss: 0.123977\n","Train Epoch: 38 [15360/60000 (25%)]\tAvg. loss: 0.126096\n","Train Epoch: 38 [30720/60000 (51%)]\tAvg. loss: 0.129169\n","Train Epoch: 38 [46080/60000 (76%)]\tAvg. loss: 0.129755\n","\n","ON TRAINING SET:\n","Average loss: 0.1048, Accuracy: 58307/60000 (97%)\n","\n","ON TEST SET:\n","Average loss: 5.7625, Accuracy: 978/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 39 [0/60000 (0%)]\tAvg. loss: 0.090980\n","Train Epoch: 39 [15360/60000 (25%)]\tAvg. loss: 0.113723\n","Train Epoch: 39 [30720/60000 (51%)]\tAvg. loss: 0.122048\n","Train Epoch: 39 [46080/60000 (76%)]\tAvg. loss: 0.124618\n","\n","ON TRAINING SET:\n","Average loss: 0.0990, Accuracy: 58418/60000 (97%)\n","\n","ON TEST SET:\n","Average loss: 5.7699, Accuracy: 1026/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 40 [0/60000 (0%)]\tAvg. loss: 0.089567\n","Train Epoch: 40 [15360/60000 (25%)]\tAvg. loss: 0.114210\n","Train Epoch: 40 [30720/60000 (51%)]\tAvg. loss: 0.112305\n","Train Epoch: 40 [46080/60000 (76%)]\tAvg. loss: 0.116148\n","\n","ON TRAINING SET:\n","Average loss: 0.0947, Accuracy: 58577/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 5.8003, Accuracy: 1022/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 41 [0/60000 (0%)]\tAvg. loss: 0.086507\n","Train Epoch: 41 [15360/60000 (25%)]\tAvg. loss: 0.108385\n","Train Epoch: 41 [30720/60000 (51%)]\tAvg. loss: 0.110577\n","Train Epoch: 41 [46080/60000 (76%)]\tAvg. loss: 0.116318\n","\n","ON TRAINING SET:\n","Average loss: 0.0940, Accuracy: 58501/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 5.8150, Accuracy: 1102/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 42 [0/60000 (0%)]\tAvg. loss: 0.099840\n","Train Epoch: 42 [15360/60000 (25%)]\tAvg. loss: 0.109413\n","Train Epoch: 42 [30720/60000 (51%)]\tAvg. loss: 0.114420\n","Train Epoch: 42 [46080/60000 (76%)]\tAvg. loss: 0.117108\n","\n","ON TRAINING SET:\n","Average loss: 0.1024, Accuracy: 58376/60000 (97%)\n","\n","ON TEST SET:\n","Average loss: 5.9558, Accuracy: 988/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 43 [0/60000 (0%)]\tAvg. loss: 0.101635\n","Train Epoch: 43 [15360/60000 (25%)]\tAvg. loss: 0.120020\n","Train Epoch: 43 [30720/60000 (51%)]\tAvg. loss: 0.118633\n","Train Epoch: 43 [46080/60000 (76%)]\tAvg. loss: 0.121549\n","\n","ON TRAINING SET:\n","Average loss: 0.0963, Accuracy: 58598/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 6.1094, Accuracy: 959/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 44 [0/60000 (0%)]\tAvg. loss: 0.100510\n","Train Epoch: 44 [15360/60000 (25%)]\tAvg. loss: 0.107648\n","Train Epoch: 44 [30720/60000 (51%)]\tAvg. loss: 0.115057\n","Train Epoch: 44 [46080/60000 (76%)]\tAvg. loss: 0.117465\n","\n","ON TRAINING SET:\n","Average loss: 0.0987, Accuracy: 58494/60000 (97%)\n","\n","ON TEST SET:\n","Average loss: 6.1684, Accuracy: 980/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 45 [0/60000 (0%)]\tAvg. loss: 0.095935\n","Train Epoch: 45 [15360/60000 (25%)]\tAvg. loss: 0.108781\n","Train Epoch: 45 [30720/60000 (51%)]\tAvg. loss: 0.110369\n","Train Epoch: 45 [46080/60000 (76%)]\tAvg. loss: 0.117940\n","\n","ON TRAINING SET:\n","Average loss: 0.1012, Accuracy: 58549/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 6.0068, Accuracy: 1026/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 46 [0/60000 (0%)]\tAvg. loss: 0.125075\n","Train Epoch: 46 [15360/60000 (25%)]\tAvg. loss: 0.130944\n","Train Epoch: 46 [30720/60000 (51%)]\tAvg. loss: 0.143027\n","Train Epoch: 46 [46080/60000 (76%)]\tAvg. loss: 0.155638\n","\n","ON TRAINING SET:\n","Average loss: 0.1577, Accuracy: 57408/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 6.1809, Accuracy: 962/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 47 [0/60000 (0%)]\tAvg. loss: 0.165384\n","Train Epoch: 47 [15360/60000 (25%)]\tAvg. loss: 0.223887\n","Train Epoch: 47 [30720/60000 (51%)]\tAvg. loss: 0.252469\n","Train Epoch: 47 [46080/60000 (76%)]\tAvg. loss: 0.287232\n","\n","ON TRAINING SET:\n","Average loss: 0.3582, Accuracy: 53065/60000 (88%)\n","\n","ON TEST SET:\n","Average loss: 6.1400, Accuracy: 1001/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 48 [0/60000 (0%)]\tAvg. loss: 0.310847\n","Train Epoch: 48 [15360/60000 (25%)]\tAvg. loss: 0.426903\n","Train Epoch: 48 [30720/60000 (51%)]\tAvg. loss: 0.469486\n","Train Epoch: 48 [46080/60000 (76%)]\tAvg. loss: 0.521655\n","\n","ON TRAINING SET:\n","Average loss: 0.5272, Accuracy: 49528/60000 (83%)\n","\n","ON TEST SET:\n","Average loss: 6.0047, Accuracy: 1062/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 49 [0/60000 (0%)]\tAvg. loss: 0.424007\n","Train Epoch: 49 [15360/60000 (25%)]\tAvg. loss: 0.550663\n","Train Epoch: 49 [30720/60000 (51%)]\tAvg. loss: 0.581655\n","Train Epoch: 49 [46080/60000 (76%)]\tAvg. loss: 0.594353\n","\n","ON TRAINING SET:\n","Average loss: 0.3924, Accuracy: 52372/60000 (87%)\n","\n","ON TEST SET:\n","Average loss: 5.5253, Accuracy: 1052/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 50 [0/60000 (0%)]\tAvg. loss: 0.424875\n","Train Epoch: 50 [15360/60000 (25%)]\tAvg. loss: 0.425689\n","Train Epoch: 50 [30720/60000 (51%)]\tAvg. loss: 0.423512\n","Train Epoch: 50 [46080/60000 (76%)]\tAvg. loss: 0.424311\n","\n","ON TRAINING SET:\n","Average loss: 0.2547, Accuracy: 55424/60000 (92%)\n","\n","ON TEST SET:\n","Average loss: 5.5932, Accuracy: 1045/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 51 [0/60000 (0%)]\tAvg. loss: 0.239134\n","Train Epoch: 51 [15360/60000 (25%)]\tAvg. loss: 0.255073\n","Train Epoch: 51 [30720/60000 (51%)]\tAvg. loss: 0.255365\n","Train Epoch: 51 [46080/60000 (76%)]\tAvg. loss: 0.256200\n","\n","ON TRAINING SET:\n","Average loss: 0.1419, Accuracy: 57851/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 5.6041, Accuracy: 1095/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 52 [0/60000 (0%)]\tAvg. loss: 0.134459\n","Train Epoch: 52 [15360/60000 (25%)]\tAvg. loss: 0.156476\n","Train Epoch: 52 [30720/60000 (51%)]\tAvg. loss: 0.155366\n","Train Epoch: 52 [46080/60000 (76%)]\tAvg. loss: 0.154949\n","\n","ON TRAINING SET:\n","Average loss: 0.0955, Accuracy: 58705/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 5.6877, Accuracy: 1027/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 53 [0/60000 (0%)]\tAvg. loss: 0.095901\n","Train Epoch: 53 [15360/60000 (25%)]\tAvg. loss: 0.100943\n","Train Epoch: 53 [30720/60000 (51%)]\tAvg. loss: 0.100107\n","Train Epoch: 53 [46080/60000 (76%)]\tAvg. loss: 0.099205\n","\n","ON TRAINING SET:\n","Average loss: 0.0634, Accuracy: 59117/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 5.8270, Accuracy: 1004/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 54 [0/60000 (0%)]\tAvg. loss: 0.068931\n","Train Epoch: 54 [15360/60000 (25%)]\tAvg. loss: 0.075064\n","Train Epoch: 54 [30720/60000 (51%)]\tAvg. loss: 0.074613\n","Train Epoch: 54 [46080/60000 (76%)]\tAvg. loss: 0.075737\n","\n","ON TRAINING SET:\n","Average loss: 0.0517, Accuracy: 59249/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 5.9040, Accuracy: 994/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 55 [0/60000 (0%)]\tAvg. loss: 0.055573\n","Train Epoch: 55 [15360/60000 (25%)]\tAvg. loss: 0.059798\n","Train Epoch: 55 [30720/60000 (51%)]\tAvg. loss: 0.061648\n","Train Epoch: 55 [46080/60000 (76%)]\tAvg. loss: 0.062997\n","\n","ON TRAINING SET:\n","Average loss: 0.0495, Accuracy: 59198/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 5.9153, Accuracy: 1080/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 56 [0/60000 (0%)]\tAvg. loss: 0.053535\n","Train Epoch: 56 [15360/60000 (25%)]\tAvg. loss: 0.052342\n","Train Epoch: 56 [30720/60000 (51%)]\tAvg. loss: 0.054079\n","Train Epoch: 56 [46080/60000 (76%)]\tAvg. loss: 0.054868\n","\n","ON TRAINING SET:\n","Average loss: 0.0457, Accuracy: 59298/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 5.9829, Accuracy: 1039/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 57 [0/60000 (0%)]\tAvg. loss: 0.055732\n","Train Epoch: 57 [15360/60000 (25%)]\tAvg. loss: 0.051747\n","Train Epoch: 57 [30720/60000 (51%)]\tAvg. loss: 0.054628\n","Train Epoch: 57 [46080/60000 (76%)]\tAvg. loss: 0.054209\n","\n","ON TRAINING SET:\n","Average loss: 0.0394, Accuracy: 59405/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 5.9760, Accuracy: 1056/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 58 [0/60000 (0%)]\tAvg. loss: 0.044486\n","Train Epoch: 58 [15360/60000 (25%)]\tAvg. loss: 0.043035\n","Train Epoch: 58 [30720/60000 (51%)]\tAvg. loss: 0.045617\n","Train Epoch: 58 [46080/60000 (76%)]\tAvg. loss: 0.047315\n","\n","ON TRAINING SET:\n","Average loss: 0.0379, Accuracy: 59379/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 6.1008, Accuracy: 1026/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 59 [0/60000 (0%)]\tAvg. loss: 0.047912\n","Train Epoch: 59 [15360/60000 (25%)]\tAvg. loss: 0.046268\n","Train Epoch: 59 [30720/60000 (51%)]\tAvg. loss: 0.046254\n","Train Epoch: 59 [46080/60000 (76%)]\tAvg. loss: 0.046538\n","\n","ON TRAINING SET:\n","Average loss: 0.0361, Accuracy: 59454/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 6.0082, Accuracy: 1083/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 60 [0/60000 (0%)]\tAvg. loss: 0.037937\n","Train Epoch: 60 [15360/60000 (25%)]\tAvg. loss: 0.037506\n","Train Epoch: 60 [30720/60000 (51%)]\tAvg. loss: 0.038098\n","Train Epoch: 60 [46080/60000 (76%)]\tAvg. loss: 0.039627\n","\n","ON TRAINING SET:\n","Average loss: 0.0327, Accuracy: 59520/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 6.0933, Accuracy: 1095/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 61 [0/60000 (0%)]\tAvg. loss: 0.030984\n","Train Epoch: 61 [15360/60000 (25%)]\tAvg. loss: 0.036537\n","Train Epoch: 61 [30720/60000 (51%)]\tAvg. loss: 0.036635\n","Train Epoch: 61 [46080/60000 (76%)]\tAvg. loss: 0.036861\n","\n","ON TRAINING SET:\n","Average loss: 0.0289, Accuracy: 59587/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 6.1676, Accuracy: 1033/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 62 [0/60000 (0%)]\tAvg. loss: 0.026895\n","Train Epoch: 62 [15360/60000 (25%)]\tAvg. loss: 0.033400\n","Train Epoch: 62 [30720/60000 (51%)]\tAvg. loss: 0.033321\n","Train Epoch: 62 [46080/60000 (76%)]\tAvg. loss: 0.033563\n","\n","ON TRAINING SET:\n","Average loss: 0.0232, Accuracy: 59701/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.1928, Accuracy: 1030/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 63 [0/60000 (0%)]\tAvg. loss: 0.027635\n","Train Epoch: 63 [15360/60000 (25%)]\tAvg. loss: 0.025819\n","Train Epoch: 63 [30720/60000 (51%)]\tAvg. loss: 0.026978\n","Train Epoch: 63 [46080/60000 (76%)]\tAvg. loss: 0.027930\n","\n","ON TRAINING SET:\n","Average loss: 0.0185, Accuracy: 59791/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.2798, Accuracy: 995/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 64 [0/60000 (0%)]\tAvg. loss: 0.018726\n","Train Epoch: 64 [15360/60000 (25%)]\tAvg. loss: 0.024065\n","Train Epoch: 64 [30720/60000 (51%)]\tAvg. loss: 0.025182\n","Train Epoch: 64 [46080/60000 (76%)]\tAvg. loss: 0.026242\n","\n","ON TRAINING SET:\n","Average loss: 0.0203, Accuracy: 59722/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.2415, Accuracy: 1037/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 65 [0/60000 (0%)]\tAvg. loss: 0.020047\n","Train Epoch: 65 [15360/60000 (25%)]\tAvg. loss: 0.025834\n","Train Epoch: 65 [30720/60000 (51%)]\tAvg. loss: 0.027018\n","Train Epoch: 65 [46080/60000 (76%)]\tAvg. loss: 0.027078\n","\n","ON TRAINING SET:\n","Average loss: 0.0199, Accuracy: 59737/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.2776, Accuracy: 1034/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 66 [0/60000 (0%)]\tAvg. loss: 0.022115\n","Train Epoch: 66 [15360/60000 (25%)]\tAvg. loss: 0.026426\n","Train Epoch: 66 [30720/60000 (51%)]\tAvg. loss: 0.026385\n","Train Epoch: 66 [46080/60000 (76%)]\tAvg. loss: 0.026700\n","\n","ON TRAINING SET:\n","Average loss: 0.0204, Accuracy: 59719/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.3772, Accuracy: 994/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 67 [0/60000 (0%)]\tAvg. loss: 0.013699\n","Train Epoch: 67 [15360/60000 (25%)]\tAvg. loss: 0.021560\n","Train Epoch: 67 [30720/60000 (51%)]\tAvg. loss: 0.022351\n","Train Epoch: 67 [46080/60000 (76%)]\tAvg. loss: 0.024368\n","\n","ON TRAINING SET:\n","Average loss: 0.0185, Accuracy: 59757/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.3004, Accuracy: 1042/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 68 [0/60000 (0%)]\tAvg. loss: 0.017185\n","Train Epoch: 68 [15360/60000 (25%)]\tAvg. loss: 0.026088\n","Train Epoch: 68 [30720/60000 (51%)]\tAvg. loss: 0.025346\n","Train Epoch: 68 [46080/60000 (76%)]\tAvg. loss: 0.025341\n","\n","ON TRAINING SET:\n","Average loss: 0.0191, Accuracy: 59747/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.3550, Accuracy: 1033/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 69 [0/60000 (0%)]\tAvg. loss: 0.009886\n","Train Epoch: 69 [15360/60000 (25%)]\tAvg. loss: 0.020942\n","Train Epoch: 69 [30720/60000 (51%)]\tAvg. loss: 0.022262\n","Train Epoch: 69 [46080/60000 (76%)]\tAvg. loss: 0.022261\n","\n","ON TRAINING SET:\n","Average loss: 0.0159, Accuracy: 59809/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.4209, Accuracy: 1046/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 70 [0/60000 (0%)]\tAvg. loss: 0.012969\n","Train Epoch: 70 [15360/60000 (25%)]\tAvg. loss: 0.020217\n","Train Epoch: 70 [30720/60000 (51%)]\tAvg. loss: 0.020943\n","Train Epoch: 70 [46080/60000 (76%)]\tAvg. loss: 0.021833\n","\n","ON TRAINING SET:\n","Average loss: 0.0162, Accuracy: 59804/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.3879, Accuracy: 1042/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 71 [0/60000 (0%)]\tAvg. loss: 0.017764\n","Train Epoch: 71 [15360/60000 (25%)]\tAvg. loss: 0.018943\n","Train Epoch: 71 [30720/60000 (51%)]\tAvg. loss: 0.019550\n","Train Epoch: 71 [46080/60000 (76%)]\tAvg. loss: 0.020488\n","\n","ON TRAINING SET:\n","Average loss: 0.0161, Accuracy: 59778/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.4784, Accuracy: 1058/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 72 [0/60000 (0%)]\tAvg. loss: 0.017492\n","Train Epoch: 72 [15360/60000 (25%)]\tAvg. loss: 0.020115\n","Train Epoch: 72 [30720/60000 (51%)]\tAvg. loss: 0.019769\n","Train Epoch: 72 [46080/60000 (76%)]\tAvg. loss: 0.019218\n","\n","ON TRAINING SET:\n","Average loss: 0.0136, Accuracy: 59840/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.4271, Accuracy: 1076/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 73 [0/60000 (0%)]\tAvg. loss: 0.012558\n","Train Epoch: 73 [15360/60000 (25%)]\tAvg. loss: 0.015817\n","Train Epoch: 73 [30720/60000 (51%)]\tAvg. loss: 0.015897\n","Train Epoch: 73 [46080/60000 (76%)]\tAvg. loss: 0.016182\n","\n","ON TRAINING SET:\n","Average loss: 0.0130, Accuracy: 59830/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.4787, Accuracy: 1041/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 74 [0/60000 (0%)]\tAvg. loss: 0.015414\n","Train Epoch: 74 [15360/60000 (25%)]\tAvg. loss: 0.014174\n","Train Epoch: 74 [30720/60000 (51%)]\tAvg. loss: 0.013422\n","Train Epoch: 74 [46080/60000 (76%)]\tAvg. loss: 0.013745\n","\n","ON TRAINING SET:\n","Average loss: 0.0088, Accuracy: 59921/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.5219, Accuracy: 1018/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 75 [0/60000 (0%)]\tAvg. loss: 0.010276\n","Train Epoch: 75 [15360/60000 (25%)]\tAvg. loss: 0.010415\n","Train Epoch: 75 [30720/60000 (51%)]\tAvg. loss: 0.010013\n","Train Epoch: 75 [46080/60000 (76%)]\tAvg. loss: 0.010598\n","\n","ON TRAINING SET:\n","Average loss: 0.0073, Accuracy: 59944/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.6127, Accuracy: 1022/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 76 [0/60000 (0%)]\tAvg. loss: 0.008966\n","Train Epoch: 76 [15360/60000 (25%)]\tAvg. loss: 0.009698\n","Train Epoch: 76 [30720/60000 (51%)]\tAvg. loss: 0.009903\n","Train Epoch: 76 [46080/60000 (76%)]\tAvg. loss: 0.009919\n","\n","ON TRAINING SET:\n","Average loss: 0.0076, Accuracy: 59933/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.5401, Accuracy: 1047/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 77 [0/60000 (0%)]\tAvg. loss: 0.007480\n","Train Epoch: 77 [15360/60000 (25%)]\tAvg. loss: 0.009276\n","Train Epoch: 77 [30720/60000 (51%)]\tAvg. loss: 0.009553\n","Train Epoch: 77 [46080/60000 (76%)]\tAvg. loss: 0.009737\n","\n","ON TRAINING SET:\n","Average loss: 0.0065, Accuracy: 59957/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.6200, Accuracy: 1063/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 78 [0/60000 (0%)]\tAvg. loss: 0.010424\n","Train Epoch: 78 [15360/60000 (25%)]\tAvg. loss: 0.008207\n","Train Epoch: 78 [30720/60000 (51%)]\tAvg. loss: 0.007853\n","Train Epoch: 78 [46080/60000 (76%)]\tAvg. loss: 0.008027\n","\n","ON TRAINING SET:\n","Average loss: 0.0061, Accuracy: 59958/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.6097, Accuracy: 1035/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 79 [0/60000 (0%)]\tAvg. loss: 0.007998\n","Train Epoch: 79 [15360/60000 (25%)]\tAvg. loss: 0.007979\n","Train Epoch: 79 [30720/60000 (51%)]\tAvg. loss: 0.008071\n","Train Epoch: 79 [46080/60000 (76%)]\tAvg. loss: 0.007905\n","\n","ON TRAINING SET:\n","Average loss: 0.0058, Accuracy: 59954/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.5902, Accuracy: 1073/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 80 [0/60000 (0%)]\tAvg. loss: 0.005630\n","Train Epoch: 80 [15360/60000 (25%)]\tAvg. loss: 0.007396\n","Train Epoch: 80 [30720/60000 (51%)]\tAvg. loss: 0.007216\n","Train Epoch: 80 [46080/60000 (76%)]\tAvg. loss: 0.007211\n","\n","ON TRAINING SET:\n","Average loss: 0.0051, Accuracy: 59965/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.6415, Accuracy: 1036/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 81 [0/60000 (0%)]\tAvg. loss: 0.003628\n","Train Epoch: 81 [15360/60000 (25%)]\tAvg. loss: 0.005804\n","Train Epoch: 81 [30720/60000 (51%)]\tAvg. loss: 0.005718\n","Train Epoch: 81 [46080/60000 (76%)]\tAvg. loss: 0.005678\n","\n","ON TRAINING SET:\n","Average loss: 0.0035, Accuracy: 59981/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.6630, Accuracy: 1069/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 82 [0/60000 (0%)]\tAvg. loss: 0.003269\n","Train Epoch: 82 [15360/60000 (25%)]\tAvg. loss: 0.004446\n","Train Epoch: 82 [30720/60000 (51%)]\tAvg. loss: 0.004422\n","Train Epoch: 82 [46080/60000 (76%)]\tAvg. loss: 0.004439\n","\n","ON TRAINING SET:\n","Average loss: 0.0029, Accuracy: 59986/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.6242, Accuracy: 1063/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 83 [0/60000 (0%)]\tAvg. loss: 0.004463\n","Train Epoch: 83 [15360/60000 (25%)]\tAvg. loss: 0.004140\n","Train Epoch: 83 [30720/60000 (51%)]\tAvg. loss: 0.004080\n","Train Epoch: 83 [46080/60000 (76%)]\tAvg. loss: 0.004202\n","\n","ON TRAINING SET:\n","Average loss: 0.0032, Accuracy: 59977/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.7029, Accuracy: 1040/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 84 [0/60000 (0%)]\tAvg. loss: 0.003190\n","Train Epoch: 84 [15360/60000 (25%)]\tAvg. loss: 0.003724\n","Train Epoch: 84 [30720/60000 (51%)]\tAvg. loss: 0.004179\n","Train Epoch: 84 [46080/60000 (76%)]\tAvg. loss: 0.004010\n","\n","ON TRAINING SET:\n","Average loss: 0.0025, Accuracy: 59991/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.7733, Accuracy: 1023/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 85 [0/60000 (0%)]\tAvg. loss: 0.003001\n","Train Epoch: 85 [15360/60000 (25%)]\tAvg. loss: 0.003200\n","Train Epoch: 85 [30720/60000 (51%)]\tAvg. loss: 0.003026\n","Train Epoch: 85 [46080/60000 (76%)]\tAvg. loss: 0.002873\n","\n","ON TRAINING SET:\n","Average loss: 0.0017, Accuracy: 59997/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.7179, Accuracy: 1060/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 86 [0/60000 (0%)]\tAvg. loss: 0.001822\n","Train Epoch: 86 [15360/60000 (25%)]\tAvg. loss: 0.002256\n","Train Epoch: 86 [30720/60000 (51%)]\tAvg. loss: 0.002192\n","Train Epoch: 86 [46080/60000 (76%)]\tAvg. loss: 0.002190\n","\n","ON TRAINING SET:\n","Average loss: 0.0014, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.7802, Accuracy: 1032/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 87 [0/60000 (0%)]\tAvg. loss: 0.002268\n","Train Epoch: 87 [15360/60000 (25%)]\tAvg. loss: 0.001825\n","Train Epoch: 87 [30720/60000 (51%)]\tAvg. loss: 0.001807\n","Train Epoch: 87 [46080/60000 (76%)]\tAvg. loss: 0.001814\n","\n","ON TRAINING SET:\n","Average loss: 0.0012, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.8085, Accuracy: 1030/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 88 [0/60000 (0%)]\tAvg. loss: 0.001696\n","Train Epoch: 88 [15360/60000 (25%)]\tAvg. loss: 0.001625\n","Train Epoch: 88 [30720/60000 (51%)]\tAvg. loss: 0.001637\n","Train Epoch: 88 [46080/60000 (76%)]\tAvg. loss: 0.001643\n","\n","ON TRAINING SET:\n","Average loss: 0.0012, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.8083, Accuracy: 1034/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 89 [0/60000 (0%)]\tAvg. loss: 0.001383\n","Train Epoch: 89 [15360/60000 (25%)]\tAvg. loss: 0.001530\n","Train Epoch: 89 [30720/60000 (51%)]\tAvg. loss: 0.001552\n","Train Epoch: 89 [46080/60000 (76%)]\tAvg. loss: 0.001558\n","\n","ON TRAINING SET:\n","Average loss: 0.0011, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.8162, Accuracy: 1032/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 90 [0/60000 (0%)]\tAvg. loss: 0.001358\n","Train Epoch: 90 [15360/60000 (25%)]\tAvg. loss: 0.001464\n","Train Epoch: 90 [30720/60000 (51%)]\tAvg. loss: 0.001493\n","Train Epoch: 90 [46080/60000 (76%)]\tAvg. loss: 0.001486\n","\n","ON TRAINING SET:\n","Average loss: 0.0010, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.8414, Accuracy: 1035/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 91 [0/60000 (0%)]\tAvg. loss: 0.001288\n","Train Epoch: 91 [15360/60000 (25%)]\tAvg. loss: 0.001421\n","Train Epoch: 91 [30720/60000 (51%)]\tAvg. loss: 0.001420\n","Train Epoch: 91 [46080/60000 (76%)]\tAvg. loss: 0.001437\n","\n","ON TRAINING SET:\n","Average loss: 0.0010, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.8303, Accuracy: 1033/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 92 [0/60000 (0%)]\tAvg. loss: 0.001296\n","Train Epoch: 92 [15360/60000 (25%)]\tAvg. loss: 0.001382\n","Train Epoch: 92 [30720/60000 (51%)]\tAvg. loss: 0.001391\n","Train Epoch: 92 [46080/60000 (76%)]\tAvg. loss: 0.001397\n","\n","ON TRAINING SET:\n","Average loss: 0.0010, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.8559, Accuracy: 1032/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 93 [0/60000 (0%)]\tAvg. loss: 0.001355\n","Train Epoch: 93 [15360/60000 (25%)]\tAvg. loss: 0.001324\n","Train Epoch: 93 [30720/60000 (51%)]\tAvg. loss: 0.001342\n","Train Epoch: 93 [46080/60000 (76%)]\tAvg. loss: 0.001338\n","\n","ON TRAINING SET:\n","Average loss: 0.0010, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.8406, Accuracy: 1050/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 94 [0/60000 (0%)]\tAvg. loss: 0.001268\n","Train Epoch: 94 [15360/60000 (25%)]\tAvg. loss: 0.001286\n","Train Epoch: 94 [30720/60000 (51%)]\tAvg. loss: 0.001290\n","Train Epoch: 94 [46080/60000 (76%)]\tAvg. loss: 0.001303\n","\n","ON TRAINING SET:\n","Average loss: 0.0009, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.8796, Accuracy: 1038/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 95 [0/60000 (0%)]\tAvg. loss: 0.001194\n","Train Epoch: 95 [15360/60000 (25%)]\tAvg. loss: 0.001261\n","Train Epoch: 95 [30720/60000 (51%)]\tAvg. loss: 0.001259\n","Train Epoch: 95 [46080/60000 (76%)]\tAvg. loss: 0.001270\n","\n","ON TRAINING SET:\n","Average loss: 0.0009, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.8651, Accuracy: 1023/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 96 [0/60000 (0%)]\tAvg. loss: 0.001159\n","Train Epoch: 96 [15360/60000 (25%)]\tAvg. loss: 0.001211\n","Train Epoch: 96 [30720/60000 (51%)]\tAvg. loss: 0.001237\n","Train Epoch: 96 [46080/60000 (76%)]\tAvg. loss: 0.001240\n","\n","ON TRAINING SET:\n","Average loss: 0.0009, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.8831, Accuracy: 1036/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 97 [0/60000 (0%)]\tAvg. loss: 0.001277\n","Train Epoch: 97 [15360/60000 (25%)]\tAvg. loss: 0.001216\n","Train Epoch: 97 [30720/60000 (51%)]\tAvg. loss: 0.001223\n","Train Epoch: 97 [46080/60000 (76%)]\tAvg. loss: 0.001208\n","\n","ON TRAINING SET:\n","Average loss: 0.0009, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.9026, Accuracy: 1025/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 98 [0/60000 (0%)]\tAvg. loss: 0.001170\n","Train Epoch: 98 [15360/60000 (25%)]\tAvg. loss: 0.001165\n","Train Epoch: 98 [30720/60000 (51%)]\tAvg. loss: 0.001168\n","Train Epoch: 98 [46080/60000 (76%)]\tAvg. loss: 0.001174\n","\n","ON TRAINING SET:\n","Average loss: 0.0008, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.9060, Accuracy: 1031/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 99 [0/60000 (0%)]\tAvg. loss: 0.001129\n","Train Epoch: 99 [15360/60000 (25%)]\tAvg. loss: 0.001154\n","Train Epoch: 99 [30720/60000 (51%)]\tAvg. loss: 0.001145\n","Train Epoch: 99 [46080/60000 (76%)]\tAvg. loss: 0.001146\n","\n","ON TRAINING SET:\n","Average loss: 0.0008, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.9269, Accuracy: 1035/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 100 [0/60000 (0%)]\tAvg. loss: 0.001041\n","Train Epoch: 100 [15360/60000 (25%)]\tAvg. loss: 0.001089\n","Train Epoch: 100 [30720/60000 (51%)]\tAvg. loss: 0.001110\n","Train Epoch: 100 [46080/60000 (76%)]\tAvg. loss: 0.001116\n","\n","ON TRAINING SET:\n","Average loss: 0.0008, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.9206, Accuracy: 1044/10000 (10%)\n","\n","\n","\n"]}],"source":["for epoch in range(1, nrepochs + 1):\n","\n","    # Training\n","    print(\"TRAINING...\")\n","    train_epoch(\n","        model, device, train_loader, lossfn, optimizer, epoch, print_every_nep=15, inner_scheduler=None, quiet=False,\n","    )\n","\n","    # Tweaks for the Lookahead optimizer (before testing)\n","    if isinstance(optimizer, Lookahead):\n","        optimizer._backup_and_load_cache()\n","\n","    # Testing: on training and testing set\n","    print(\"\\nON TRAINING SET:\")\n","    _ = test(model, device, test_on_train_loader, lossfn, quiet=False)\n","    print(\"\\nON TEST SET:\")\n","    _ = test(model, device, test_loader, lossfn, quiet=False)\n","    print(\"\\n\\n\")\n","\n","    # Tweaks for the Lookahead optimizer (after testing)\n","    if isinstance(optimizer, Lookahead):\n","        optimizer._clear_and_load_backup()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"softmax_mnist.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('RDDL': conda)","name":"python388jvsc74a57bd0eb8633c4d4e251251708d3c7ece77ee33d393b5bf4628cd3b0e51f052595f5d6"},"language_info":{"name":"python","version":""}},"nbformat":4,"nbformat_minor":0}