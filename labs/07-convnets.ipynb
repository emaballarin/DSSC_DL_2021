{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning course - LAB 7\n",
    "\n",
    "## ConvNets 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap from previous Lab\n",
    "\n",
    "* we introduced some basic image processing functionalities with OpenCV\n",
    "* we saw how to import a custom dataset in PyTorch, how to operate data augmentation, and how to create a DataLoader out of it\n",
    "\n",
    "### Agenda for today\n",
    "\n",
    "* we will construct our first Convolutional Neural Network (CNN)\n",
    "* we will show how to do transfer learning on CNNs\n",
    "* we will show how to introduce Deconvolution/Inverse Convolution inside CNNs\n",
    "* we will construct our own implementation of the ResNet\n",
    "* we will learn about the U-Net, a CNN for image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first CNN\n",
    "\n",
    "Building CNNs is actually not hard once you know all the pieces to construct a MultiLayer Perceptron.\n",
    "\n",
    "We can distinguish between two macro-categories of CNNs, at least for the part concerning image classification. We might call them \"historical\" and \"modern\", although characteristics of both can sometimes get pretty much mixed-up.\n",
    "\n",
    "* \"Classical\" (\"Historical\") CNNs are a stack composed of two parts:\n",
    "  * a **convolutional** part, where have a cascade of convolutional layers intertwined with pooling layers for dimensionality and complexity reduction\n",
    "    * usually the filters in each convolutional layer are more numerous as the image size shrinks (i.e., as we get further from the input)\n",
    "  * a **fully-connected** part, where we have a sequence of few fully-connected layer, ending up in the output layer, where, as usual, we have as many neurons as there are categories\n",
    "  \n",
    " the epitome of the historical CNN (which is still used in research today nonetheless) is VGGNet (or simply VGG) [5](https://arxiv.org/abs/1409.1556v6). Its *core* is a **convolutional block** composed of two or three convolutional layers each with the same number of filters, which is double the number of filters of the previous layer, up to 512 filters per layer. At the end of each block, there's a Max Pooling layer which halves the spatial dimension of the image.\n",
    "\n",
    " In the picture below, you can see a modern implementation of VGG with only one fully-connected layer.\n",
    "  ![](img/vgg.png)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"Modern\" CNNs, instead, get rid of the fully-connected part, as usually\n",
    "    1. it introduces a considerable amount of parameters in the network (note that the convolutional layers, due to their local connectivity and shared weights, have a much lower number of parameters w.r.t. fully connected layers)\n",
    "    2. it keeps the 2D spatial structure of the image intact up to the last hidden layer, allowing for more interpretal parameters/neurons (*insert Olah citation here*)\n",
    "    3. it represents a \"rigid\" portion of the network in the sense that it constrains they size of the image to be fixed. We will see later how this can pose a problem.\n",
    "\n",
    " Sometimes, even the pooling layers may get replaced by convolutional layers with large kernel size, as its effect is to reduce dimensionality (i.e., height and width) of the corresponding image.\n",
    " \n",
    " Recently, the **residual block** has become one of the paramount structures in modern CNNs.\n",
    "\n",
    " \"Classical\" CNNs stack a large number of convolutional layers on top of each other. This though can lead to problems such as the **vanishing gradient**. In a periodic fashion, resnets introduce **skip connections**:\n",
    "    * given the layer of index $k$, we call $a_k$ its output/activation\n",
    "    * $a_k$ gets pushed towards the next layer $k+1$, but we also \"propagate\" it *after* the layer $k+m$ (usually $m$ is 1 or 2) and we sum $a_k$ to $a_{k+m}$.\n",
    "    \n",
    " More on that later.\n",
    "\n",
    " ![](img/resnet.png)\n",
    "\n",
    " In the diagram above there're two residual blocks. The skip connections are indicated with the protruding connector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:21:24.884576Z",
     "iopub.status.busy": "2021-04-19T10:21:24.884345Z",
     "iopub.status.idle": "2021-04-19T10:21:25.975033Z",
     "shell.execute_reply": "2021-04-19T10:21:25.974219Z",
     "shell.execute_reply.started": "2021-04-19T10:21:24.884551Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import vgg11_bn\n",
    "from torchinfo import summary\n",
    "from scripts import mnistm\n",
    "from scripts import mnist\n",
    "from scripts import train\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:21:25.976474Z",
     "iopub.status.busy": "2021-04-19T10:21:25.976207Z",
     "iopub.status.idle": "2021-04-19T10:21:26.028364Z",
     "shell.execute_reply": "2021-04-19T10:21:26.027432Z",
     "shell.execute_reply.started": "2021-04-19T10:21:25.976450Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def _build_vgg_block(self, num_conv_layers, in_channels, out_channels, kernel_size=3, batchnorm=True, activation=nn.ReLU, maxpool=True):\n",
    "        layers = []\n",
    "        for i in range(num_conv_layers):\n",
    "            if i == 0:\n",
    "                num_channels_in = in_channels\n",
    "            else:\n",
    "                num_channels_in = out_channels\n",
    "            \n",
    "            layers.append(nn.Conv2d(in_channels=num_channels_in, out_channels=out_channels, kernel_size=kernel_size))\n",
    "            if batchnorm:\n",
    "                layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(activation())\n",
    "        \n",
    "        if maxpool:\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def __init__(self, num_classes=10, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            self._build_vgg_block(2, in_channels, 16, activation=nn.SiLU),\n",
    "            self._build_vgg_block(2, 16, 32, activation=nn.SiLU)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.classifier(self.avgpool(self.features(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:21:26.029731Z",
     "iopub.status.busy": "2021-04-19T10:21:26.029452Z",
     "iopub.status.idle": "2021-04-19T10:21:26.363110Z",
     "shell.execute_reply": "2021-04-19T10:21:26.362375Z",
     "shell.execute_reply.started": "2021-04-19T10:21:26.029706Z"
    }
   },
   "outputs": [],
   "source": [
    "net = CNN(in_channels=3)\n",
    "_ = summary(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if the net works on random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:21:26.364246Z",
     "iopub.status.busy": "2021-04-19T10:21:26.364002Z",
     "iopub.status.idle": "2021-04-19T10:21:27.262395Z",
     "shell.execute_reply": "2021-04-19T10:21:27.261468Z",
     "shell.execute_reply.started": "2021-04-19T10:21:26.364220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand((100,3,28,28))\n",
    "y = net(X)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go more in detail in the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:21:27.264705Z",
     "iopub.status.busy": "2021-04-19T10:21:27.264488Z",
     "iopub.status.idle": "2021-04-19T10:21:27.290468Z",
     "shell.execute_reply": "2021-04-19T10:21:27.289719Z",
     "shell.execute_reply.started": "2021-04-19T10:21:27.264686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in torch.Size([100, 3, 28, 28])\n",
      "0 torch.Size([100, 16, 12, 12])\n",
      "1 torch.Size([100, 32, 4, 4])\n",
      "avgpool torch.Size([100, 32, 1, 1])\n",
      "out torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "z = X\n",
    "print(\"in\", z.shape)\n",
    "for i, mod in enumerate(net.features):\n",
    "    z = mod(z)\n",
    "    print(i, z.shape)\n",
    "\n",
    "z = net.avgpool(z)\n",
    "print(\"avgpool\", z.shape)\n",
    "\n",
    "z = net.classifier(z)\n",
    "print(\"out\", z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:21:27.292540Z",
     "iopub.status.busy": "2021-04-19T10:21:27.292234Z",
     "iopub.status.idle": "2021-04-19T10:21:43.708779Z",
     "shell.execute_reply": "2021-04-19T10:21:43.708008Z",
     "shell.execute_reply.started": "2021-04-19T10:21:27.292509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/VanushVaswani/keras_mnistm/releases/download/1.0/keras_mnistm.pkl.gz\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emaballarin/anaconda3/envs/RDDL/lib/python3.8/site-packages/torchvision/datasets/mnist.py:54: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/home/emaballarin/anaconda3/envs/RDDL/lib/python3.8/site-packages/torchvision/datasets/mnist.py:59: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.4639, 0.4676, 0.4199], [0.2534, 0.2380, 0.2618]), # I pre-computed these data\n",
    "])\n",
    "\n",
    "mnistm_train = mnistm.MNISTM(root=\"datasets/MNISTM\", download=True, transform=transforms)\n",
    "mnistm_test = mnistm.MNISTM(root=\"datasets/MNISTM\", train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:21:43.709927Z",
     "iopub.status.busy": "2021-04-19T10:21:43.709654Z",
     "iopub.status.idle": "2021-04-19T10:21:44.356079Z",
     "shell.execute_reply": "2021-04-19T10:21:44.355163Z",
     "shell.execute_reply.started": "2021-04-19T10:21:43.709886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = (mnistm_train.train_data[0].numpy() * 255).astype(\"uint8\")\n",
    "plt.imshow(img)\n",
    "print(\"Ground truth:\", mnistm_train.train_labels[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:21:44.357618Z",
     "iopub.status.busy": "2021-04-19T10:21:44.357215Z",
     "iopub.status.idle": "2021-04-19T10:21:44.364898Z",
     "shell.execute_reply": "2021-04-19T10:21:44.363764Z",
     "shell.execute_reply.started": "2021-04-19T10:21:44.357579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 28, 28, 3]), torch.Size([10000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnistm_test.test_data.shape, mnistm_test.test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:21:44.367006Z",
     "iopub.status.busy": "2021-04-19T10:21:44.366600Z",
     "iopub.status.idle": "2021-04-19T10:21:44.619401Z",
     "shell.execute_reply": "2021-04-19T10:21:44.618702Z",
     "shell.execute_reply.started": "2021-04-19T10:21:44.366968Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emaballarin/anaconda3/envs/RDDL/lib/python3.8/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "batch_train = 128\n",
    "batch_test = 512\n",
    "mnistm_trainloader = DataLoader(mnistm_train, batch_train, shuffle=True, num_workers=8)\n",
    "mnistm_testloader = DataLoader(mnistm_test, batch_test, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:21:44.620770Z",
     "iopub.status.busy": "2021-04-19T10:21:44.620458Z",
     "iopub.status.idle": "2021-04-19T10:21:44.791371Z",
     "shell.execute_reply": "2021-04-19T10:21:44.789847Z",
     "shell.execute_reply.started": "2021-04-19T10:21:44.620738Z"
    }
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(net.parameters(), lr=0.1, weight_decay=5e-4, momentum=.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:21:44.797916Z",
     "iopub.status.busy": "2021-04-19T10:21:44.797313Z",
     "iopub.status.idle": "2021-04-19T10:22:28.063109Z",
     "shell.execute_reply": "2021-04-19T10:22:28.060483Z",
     "shell.execute_reply.started": "2021-04-19T10:21:44.797857Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 --- learning rate 0.10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-de01cbbe4fcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnistm_trainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/repositories/DSSC_DL_2021/labs/scripts/train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc, checkpoint_name, performance, lr_scheduler, device, lr_scheduler_step_on_epoch)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mlr_scheduler_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlr_scheduler_step_on_epoch\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_meter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance_meter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum:.4f} - average: {loss_meter.avg:.4f}; Performance: {performance_meter.avg:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repositories/DSSC_DL_2021/labs/scripts/train.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, lr_scheduler)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# 6. calculate the accuracy for this mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperformance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# 7. update the loss and accuracy AverageMeter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss_meter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repositories/DSSC_DL_2021/labs/scripts/train_utils.py\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(nn_output, ground_truth, k)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcorrect_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_out_classes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mground_truth_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# now getting the accuracy is easy, we just operate the sum of the tensor and divide it by the number of examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train.train_model(net, mnistm_trainloader, loss_fn=nn.CrossEntropyLoss(), optimizer=optim, num_epochs=50, lr_scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:22:35.839239Z",
     "iopub.status.busy": "2021-04-19T10:22:35.838693Z",
     "iopub.status.idle": "2021-04-19T10:22:36.010221Z",
     "shell.execute_reply": "2021-04-19T10:22:36.009297Z",
     "shell.execute_reply.started": "2021-04-19T10:22:35.839183Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"models_push/cnn_mnistm/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:22:37.127973Z",
     "iopub.status.busy": "2021-04-19T10:22:37.127640Z",
     "iopub.status.idle": "2021-04-19T10:22:37.213096Z",
     "shell.execute_reply": "2021-04-19T10:22:37.212387Z",
     "shell.execute_reply.started": "2021-04-19T10:22:37.127943Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"models_push/cnn_mnistm/model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:22:38.307763Z",
     "iopub.status.busy": "2021-04-19T10:22:38.307305Z",
     "iopub.status.idle": "2021-04-19T10:22:41.252109Z",
     "shell.execute_reply": "2021-04-19T10:22:41.251228Z",
     "shell.execute_reply.started": "2021-04-19T10:22:38.307720Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING - loss -- - performance 0.1154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 0.1154)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.test_model(net, mnistm_testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "Transfer Learning is the utilization of a model, built for a given task (**upstream task**) on one or more different tasks (**downstream task**).\n",
    "\n",
    "Usually, in Deep Learning we operate a **model finetuning**, i.e., we use a model fully-trained on the upstream task and we adapt it someway on the downstream task.\n",
    "\n",
    "Fine-tuning comes in different flavors:\n",
    "\n",
    "1. we might just re-use the model for the upstream task without re-training\n",
    "\n",
    "2. we might want to re-train the model on the downstream task leaving fixed the parameters of the majority of the layers, while re-training only the remaining parameters.\n",
    " Usually, we train only the weights in the final fully-connected layer. This method is called **feature extraction**.\n",
    "\n",
    "3. we might want to fully re-train the model, using the `state_dict` from the upstream task as an **initialization** for the training on the downstream task. This is the proper **fine-tuning**.\n",
    "\n",
    "NB: if the ground truth for the downstream task and the upstream task have different domain (e.g. different number of categories), I'll need to modify the architecture of my CNN by **changing the number of output features** of the last layer. This, of course, will mean that the weights in this layer need to be re-initialized, hence, we'll for sure need to resort to strategies 2 or 3.\n",
    "\n",
    "NB2: a common techinque which is operated for **small downstream tasks datasets** is to obtain the features before classification (or, in principle, before any one of the layers of my CNN) and to operate feature extraction by fitting a **linear SVM** on top of these features (similar, in principle, to re-training on the last layer only, but with different loss). This approach has been shown to be much more robust to small dataset size.\n",
    "\n",
    "#### Re-use the same model on MNIST\n",
    "\n",
    "Basically, we wish to do this:\n",
    "\n",
    "![](img/tl1.jpg)\n",
    "\n",
    "We just need to import MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:22:46.530698Z",
     "iopub.status.busy": "2021-04-19T10:22:46.530421Z",
     "iopub.status.idle": "2021-04-19T10:22:47.407420Z",
     "shell.execute_reply": "2021-04-19T10:22:47.405472Z",
     "shell.execute_reply.started": "2021-04-19T10:22:46.530672Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, mnist_train, mnist_test = mnist.get_data()\n",
    "mnist_train.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the shape.\n",
    "\n",
    "**Q**: What do we need to do?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:22:50.438566Z",
     "iopub.status.busy": "2021-04-19T10:22:50.438269Z",
     "iopub.status.idle": "2021-04-19T10:22:50.446901Z",
     "shell.execute_reply": "2021-04-19T10:22:50.446173Z",
     "shell.execute_reply.started": "2021-04-19T10:22:50.438545Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MNIST_3(torch.utils.data.Dataset):\n",
    "    def __init__(self, mnist_dataset, transform=None):\n",
    "        n, h, w = mnist_dataset.data.shape\n",
    "        self.data = mnist_dataset.data.clone().unsqueeze(1).expand(n, 3, h, w)\n",
    "        self.targets = mnist_dataset.targets.clone()\n",
    "        if transform is None:\n",
    "            self.transform = lambda x: x\n",
    "        else:\n",
    "            self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.transform(self.data[index]), self.targets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:22:52.098036Z",
     "iopub.status.busy": "2021-04-19T10:22:52.097685Z",
     "iopub.status.idle": "2021-04-19T10:22:52.135655Z",
     "shell.execute_reply": "2021-04-19T10:22:52.134851Z",
     "shell.execute_reply.started": "2021-04-19T10:22:52.098008Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transforms_mnist = T.Compose([\n",
    "    lambda x: x/255,\n",
    "    T.Normalize([0.1307, 0.1307, 0.1307], [0.3081, 0.3081, 0.3081])\n",
    "])\n",
    "mnist3_train = MNIST_3(mnist_train, transforms_mnist)\n",
    "mnist3_test = MNIST_3(mnist_test, transforms_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:22:53.255683Z",
     "iopub.status.busy": "2021-04-19T10:22:53.255262Z",
     "iopub.status.idle": "2021-04-19T10:22:53.365873Z",
     "shell.execute_reply": "2021-04-19T10:22:53.364994Z",
     "shell.execute_reply.started": "2021-04-19T10:22:53.255659Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f231c0382e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist3_train.data[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:23:18.583962Z",
     "iopub.status.busy": "2021-04-19T10:23:18.583686Z",
     "iopub.status.idle": "2021-04-19T10:23:18.588022Z",
     "shell.execute_reply": "2021-04-19T10:23:18.587275Z",
     "shell.execute_reply.started": "2021-04-19T10:23:18.583938Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mnist_trainloader = DataLoader(mnist3_train, 512, shuffle=False, num_workers=0)\n",
    "mnist_testloader = DataLoader(mnist3_test, 512, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-19T10:23:19.720191Z",
     "iopub.status.busy": "2021-04-19T10:23:19.719930Z",
     "iopub.status.idle": "2021-04-19T10:23:28.048880Z",
     "shell.execute_reply": "2021-04-19T10:23:28.048145Z",
     "shell.execute_reply.started": "2021-04-19T10:23:19.720166Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING - loss -- - performance 0.1691\n",
      "TESTING - loss -- - performance 0.1715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 0.1715)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.test_model(net, mnist_trainloader)\n",
    "train.test_model(net, mnist_testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the performance is *kinda good*, but we'd expect more out of a model on MNIST.\n",
    "\n",
    "#### Feature extraction\n",
    "\n",
    "Basically we do this:\n",
    "\n",
    "![](img/tl2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to do:\n",
    "\n",
    "1. Freeze the training in the layers from the first to the second-to-last by *inhibiting* the `grad` in these layers\n",
    "2. Create the optimizer passing only the parameters of the last layer\n",
    "\n",
    "Notice that, if we consider the `named_parameters` of our net, the weight and bias of the last layer share the name `classifier.1`. Knowing that, we can inhibit the gradient by setting `param.requires_grad` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-19T10:22:28.078987Z",
     "iopub.status.idle": "2021-04-19T10:22:28.079371Z",
     "shell.execute_reply": "2021-04-19T10:22:28.079217Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, pars in net.named_parameters():\n",
    "    if \"classifier.1\" not in name:\n",
    "        pars.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mirroring the strategy above, we can *filter* the parameters being trained by the optimizer with a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-19T10:22:28.080476Z",
     "iopub.status.idle": "2021-04-19T10:22:28.080951Z",
     "shell.execute_reply": "2021-04-19T10:22:28.080720Z"
    }
   },
   "outputs": [],
   "source": [
    "optim_finetune = torch.optim.SGD([pars for name, pars in net.named_parameters() if \"classifier.1\" in name], lr=.1, momentum=.9, weight_decay=5e-4)\n",
    "train.train_model(net, mnist_trainloader, nn.CrossEntropyLoss(), optim_finetune, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper finetuning (ILSVRC2012 → CIFAR10)\n",
    "\n",
    "`torchvision` enables us to directly download pre-trained models from the web. Normally, we can access the weights of a large array of CNNs pretrained on the large-scale image classification dataset `ILSVRC2012`, also known as ImageNet, although the proper ImageNet dataset is a larger version of `ILSVRC2012`.\n",
    "\n",
    "These CNNs have the last layer whose output size is 1000 (the number of classes in `ILSVRC2012`).\n",
    "\n",
    "We wish to fine-tune a pre-trained `vgg11` model on the CIFAR10 dataset.\n",
    "\n",
    "This dataset is comparable in size to MNIST, with 10 categories:\n",
    "\n",
    "![](https://user-images.githubusercontent.com/16641054/46775076-8b17e480-cd40-11e8-9501-89c6fbca36bd.jpg)\n",
    "\n",
    "The images are slightly larger ($32 \\times 32$ instead of $28 \\times 28$) and are encoded as RGB (→ 3 channels).\n",
    "\n",
    "Let's import the dataset and create the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-19T10:22:28.081863Z",
     "iopub.status.idle": "2021-04-19T10:22:28.082229Z",
     "shell.execute_reply": "2021-04-19T10:22:28.082049Z"
    }
   },
   "outputs": [],
   "source": [
    "transform_cifar = T.Compose([T.ToTensor(), T.Normalize([0.4913997551666284, 0.48215855929893703, 0.4465309133731618], [0.24703225141799082, 0.24348516474564, 0.26158783926049628])])\n",
    "\n",
    "cifar_train = CIFAR10(\"datasets/CIFAR\", train=True, transform=transform_cifar, download=True)\n",
    "cifar_test = CIFAR10(\"datasets/CIFAR\", train=False, transform=transform_cifar, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-19T10:22:28.083079Z",
     "iopub.status.idle": "2021-04-19T10:22:28.083379Z",
     "shell.execute_reply": "2021-04-19T10:22:28.083235Z"
    }
   },
   "outputs": [],
   "source": [
    "cifar_trainloader = DataLoader(cifar_train, batch_size=128, shuffle=True)\n",
    "cifar_testloader = DataLoader(cifar_test, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-19T10:22:28.084162Z",
     "iopub.status.idle": "2021-04-19T10:22:28.084466Z",
     "shell.execute_reply": "2021-04-19T10:22:28.084319Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Shape of trainset\", cifar_train.data.shape)\n",
    "print(\"Num classes\", max(cifar_test.targets) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG11 \n",
    "\n",
    "VGG11 is a network similar to the CNN we created before:\n",
    "\n",
    "![](https://www.researchgate.net/publication/336550999/figure/fig1/AS:814110748987402@1571110536721/Figure-S1-Block-diagram-of-the-VGG11-architecture-Adapted-from-https-bitly-2ksX5Eq.png)\n",
    "\n",
    "In addition to the picture above, we also use batchnorm (at the time of the invention of VGG, batchnorm wasn't known)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-19T10:22:28.085316Z",
     "iopub.status.idle": "2021-04-19T10:22:28.085628Z",
     "shell.execute_reply": "2021-04-19T10:22:28.085479Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg = vgg11_bn(pretrained=True)\n",
    "vgg.classifier[-1].out_features = 10 # update the num_classes to reflect the new dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the network performs without fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-19T10:22:28.086400Z",
     "iopub.status.idle": "2021-04-19T10:22:28.086707Z",
     "shell.execute_reply": "2021-04-19T10:22:28.086558Z"
    }
   },
   "outputs": [],
   "source": [
    "train.test_model(vgg, cifar_testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, you'll find how to fine-tune the CNN. It takes around 10 mins on single GPU, so I leave the commands and output as markdown notes.\n",
    "\n",
    "You can find the weights under the `models_push/vgg11_cifar` folder.\n",
    "\n",
    "Then, I'll also show how to train the `VGG11_bn` from scratch and leave the output of the training. You can clearly notice how the performance of the fine-tuned model is much better after 10 epochs (actually, this is not shown in the output, but the performance at epoch 1 is much higher in the pre-trained model).\n",
    "\n",
    "With proper data augmentation and lr scheduling (and maybe a deeper CNN), we can actually reach a test-set accuracy of 94% or higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "optimizer = torch.optim.Adam(vgg.parameters())\n",
    "train.train_model(vgg, cifar_trainloader, nn.CrossEntropyLoss(), optimizer, 20)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 10 completed. Loss - total: 5852.242087006569 - average: 0.11704484174013138; Performance: 0.96644\n",
    "\n",
    "Epoch 11 completed. Loss - total: 5551.681170940399 - average: 0.11103362341880799; Performance: 0.9682\n",
    "\n",
    "Epoch 12 completed. Loss - total: 5018.1823744773865 - average: 0.10036364748954774; Performance: 0.9718\n",
    "\n",
    "Epoch 13 completed. Loss - total: 17709.622314095497 - average: 0.35419244628190993; Performance: 0.92888\n",
    "\n",
    "Epoch 14 completed. Loss - total: 17280.205318450928 - average: 0.34560410636901856; Performance: 0.89974\n",
    "\n",
    "Epoch 15 completed. Loss - total: 3871.2677970528603 - average: 0.0774253559410572; Performance: 0.97692\n",
    "\n",
    "Epoch 16 completed. Loss - total: 2241.83546949178 - average: 0.0448367093898356; Performance: 0.9872\n",
    "\n",
    "Epoch 17 completed. Loss - total: 2088.5263759568334 - average: 0.041770527519136666; Performance: 0.9882\n",
    "\n",
    "Epoch 18 completed. Loss - total: 2305.237785771489 - average: 0.04610475571542978; Performance: 0.98758\n",
    "\n",
    "Epoch 19 completed. Loss - total: 2979.0519604235888 - average: 0.05958103920847178; Performance: 0.98364\n",
    "\n",
    "Epoch 20 completed. Loss - total: 3053.6747498363256 - average: 0.06107349499672651; Performance: 0.98368\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "train.test_model(vgg, cifar_testloader)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TESTING - loss -- - performance 0.8552\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "torch.save(vgg.state_dict, \"models/vgg11_cifar/vgg11_tl.pt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "vgg2 = vgg11_bn(pretrained=False, num_classes=10)\n",
    "optimizer = torch.optim.Adam(vgg2.parameters())\n",
    "train.train_model(vgg2, cifar_trainloader, nn.CrossEntropyLoss(), optimizer, 20)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 10 completed. Loss - total: 48581.04235076904 - average: 0.9716208470153809; Performance: 0.66648\n",
    "\n",
    "Epoch 11 completed. Loss - total: 39186.03456878662 - average: 0.7837206913757324; Performance: 0.72996\n",
    "\n",
    "Epoch 12 completed. Loss - total: 35287.52504825592 - average: 0.7057505009651184; Performance: 0.76124\n",
    "\n",
    "Epoch 13 completed. Loss - total: 38614.65808200836 - average: 0.7722931616401673; Performance: 0.7412\n",
    "\n",
    "Epoch 14 completed. Loss - total: 30812.80309009552 - average: 0.6162560618019104; Performance: 0.79158\n",
    "\n",
    "Epoch 15 completed. Loss - total: 30947.483282089233 - average: 0.6189496656417847; Performance: 0.79536\n",
    "\n",
    "Epoch 16 completed. Loss - total: 30550.97905731201 - average: 0.6110195811462402; Performance: 0.80206\n",
    "\n",
    "Epoch 17 completed. Loss - total: 23598.16388607025 - average: 0.471963277721405; Performance: 0.85028\n",
    "\n",
    "Epoch 18 completed. Loss - total: 32089.43673324585 - average: 0.641788734664917; Performance: 0.80358\n",
    "\n",
    "Epoch 19 completed. Loss - total: 37266.2276263237 - average: 0.745324552526474; Performance: 0.78556\n",
    "\n",
    "Epoch 20 completed. Loss - total: 23250.735929965973 - average: 0.4650147185993195; Performance: 0.84466\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "train.test_model(vgg2, cifar_testloader)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TESTING - loss -- - performance 0.7927\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "torch.save(vgg.state_dict(), \"models/vgg11_cifar/vgg11_scratch.pt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNets [3](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)\n",
    "\n",
    "As told before, the building block of resnets is the **residual block**, which enables the network to train faster and avoid the vanishing gradient problem:\n",
    "\n",
    "![](https://www.aiuai.cn/uploads/sina/5ce8dfe46e373.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the image above, we see two popular implementations of the residual block.\n",
    "\n",
    "* the first implementation is the simplest one. We have two $3\\times 3$, 1-padding, convolutions and the same number of filters as the incoming data.\n",
    "\n",
    " We can obtain the spatial dimension of the output (i.e., height and width) with this formula (found in [1](https://arxiv.org/abs/1603.07285)): $\\text{output_size} = \\text{in_size} + 2\\cdot \\text{pad} - \\text{ker_size} + 1$. Since $p=1,~\\text{pad}=2,~\\text{ker_size}=3$, then $\\text{output_size} = \\text{in_size}$. Since we do not vary the number of channels in the data, we can safely sum the incoming data to the output of the second convolutional layer.\n",
    "    * In some layers (architecture-dependent) a *subsampling* may be required (i.e., we need to decrease the spatial dimensions of the data). This is solved by applying a convolution with stride of 2 (the moving window *shifts* by 2 pixels instead of 1).\n",
    "     \n",
    "     In this case, **we need to downsample the data in the skip connections** as well. This is done via **$1\\times 1$ convolutions with stride of 2**.\n",
    "\n",
    " Let's implement this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-19T10:22:28.087609Z",
     "iopub.status.idle": "2021-04-19T10:22:28.087930Z",
     "shell.execute_reply": "2021-04-19T10:22:28.087777Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, n_channels, activ=nn.ReLU, bias=False, batchnorm=True, downsample=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=n_channels, kernel_size=3, padding=1, bias=bias, stride=(2 if downsample else 1))\n",
    "        self.bnorm1 = nn.BatchNorm2d(n_channels)\n",
    "        self.activ1 = activ()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=3, padding=1, bias=bias)\n",
    "        self.bnorm2 = nn.BatchNorm2d(n_channels)\n",
    "        self.activ2 = activ()\n",
    "\n",
    "        if downsample:\n",
    "            self.conv_down = nn.Conv2d(in_channels=in_channels, out_channels=n_channels, kernel_size=1, stride=2, padding=1, bias=bias)\n",
    "            self.bnorm_down = nn.BatchNorm2d(n_channels) # note that this is necessary in order to put data in the skip connection and data in the regular stream on the same domain!\n",
    "            \n",
    "    def forward(self, X):\n",
    "        out = self.activ1(self.bnorm1(self.conv1(X)))\n",
    "        # let us complete this\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the second implementation is also called a **bottleneck layer** and operates an expansion of the number of channels (in a fashion similar to VGG). Differently from the residual block:\n",
    "    * the bottleneck has 3 instead of 2 convolutional layers\n",
    "        * the 1st and 3rd layer operate a $1\\times 1$ correlation with no padding (thus, leaving the image untouched in size)\n",
    "        * the 2nd is a regular $3\\times 3$ correlation which we saw before (padding of 1); however this can be a \"**dilated convolution**\":\n",
    "         ![](https://www.researchgate.net/publication/320195101/figure/fig2/AS:669211164692494@1536563783748/Dilated-convolution-On-the-left-we-have-the-dilated-convolution-with-dilation-rate-r.png)\n",
    "    * the width of the convolutional layers is controlled by a parameter (it should be $\\geq$ than the width of the incoming data -- canonical values are 64, 128...)\n",
    "    * the output of the block has a number of channels which is usually $\\times 4$ the width of the block.\n",
    "    * if a **downsampling** is required, it's operated by setting the stride to 2 for the second convolutional layer only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-19T10:22:28.088697Z",
     "iopub.status.idle": "2021-04-19T10:22:28.089011Z",
     "shell.execute_reply": "2021-04-19T10:22:28.088859Z"
    }
   },
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, n_channels, activ=nn.ReLU, bias=False, batchnorm=True, dilation_rate=1, expansion_factor=4, downsample=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=n_channels, kernel_size=1, padding=0, bias=bias)\n",
    "        self.bnorm1 = nn.BatchNorm2d(n_channels)\n",
    "        self.activ1 = activ()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=3, padding=1, bias=bias, dilation=dilation_rate,  stride=(2 if downsample else 1))\n",
    "        self.bnorm2 = nn.BatchNorm2d(n_channels)\n",
    "        self.activ2 = activ()\n",
    "\n",
    "        out_channels = n_channels * expansion_factor\n",
    "        self.conv3 = nn.Conv2d(in_channels=n_channels, out_channels=out_channels, kernel_size=1, padding=0, bias=bias)\n",
    "        self.bnorm3 = nn.BatchNorm2d(out_channels)\n",
    "        self.activ3 = activ()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # this is wrong and serves just as a way to check the dims of the output\n",
    "        out = self.activ1(self.bnorm1(self.conv1(X)))\n",
    "        print(out.shape)\n",
    "        out = self.activ2(self.bnorm2(self.conv2(out)))\n",
    "        print(out.shape)\n",
    "        out = self.activ3(self.bnorm3(self.conv3(out)))\n",
    "        print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-19T10:22:28.089815Z",
     "iopub.status.idle": "2021-04-19T10:22:28.090128Z",
     "shell.execute_reply": "2021-04-19T10:22:28.089976Z"
    }
   },
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    BottleneckBlock(64, 64),\n",
    "    BottleneckBlock(256, 64),\n",
    "    BottleneckBlock(256, 128)\n",
    ")\n",
    "X = torch.rand((24, 64, 100, 100))\n",
    "_ = net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: we're missing a step to \"close the loop\" on the skip connection. What is it?\n",
    "\n",
    "*Hint*: have a look at the shapes of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet18\n",
    "\n",
    "Resnet18 is the simplest resnet architecture (18 total layers) presented in [2]. Let's see how we can implement it:\n",
    "\n",
    "![](https://pytorch.org/assets/images/resnet.png)\n",
    "\n",
    "From the image above, we can see that the Resnet18 is composed of one initial convolution (preparatory layer), four residual blocks, and one flattening/classification block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparatory layer\n",
    "\n",
    "Is a $7\\times 7$ convolution with stride 2 and 64 output channels, followed by batchnorm, relu, and maxpooling with a $3\\times 3$ kernel (!) and stride 2.\n",
    "\n",
    "Its function is mainly to reduce the image in size while extracting a large array of low-level features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-19T10:22:28.090877Z",
     "iopub.status.idle": "2021-04-19T10:22:28.091191Z",
     "shell.execute_reply": "2021-04-19T10:22:28.091038Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNetPrep(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=64, activ=nn.ReLU, bias=False):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=7, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activ(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Residual blocks\n",
    "\n",
    "We have eight residual blocks with increasing number of channels, VGG-style (64,64→128,128→256,256→512,512).\n",
    "\n",
    "Whenever the number of channels is increased, we have also a subsampling.\n",
    "\n",
    "##### Average pooling and classification\n",
    "\n",
    "We flatten the array with \"adaptive average pooling\", which is a nice way to say that we're averaging across width.\n",
    "\n",
    "Now, we can start preparing our Resnet18 class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-19T10:22:28.092310Z",
     "iopub.status.idle": "2021-04-19T10:22:28.092665Z",
     "shell.execute_reply": "2021-04-19T10:22:28.092488Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10, in_channels=3, base_width=64, activ=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.prep = ResNetPrep(in_channels=in_channels, out_channels=base_width, activ=activ)\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels=base_width, n_channels=base_width, activ=activ),\n",
    "            ResidualBlock(in_channels=base_width, n_channels=base_width, activ=activ),\n",
    "\n",
    "            ResidualBlock(in_channels=base_width, n_channels=base_width * 2, activ=activ, downsample=True),\n",
    "            ResidualBlock(in_channels=base_width * 2, n_channels=base_width * 2, activ=activ),\n",
    "\n",
    "            ResidualBlock(in_channels=base_width * 2, n_channels=base_width * 4, activ=activ, downsample=True),\n",
    "            ResidualBlock(in_channels=base_width * 4, n_channels=base_width * 4, activ=activ),\n",
    "\n",
    "            ResidualBlock(in_channels=base_width * 4, n_channels=base_width * 8, activ=activ, downsample=True),\n",
    "            ResidualBlock(in_channels=base_width * 8, n_channels=base_width * 8, activ=activ)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.classifier = nn.Linear(in_features=base_width*8, out_features=num_classes, bias=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = self.prep(X)\n",
    "        out = self.res_blocks(out)\n",
    "        out = self.classifier(self.avgpool(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-19T10:22:28.093555Z",
     "iopub.status.idle": "2021-04-19T10:22:28.093862Z",
     "shell.execute_reply": "2021-04-19T10:22:28.093715Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = summary(ResNet18())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-19T10:22:28.094955Z",
     "iopub.status.idle": "2021-04-19T10:22:28.095490Z",
     "shell.execute_reply": "2021-04-19T10:22:28.095227Z"
    }
   },
   "outputs": [],
   "source": [
    "ResNet18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide ResNets\n",
    "\n",
    "Wide ResNets ([4](https://arxiv.org/abs/1605.07146)) are an extension of ResNets allowing for \"wider\" nets, not necessarily very deep.\n",
    "\n",
    "While the focal point of ResNets is to expand towards depth, a feat that was difficult to achieve in other nets like VGG, because of the vanishing gradient, the inventors of Wide ResNets argue that there can be a lot of components inside a regular ResNet with little contribution to the output; on the other hand, they noticed that, by decreasing the number of layers while increasing the number of channels, the resulting wide CNN can outperform the deep one.\n",
    "\n",
    "Wide ResNets, for instance, allow to reach accuracy > 95% on CIFAR10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deconvolution\n",
    "\n",
    "This is a summary of what found in [1](https://arxiv.org/abs/1603.07285)\n",
    "\n",
    "Deconvolution (*inverse convolution, transposed convolution*) is a process analogous to correlation. Usually, given a generic square matrix $M\\in \\mathbb{R}^{n\\times n}$ and kernel $K\\in \\mathbb{R}^{k\\times k},~k<n$, we have:\n",
    "\n",
    "$M \\otimes K = L$, with $L\\in\\mathbb{R}^{(n-k+1)~\\times~(n-k+1)}$\n",
    "\n",
    "With the deconvolution we want to solve the \"inverse\" problem, i.e., given the same kernel $K$, from a smaller matrix $H$, obtain a larger matrix $N$.\n",
    "\n",
    "$H \\tilde\\otimes K = N$\n",
    "\n",
    "The first equation can be transformed into a matrix-matrix multiplication prolbem by reshaping the kernel in the following way:\n",
    "\n",
    "$\\mathcal{K} =\n",
    "\\begin{Bmatrix}\n",
    "k_{00} & k_{01} & k_{02} & 0 & k_{10} & k_{11} & k_{12} & 0 & k_{20} & k_{21} & k_{22} & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & k_{00} & k_{01} & k_{02} & 0 & k_{10} & k_{11} & k_{12} & 0 & k_{20} & k_{21} & k_{22} & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & k_{00} & k_{01} & k_{02} & 0 & k_{10} & k_{11} & k_{12} & 0 & k_{20} & k_{21} & k_{22} & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & k_{00} & k_{01} & k_{02} & 0 & k_{10} & k_{11} & k_{12} & 0 & k_{20} & k_{21} & k_{22} \\\\\n",
    "\\end{Bmatrix}\n",
    "$\n",
    "\n",
    "Then, the convolution can be rethought as:\n",
    "\n",
    "$ \\mathcal{K} \\cdot\\text{vec}(M) = \\text{vec}(L)$, with a later row-wise reshaping of $\\text{vec}(L)$\n",
    "\n",
    "Similarly, we can view the deconvolution problem as:\n",
    "\n",
    "$ \\mathcal{K}^\\top \\cdot\\text{vec}(H) = \\text{vec}(N)$\n",
    "\n",
    "dimensions: $(16\\times4) \\cdot (4\\times1) = (16\\times1)$\n",
    "\n",
    "Thus, by properly adjusting the weights $\\begin{Bmatrix}k_{00} & k_{01} & k_{02} & k_{10} & k_{11} & k_{12} & k_{20} & k_{21} & k_{22}\\end{Bmatrix}$, we may also be able to recover the matrix $M$ starting from $L$.\n",
    "\n",
    "#### Where is it used?\n",
    "\n",
    "Deconvolution is used in tasks requiring an \"intelligent upsampling\", or we might call it an \"upsampling with feature identification\". A trivial upsampling can be achieved by using techniques like \n",
    "\n",
    "*max-unpooling*: ![](https://www.researchgate.net/publication/335754645/figure/fig4/AS:839690152329216@1577209141459/Example-of-an-unpooling-process-Indices-of-max-pooling-are-kept-up-and-reused-to.png)\n",
    "\n",
    "*regular convolution with a lot of padding*: ![](https://miro.medium.com/max/2588/1*uk4KJEtyDuPOipfG4yd-WA.gif)\n",
    "\n",
    "but the final result will be really unsatisfying.\n",
    "\n",
    "Upsampling may be required e.g. in tasks where the output size must be the same as the input size.\n",
    "\n",
    "This strategy is often employed for tasks of **semantic segmentation**\n",
    "\n",
    "![](https://i.pinimg.com/originals/62/d8/5a/62d85a6488ffa85bde7af3a9a8ed70ff.gif)\n",
    "\n",
    "The task can be summarized with this phrase: instead of trying to classify the whole image, we wish to classify each pixel within a large image.\n",
    "\n",
    "One of the most succesful CNNs for image segmentation is U-Net, which was originally proposed for medical imaging.\n",
    "\n",
    "![](img/unet_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### U-Net [3](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "The architecture is a composition of two parts: a contracting module and an expanding module.\n",
    "\n",
    "The contracting module is a sequence of VGG convolutional blocks. \n",
    "\n",
    "After reaching the bottom part, we begin upsampling the image following the inverse of the scheme from the contracting module, with an additional operation: we concatenate the output of the upsampling with the output from the last convolutional layer of the corresponding block (as in the image). Thus, if the upsampling yields a 256-channel output, we concatenate this output with the output of the last 256-channel convolutional layer from the contracting module. This leaves us with a 512-channel tensor which we convolve to 256-channels once again. Note that, if the spatial dimensions of the data from the contracting module doesn't match those of the upsampled data, cropping is operated so that we can safely concatenate the two tensors.\n",
    "\n",
    "Actually, the original implementation of U-Net operates a semantic segmentation on a window which is approximately 2/3 of the original image (there will hence be a leftover band of pixels outside the center of the image). In the image below, the white thin lines represent the area that will be subject to the segmentation.\n",
    "\n",
    "![](img/unet_crop.jpg)\n",
    "\n",
    "For what concerns the output, instead, we end up with a tensor of shape $h^\\prime \\times w^\\prime \\times C$, where $C$ denotes the number of the classes we want to operate segmentation (logically speaking, **if we want to classify each pixel, we wish to produce a softmax for each pixel**).\n",
    "\n",
    "![](img/unet_last.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "Taking inspiration from the picture above, implement a U-Net-style CNN with the following specs:\n",
    "\n",
    "1. All convolutions must use a $3\\times 3$ kernel and **leave the spatial dimensions (i.e. height, width) of the input untouched**.\n",
    "2. Downsampling in the contracting part is performed via maxpooling with a $2\\times 2$ kernel and stride of 2.\n",
    "3. Upsampling is operated by a deconvolution with a $2\\times 2$ kernel and stride of 2. The PyTorch module that implements the deconvolution is `nn.ConvTranspose2d`\n",
    "4. The final layer of the expanding part has only 1 channel \n",
    "    * between how many classes are we discriminating?\n",
    "\n",
    "Create a network class with (at least) a `__init__` and a `forward` method. Please resort to additional structures (e.g., `nn.Module`s, private methods...) if you believe it helps readability of your code.\n",
    "\n",
    "Test, at least with random data, that the network is doing the correct tensor operations and that the output has the correct shape (e.g., use `assert`s in your code to see if the byproduct is of the expected shape).\n",
    "\n",
    "*Note: the overall organization of your work can greatly improve readability and understanding of your code by others. Please consider preparing your notebook in an organized fashion so that we can better understand (and correct) your implementation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1](https://arxiv.org/abs/1603.07285) Dumoulin, Vincent, and Francesco Visin. \"A guide to convolution arithmetic for deep learning.\"\n",
    "\n",
    "[2](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition\n",
    "\n",
    "[3](https://arxiv.org/abs/1505.04597) Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" International Conference on Medical image computing and computer-assisted intervention.\n",
    "\n",
    "[4](https://arxiv.org/abs/1605.07146) Zagoruyko, Sergey, and Nikos Komodakis. \"Wide residual networks.\"\n",
    "\n",
    "[5](https://arxiv.org/abs/1409.1556v6) Simonyan and Zisserman. \"Very Deep Convolutional Networks for Large-Scale Image Recognition.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-RDDL_W]",
   "language": "python",
   "name": "conda-env-.conda-RDDL_W-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "8aec2e4cca6a43ecda9b11f31ea0f9f4b012d28e6de8cbdf64a5e136ca9a5fb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
