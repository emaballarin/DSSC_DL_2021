{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Homework \\#05\n",
    "### Deep Learning Course $\\in$ DSSC @ UniTS (Spring 2021)  \n",
    "\n",
    "#### Submitted by [Emanuele Ballarin](mailto:emanuele@ballarin.cc)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:\n",
    "\n",
    "We start off by importing all the libraries, modules, classes and functions we are going to use *today*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System interaction\n",
    "import os\n",
    "\n",
    "# Typing\n",
    "from torch import Tensor\n",
    "\n",
    "# Tensor computation and ANNs\n",
    "import torch        # Backward compatibility\n",
    "import torch as th  # Forward compatibility\n",
    "\n",
    "# Pruning utilities\n",
    "from imp_roved import IdxSet, Mask, paramsplit, maskterialize, magnitude_pruning, mask_size\n",
    "\n",
    "# Scripted easers\n",
    "from scripts import mnist, train_utils, architectures, train\n",
    "from scripts.torch_utils import use_gpu_if_possible\n",
    "from scripts.train_utils import accuracy, AverageMeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Training with pruning* routine\n",
    "\n",
    "Taken from the provided *Jupyter* notebook, and slightly adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, mask, layers_to_prune, params_type_to_prune):\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        y_hat = model(X)\n",
    "\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if mask is not None:\n",
    "            for name, param in model.named_parameters():\n",
    "\n",
    "                layname = paramsplit(name, 1)[0]\n",
    "                parname = paramsplit(name, 1)[1]\n",
    "\n",
    "                if (not layers_to_prune or layname in layers_to_prune) and (\n",
    "                    not params_type_to_prune or parname in params_type_to_prune\n",
    "                ):\n",
    "                    param.grad *= maskterialize(param.grad.numel(), mask[layname][parname]).view(param.grad.shape).to(device)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = performance(y_hat, y)\n",
    "\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy, lr_scheduler=None, device=None, mask=None, layers_to_prune=None, params_type_to_prune=[\"weight\", \"bias\"]):\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "\n",
    "    if device is None:\n",
    "        device = use_gpu_if_possible()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} --- learning rate {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, mask, layers_to_prune, params_type_to_prune)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPCustom(\n",
      "  (layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=784, out_features=16, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): Linear(in_features=16, out_features=32, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (8): ReLU()\n",
      "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Linear(in_features=64, out_features=10, bias=True)\n",
      "    (11): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    {\"n_in\": 784, \"n_out\": 16, \"batchnorm\": False},\n",
    "    {\"n_out\": 32, \"batchnorm\": True},\n",
    "    {\"n_out\": 64, \"batchnorm\": True},\n",
    "    {\"n_out\": 10, \"batchnorm\": True}\n",
    "]\n",
    "# MLPCustom is a net architecture I prepared to create MLPs with less code to write than before.\n",
    "# See the implementation in the `architectures` script for further insights\n",
    "net = architectures.MLPCustom(layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, _, _ = mnist.get_data()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = th.optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 --- learning rate 0.10000\n",
      "Epoch 1 completed. Loss - total: 23144.67014503479 - average: 0.3857445024172465; Performance: 0.8955833333333333\n",
      "Epoch 2 --- learning rate 0.10000\n",
      "Epoch 2 completed. Loss - total: 11758.425701618195 - average: 0.1959737616936366; Performance: 0.9435666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": "(11758.425701618195, 0.9435666666666667)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(net, trainloader, loss_fn, optimizer, num_epochs=2, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymask = magnitude_pruning(net, 0.6, set([\"1\", \"4\", \"7\", \"10\"]), set([\"weight\", \"bias\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "9519"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_size(mymask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 --- learning rate 0.10000\n",
      "Epoch 1 completed. Loss - total: 9833.770930290222 - average: 0.1638961821715037; Performance: 0.9519333333333333\n",
      "Epoch 2 --- learning rate 0.10000\n",
      "Epoch 2 completed. Loss - total: 8796.41000509262 - average: 0.14660683341821035; Performance: 0.95625\n"
     ]
    },
    {
     "data": {
      "text/plain": "(8796.41000509262, 0.95625)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(net, trainloader, loss_fn, optimizer, num_epochs=2, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"], mask=mymask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymask = magnitude_pruning(net, 0.6, set([\"1\", \"4\", \"7\", \"10\"]), set([\"weight\", \"bias\"]), mask=mymask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "13327"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_size(mymask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 --- learning rate 0.10000\n",
      "Epoch 1 completed. Loss - total: 16329.837278842926 - average: 0.2721639546473821; Performance: 0.91915\n",
      "Epoch 2 --- learning rate 0.10000\n",
      "Epoch 2 completed. Loss - total: 13796.016090869904 - average: 0.2299336015144984; Performance: 0.9305\n"
     ]
    },
    {
     "data": {
      "text/plain": "(13796.016090869904, 0.9305)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(net, trainloader, loss_fn, optimizer, num_epochs=2, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"], mask=mymask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymask = magnitude_pruning(net, 0.6, set([\"1\", \"4\", \"7\", \"10\"]), set([\"weight\", \"bias\"]), mask=mymask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "14850"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_size(mymask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 --- learning rate 0.10000\n",
      "Epoch 1 completed. Loss - total: 35785.282616615295 - average: 0.5964213769435882; Performance: 0.8122333333333334\n",
      "Epoch 2 --- learning rate 0.10000\n",
      "Epoch 2 completed. Loss - total: 30058.1871881485 - average: 0.5009697864691416; Performance: 0.8438666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": "(30058.1871881485, 0.8438666666666667)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(net, trainloader, loss_fn, optimizer, num_epochs=2, layers_to_prune=[\"1\", \"4\", \"7\", \"10\"], mask=mymask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('RDDL': conda)",
   "name": "python388jvsc74a57bd0eb8633c4d4e251251708d3c7ece77ee33d393b5bf4628cd3b0e51f052595f5d6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "8aec2e4cca6a43ecda9b11f31ea0f9f4b012d28e6de8cbdf64a5e136ca9a5fb0"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}