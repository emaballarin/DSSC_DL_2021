{"cells": [{"cell_type": "markdown", "metadata": {"id": "WZnV3RNgU0Mi"}, "source": ["# Training a *SotA MLP SoftMax classifier* on the *MNIST* dataset"]}, {"cell_type": "code", "execution_count": 1, "metadata": {"id": "Ra6EQvDLUxqG"}, "outputs": [], "source": ["# Data download:\n", "import os\n", "\n", "# NNets & co.:\n", "import numpy as np\n", "import torch as th\n", "import torch.nn.functional as F\n", "from torch.optim.lr_scheduler import StepLR\n", "from ebtorch.nn import FCBlock\n", "\n", "# Data(set) handling\n", "from torch.utils.data import DataLoader\n", "from torchvision.datasets import MNIST\n", "from torchvision.transforms import ToTensor, Normalize, Compose, Lambda\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"id": "ANKugUNmUyFo"}, "outputs": [], "source": ["# MNIST DataLoader(s) builder\n", "\n", "\n", "def spawn_mnist_loaders(\n", "    data_root=\"datasets/\",\n", "    batch_size_train=256,\n", "    batch_size_test=512,\n", "    cuda_accel=False,\n", "    **kwargs\n", "):\n", "\n", "    os.makedirs(data_root, exist_ok=True)\n", "\n", "    transforms = Compose(\n", "        [\n", "            ToTensor(),\n", "            Normalize((0.1307,), (0.3081,)),  # usual magic constants for MNIST\n", "            Lambda(lambda x: th.flatten(x)),\n", "        ]\n", "    )\n", "\n", "    trainset = MNIST(data_root, train=True, transform=transforms, download=True)\n", "    testset = MNIST(data_root, train=False, transform=transforms, download=True)\n", "\n", "    cuda_args = {}\n", "    if cuda_accel:\n", "        cuda_args = {\"num_workers\": 1, \"pin_memory\": True}\n", "\n", "    trainloader = DataLoader(\n", "        trainset, batch_size=batch_size_train, shuffle=True, **cuda_args\n", "    )\n", "    testloader = DataLoader(\n", "        trainset, batch_size=batch_size_test, shuffle=False, **cuda_args\n", "    )\n", "\n", "    return trainloader, testloader\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"id": "0xgs3EZBUyJ4"}, "outputs": [], "source": ["# Train / Test tooling\n", "\n", "\n", "def train_epoch(\n", "    model, device, train_loader, loss_fn, optimizer, epoch, print_every_nep\n", "):\n", "    model.train()\n", "    for batch_idx, (data, target) in enumerate(train_loader):\n", "        data, target = data.to(device), target.to(device)\n", "        optimizer.zero_grad()\n", "        output = model(data)\n", "        loss = loss_fn(output, target)\n", "        loss.backward()\n", "        optimizer.step()\n", "        if batch_idx % print_every_nep == 0:\n", "            print(\n", "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n", "                    epoch,\n", "                    batch_idx * len(data),\n", "                    len(train_loader.dataset),\n", "                    100.0 * batch_idx / len(train_loader),\n", "                    loss.item(),\n", "                )\n", "            )\n", "\n", "\n", "def test(model, device, test_loader, loss_fn):\n", "    model.eval()\n", "    test_loss = 0\n", "    correct = 0\n", "    with th.no_grad():\n", "        for data, target in test_loader:\n", "            data, target = data.to(device), target.to(device)\n", "            output = model(data)\n", "            test_loss += loss_fn(\n", "                output, target, reduction=\"sum\"\n", "            ).item()  # sum up batch loss\n", "            pred = output.argmax(\n", "                dim=1, keepdim=True\n", "            )  # get the index of the max log-probability\n", "            correct += pred.eq(target.view_as(pred)).sum().item()\n", "\n", "    test_loss /= len(test_loader.dataset)\n", "\n", "    print(\n", "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n", "            test_loss,\n", "            correct,\n", "            len(test_loader.dataset),\n", "            100.0 * correct / len(test_loader.dataset),\n", "        )\n", "    )\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"id": "i2aclwpGUyOP"}, "outputs": [], "source": ["device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"id": "PtJNzN33cX_e"}, "outputs": [], "source": ["# Hyperparameters & co.\n", "\n", "minibatch_size_train: int = 32  # (cfr. Masters & Luschi, 2018)\n", "minibatch_size_test: int = 512\n", "\n", "nrepochs = 20\n", "\n", "lossfn = F.nll_loss\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"id": "N8YTl9NTUySq"}, "outputs": [], "source": ["train_loader, test_loader = spawn_mnist_loaders(\n", "    batch_size_train=minibatch_size_test,\n", "    batch_size_test=minibatch_size_test,\n", "    cuda_accel=True,\n", ")\n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"id": "A5QVDGBfUyW7"}, "outputs": [], "source": ["model = FCBlock(\n", "    28 * 28,\n", "    [150, 40],\n", "    10,\n", "    hactiv=lambda x: F.leaky_relu_(x, negative_slope=0.06),\n", "    oactiv=lambda x: F.log_softmax(x, dim=1),\n", "    bias=True,\n", ").to(device)\n", "optimizer = th.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, nesterov=True)\n", "scheduler = StepLR(optimizer, step_size=2, gamma=0.7)  # or a lot of patience :)\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Cda0zuTDUyZQ", "outputId": "27fbb0b9-0bc2-453b-a082-66a35674e9f0"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304295\n", "Train Epoch: 1 [5120/60000 (8%)]\tLoss: 0.829657\n", "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.670204\n", "Train Epoch: 1 [15360/60000 (25%)]\tLoss: 0.364339\n", "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.293912\n", "Train Epoch: 1 [25600/60000 (42%)]\tLoss: 0.248731\n", "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.157993\n", "Train Epoch: 1 [35840/60000 (59%)]\tLoss: 0.185503\n", "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.161975\n", "Train Epoch: 1 [46080/60000 (76%)]\tLoss: 0.153820\n", "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.197868\n", "Train Epoch: 1 [56320/60000 (93%)]\tLoss: 0.199731\n", "\n", "Test set: Average loss: 0.1617, Accuracy: 56918/60000 (95%)\n", "\n", "\n", "Test set: Average loss: 0.1617, Accuracy: 56918/60000 (95%)\n", "\n", "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.143456\n", "Train Epoch: 2 [5120/60000 (8%)]\tLoss: 0.177902\n", "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.168497\n", "Train Epoch: 2 [15360/60000 (25%)]\tLoss: 0.182525\n", "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.159962\n", "Train Epoch: 2 [25600/60000 (42%)]\tLoss: 0.126228\n", "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.145277\n", "Train Epoch: 2 [35840/60000 (59%)]\tLoss: 0.146878\n", "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.088134\n", "Train Epoch: 2 [46080/60000 (76%)]\tLoss: 0.134269\n", "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.105493\n", "Train Epoch: 2 [56320/60000 (93%)]\tLoss: 0.099883\n", "\n", "Test set: Average loss: 0.0819, Accuracy: 58541/60000 (98%)\n", "\n", "\n", "Test set: Average loss: 0.0819, Accuracy: 58541/60000 (98%)\n", "\n", "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.091712\n", "Train Epoch: 3 [5120/60000 (8%)]\tLoss: 0.062578\n", "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.057294\n", "Train Epoch: 3 [15360/60000 (25%)]\tLoss: 0.100857\n", "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.091403\n", "Train Epoch: 3 [25600/60000 (42%)]\tLoss: 0.106477\n", "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.057572\n", "Train Epoch: 3 [35840/60000 (59%)]\tLoss: 0.062746\n", "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.085763\n", "Train Epoch: 3 [46080/60000 (76%)]\tLoss: 0.080712\n", "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.057865\n", "Train Epoch: 3 [56320/60000 (93%)]\tLoss: 0.072054\n", "\n", "Test set: Average loss: 0.0676, Accuracy: 58770/60000 (98%)\n", "\n", "\n", "Test set: Average loss: 0.0676, Accuracy: 58770/60000 (98%)\n", "\n", "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.093922\n", "Train Epoch: 4 [5120/60000 (8%)]\tLoss: 0.073329\n", "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.047699\n", "Train Epoch: 4 [15360/60000 (25%)]\tLoss: 0.038891\n", "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.059778\n", "Train Epoch: 4 [25600/60000 (42%)]\tLoss: 0.058697\n", "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.070508\n", "Train Epoch: 4 [35840/60000 (59%)]\tLoss: 0.040455\n", "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.045455\n", "Train Epoch: 4 [46080/60000 (76%)]\tLoss: 0.086081\n", "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.057214\n", "Train Epoch: 4 [56320/60000 (93%)]\tLoss: 0.067967\n", "\n", "Test set: Average loss: 0.0567, Accuracy: 58931/60000 (98%)\n", "\n", "\n", "Test set: Average loss: 0.0567, Accuracy: 58931/60000 (98%)\n", "\n", "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.076006\n", "Train Epoch: 5 [5120/60000 (8%)]\tLoss: 0.044065\n", "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.040533\n", "Train Epoch: 5 [15360/60000 (25%)]\tLoss: 0.032306\n", "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.066222\n", "Train Epoch: 5 [25600/60000 (42%)]\tLoss: 0.038849\n", "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.023119\n", "Train Epoch: 5 [35840/60000 (59%)]\tLoss: 0.061060\n", "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.049193\n", "Train Epoch: 5 [46080/60000 (76%)]\tLoss: 0.041946\n", "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.040864\n", "Train Epoch: 5 [56320/60000 (93%)]\tLoss: 0.033657\n", "\n", "Test set: Average loss: 0.0332, Accuracy: 59433/60000 (99%)\n", "\n", "\n", "Test set: Average loss: 0.0332, Accuracy: 59433/60000 (99%)\n", "\n", "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.044354\n", "Train Epoch: 6 [5120/60000 (8%)]\tLoss: 0.025742\n", "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.037700\n", "Train Epoch: 6 [15360/60000 (25%)]\tLoss: 0.055250\n", "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.032784\n", "Train Epoch: 6 [25600/60000 (42%)]\tLoss: 0.024869\n", "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.038963\n", "Train Epoch: 6 [35840/60000 (59%)]\tLoss: 0.025131\n", "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.050447\n", "Train Epoch: 6 [46080/60000 (76%)]\tLoss: 0.044125\n", "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.036824\n", "Train Epoch: 6 [56320/60000 (93%)]\tLoss: 0.051100\n", "\n", "Test set: Average loss: 0.0287, Accuracy: 59526/60000 (99%)\n", "\n", "\n", "Test set: Average loss: 0.0287, Accuracy: 59526/60000 (99%)\n", "\n", "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.020755\n", "Train Epoch: 7 [5120/60000 (8%)]\tLoss: 0.028850\n", "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.014682\n", "Train Epoch: 7 [15360/60000 (25%)]\tLoss: 0.028575\n", "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.017527\n", "Train Epoch: 7 [25600/60000 (42%)]\tLoss: 0.025128\n", "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.027048\n", "Train Epoch: 7 [35840/60000 (59%)]\tLoss: 0.029815\n", "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.030194\n", "Train Epoch: 7 [46080/60000 (76%)]\tLoss: 0.024123\n", "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.020697\n", "Train Epoch: 7 [56320/60000 (93%)]\tLoss: 0.027606\n", "\n", "Test set: Average loss: 0.0204, Accuracy: 59721/60000 (100%)\n", "\n", "\n", "Test set: Average loss: 0.0204, Accuracy: 59721/60000 (100%)\n", "\n", "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.020991\n", "Train Epoch: 8 [5120/60000 (8%)]\tLoss: 0.018593\n", "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.035120\n", "Train Epoch: 8 [15360/60000 (25%)]\tLoss: 0.013997\n", "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.030595\n", "Train Epoch: 8 [25600/60000 (42%)]\tLoss: 0.019372\n", "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.019447\n", "Train Epoch: 8 [35840/60000 (59%)]\tLoss: 0.009067\n", "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.017459\n", "Train Epoch: 8 [46080/60000 (76%)]\tLoss: 0.010490\n", "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.021798\n", "Train Epoch: 8 [56320/60000 (93%)]\tLoss: 0.026126\n", "\n", "Test set: Average loss: 0.0178, Accuracy: 59768/60000 (100%)\n", "\n", "\n", "Test set: Average loss: 0.0178, Accuracy: 59768/60000 (100%)\n", "\n", "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.018502\n", "Train Epoch: 9 [5120/60000 (8%)]\tLoss: 0.025963\n", "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.013711\n", "Train Epoch: 9 [15360/60000 (25%)]\tLoss: 0.009497\n", "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.015942\n", "Train Epoch: 9 [25600/60000 (42%)]\tLoss: 0.012580\n", "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.016554\n", "Train Epoch: 9 [35840/60000 (59%)]\tLoss: 0.015371\n", "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.017320\n", "Train Epoch: 9 [46080/60000 (76%)]\tLoss: 0.017269\n", "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.025638\n", "Train Epoch: 9 [56320/60000 (93%)]\tLoss: 0.014654\n", "\n", "Test set: Average loss: 0.0154, Accuracy: 59820/60000 (100%)\n", "\n", "\n", "Test set: Average loss: 0.0154, Accuracy: 59820/60000 (100%)\n", "\n", "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.009816\n", "Train Epoch: 10 [5120/60000 (8%)]\tLoss: 0.013583\n", "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.011849\n", "Train Epoch: 10 [15360/60000 (25%)]\tLoss: 0.012534\n", "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.021728\n", "Train Epoch: 10 [25600/60000 (42%)]\tLoss: 0.028684\n", "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.024683\n", "Train Epoch: 10 [35840/60000 (59%)]\tLoss: 0.020662\n", "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.009497\n", "Train Epoch: 10 [46080/60000 (76%)]\tLoss: 0.012751\n", "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.030206\n", "Train Epoch: 10 [56320/60000 (93%)]\tLoss: 0.009565\n", "\n", "Test set: Average loss: 0.0140, Accuracy: 59855/60000 (100%)\n", "\n", "\n", "Test set: Average loss: 0.0140, Accuracy: 59855/60000 (100%)\n", "\n", "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.008792\n", "Train Epoch: 11 [5120/60000 (8%)]\tLoss: 0.014050\n", "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.011414\n", "Train Epoch: 11 [15360/60000 (25%)]\tLoss: 0.012279\n", "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.030573\n", "Train Epoch: 11 [25600/60000 (42%)]\tLoss: 0.010422\n", "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.015848\n", "Train Epoch: 11 [35840/60000 (59%)]\tLoss: 0.010831\n", "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.014059\n", "Train Epoch: 11 [46080/60000 (76%)]\tLoss: 0.011372\n", "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.012799\n", "Train Epoch: 11 [56320/60000 (93%)]\tLoss: 0.015632\n", "\n", "Test set: Average loss: 0.0126, Accuracy: 59878/60000 (100%)\n", "\n", "\n", "Test set: Average loss: 0.0126, Accuracy: 59878/60000 (100%)\n", "\n", "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.008051\n", "Train Epoch: 12 [5120/60000 (8%)]\tLoss: 0.005668\n", "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.007905\n", "Train Epoch: 12 [15360/60000 (25%)]\tLoss: 0.016736\n", "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.008021\n", "Train Epoch: 12 [25600/60000 (42%)]\tLoss: 0.018190\n", "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.016972\n", "Train Epoch: 12 [35840/60000 (59%)]\tLoss: 0.010394\n", "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.011242\n", "Train Epoch: 12 [46080/60000 (76%)]\tLoss: 0.016200\n", "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.007169\n", "Train Epoch: 12 [56320/60000 (93%)]\tLoss: 0.009894\n", "\n", "Test set: Average loss: 0.0115, Accuracy: 59894/60000 (100%)\n", "\n", "\n", "Test set: Average loss: 0.0115, Accuracy: 59894/60000 (100%)\n", "\n", "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.007469\n", "Train Epoch: 13 [5120/60000 (8%)]\tLoss: 0.006125\n", "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.007372\n", "Train Epoch: 13 [15360/60000 (25%)]\tLoss: 0.025772\n", "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.007918\n", "Train Epoch: 13 [25600/60000 (42%)]\tLoss: 0.012735\n", "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.008148\n", "Train Epoch: 13 [35840/60000 (59%)]\tLoss: 0.009923\n", "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.009366\n", "Train Epoch: 13 [46080/60000 (76%)]\tLoss: 0.009424\n", "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.008709\n", "Train Epoch: 13 [56320/60000 (93%)]\tLoss: 0.010488\n", "\n", "Test set: Average loss: 0.0107, Accuracy: 59916/60000 (100%)\n", "\n", "\n", "Test set: Average loss: 0.0107, Accuracy: 59916/60000 (100%)\n", "\n", "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.008675\n", "Train Epoch: 14 [5120/60000 (8%)]\tLoss: 0.006808\n", "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.010762\n", "Train Epoch: 14 [15360/60000 (25%)]\tLoss: 0.008207\n", "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.008447\n", "Train Epoch: 14 [25600/60000 (42%)]\tLoss: 0.008520\n", "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.005015\n", "Train Epoch: 14 [35840/60000 (59%)]\tLoss: 0.005926\n", "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.015314\n", "Train Epoch: 14 [46080/60000 (76%)]\tLoss: 0.011165\n", "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.007128\n", "Train Epoch: 14 [56320/60000 (93%)]\tLoss: 0.008846\n", "\n", "Test set: Average loss: 0.0100, Accuracy: 59923/60000 (100%)\n", "\n", "\n", "Test set: Average loss: 0.0100, Accuracy: 59923/60000 (100%)\n", "\n", "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.008952\n", "Train Epoch: 15 [5120/60000 (8%)]\tLoss: 0.009914\n", "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.013312\n", "Train Epoch: 15 [15360/60000 (25%)]\tLoss: 0.003268\n", "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.006864\n", "Train Epoch: 15 [25600/60000 (42%)]\tLoss: 0.023136\n", "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.011355\n", "Train Epoch: 15 [35840/60000 (59%)]\tLoss: 0.005970\n", "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.005867\n", "Train Epoch: 15 [46080/60000 (76%)]\tLoss: 0.006716\n", "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.008144\n", "Train Epoch: 15 [56320/60000 (93%)]\tLoss: 0.016001\n", "\n", "Test set: Average loss: 0.0096, Accuracy: 59936/60000 (100%)\n", "\n", "\n", "Test set: Average loss: 0.0096, Accuracy: 59936/60000 (100%)\n", "\n", "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.003959\n", "Train Epoch: 16 [5120/60000 (8%)]\tLoss: 0.006503\n", "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.011546\n", "Train Epoch: 16 [15360/60000 (25%)]\tLoss: 0.018701\n", "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.010704\n", "Train Epoch: 16 [25600/60000 (42%)]\tLoss: 0.018513\n", "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.008969\n", "Train Epoch: 16 [35840/60000 (59%)]\tLoss: 0.007443\n", "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.004257\n", "Train Epoch: 16 [46080/60000 (76%)]\tLoss: 0.007868\n", "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.008656\n", "Train Epoch: 16 [56320/60000 (93%)]\tLoss: 0.007015\n", "\n", "Test set: Average loss: 0.0093, Accuracy: 59936/60000 (100%)\n", "\n", "\n", "Test set: Average loss: 0.0093, Accuracy: 59936/60000 (100%)\n", "\n", "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.022206\n", "Train Epoch: 17 [5120/60000 (8%)]\tLoss: 0.012591\n", "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.015931\n", "Train Epoch: 17 [15360/60000 (25%)]\tLoss: 0.016030\n", "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.019362\n", "Train Epoch: 17 [25600/60000 (42%)]\tLoss: 0.010071\n", "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.006619\n", "Train Epoch: 17 [35840/60000 (59%)]\tLoss: 0.005649\n", "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.014286\n", "Train Epoch: 17 [46080/60000 (76%)]\tLoss: 0.020926\n", "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.006534\n", "Train Epoch: 17 [56320/60000 (93%)]\tLoss: 0.006059\n", "\n", "Test set: Average loss: 0.0091, Accuracy: 59936/60000 (100%)\n", "\n", "\n", "Test set: Average loss: 0.0091, Accuracy: 59936/60000 (100%)\n", "\n", "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.006819\n", "Train Epoch: 18 [5120/60000 (8%)]\tLoss: 0.006548\n", "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 0.010693\n", "Train Epoch: 18 [15360/60000 (25%)]\tLoss: 0.003276\n", "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 0.004270\n", "Train Epoch: 18 [25600/60000 (42%)]\tLoss: 0.006159\n", "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 0.006233\n", "Train Epoch: 18 [35840/60000 (59%)]\tLoss: 0.008648\n", "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 0.009388\n", "Train Epoch: 18 [46080/60000 (76%)]\tLoss: 0.017804\n", "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.006501\n", "Train Epoch: 18 [56320/60000 (93%)]\tLoss: 0.006722\n", "\n", "Test set: Average loss: 0.0089, Accuracy: 59946/60000 (100%)\n", "\n", "\n", "Test set: Average loss: 0.0089, Accuracy: 59946/60000 (100%)\n", "\n", "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.011154\n", "Train Epoch: 19 [5120/60000 (8%)]\tLoss: 0.009038\n", "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 0.027631\n", "Train Epoch: 19 [15360/60000 (25%)]\tLoss: 0.006755\n", "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 0.022951\n", "Train Epoch: 19 [25600/60000 (42%)]\tLoss: 0.017808\n", "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 0.006889\n", "Train Epoch: 19 [35840/60000 (59%)]\tLoss: 0.008754\n", "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 0.006648\n", "Train Epoch: 19 [46080/60000 (76%)]\tLoss: 0.009425\n", "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.023977\n", "Train Epoch: 19 [56320/60000 (93%)]\tLoss: 0.007552\n", "\n", "Test set: Average loss: 0.0087, Accuracy: 59949/60000 (100%)\n", "\n", "\n", "Test set: Average loss: 0.0087, Accuracy: 59949/60000 (100%)\n", "\n", "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.008316\n", "Train Epoch: 20 [5120/60000 (8%)]\tLoss: 0.007921\n", "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 0.005576\n", "Train Epoch: 20 [15360/60000 (25%)]\tLoss: 0.006183\n", "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 0.012119\n", "Train Epoch: 20 [25600/60000 (42%)]\tLoss: 0.008167\n", "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 0.014483\n", "Train Epoch: 20 [35840/60000 (59%)]\tLoss: 0.010334\n", "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 0.004373\n", "Train Epoch: 20 [46080/60000 (76%)]\tLoss: 0.015288\n", "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.007859\n", "Train Epoch: 20 [56320/60000 (93%)]\tLoss: 0.007263\n", "\n", "Test set: Average loss: 0.0086, Accuracy: 59951/60000 (100%)\n", "\n", "\n", "Test set: Average loss: 0.0086, Accuracy: 59951/60000 (100%)\n", "\n"]}], "source": ["for epoch in range(1, nrepochs + 1):\n", "    train_epoch(\n", "        model, device, train_loader, lossfn, optimizer, epoch, print_every_nep=10\n", "    )\n", "    test(model, device, test_loader, lossfn)\n", "    test(model, device, test_loader, lossfn)\n", "    scheduler.step()\n"]}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": [], "name": "softmax_mnist.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3.8.8 64-bit ('RDDL': conda)", "name": "python388jvsc74a57bd0eb8633c4d4e251251708d3c7ece77ee33d393b5bf4628cd3b0e51f052595f5d6"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.8"}}, "nbformat": 4, "nbformat_minor": 0}