{"cells":[{"cell_type":"markdown","metadata":{"id":"WZnV3RNgU0Mi"},"source":["# Training a *SotA MLP SoftMax classifier* on the *MNIST* dataset"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Ra6EQvDLUxqG"},"outputs":[],"source":["# Data download:\n","import os\n","\n","# NNets & co.:\n","import numpy as np\n","import torch as th\n","import torch.nn.functional as F\n","from torch.optim.lr_scheduler import StepLR\n","from ebtorch.nn import FCBlock\n","\n","# Data(set) handling\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import MNIST\n","from torchvision.transforms import ToTensor, Normalize, Compose, Lambda\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ANKugUNmUyFo"},"outputs":[],"source":["# MNIST DataLoader(s) builder\n","\n","\n","def spawn_mnist_loaders(\n","    data_root=\"datasets/\",\n","    batch_size_train=256,\n","    batch_size_test=512,\n","    cuda_accel=False,\n","    **kwargs\n","):\n","\n","    os.makedirs(data_root, exist_ok=True)\n","\n","    transforms = Compose(\n","        [\n","            ToTensor(),\n","            Normalize((0.1307,), (0.3081,)),  # usual magic constants for MNIST\n","            Lambda(lambda x: th.flatten(x)),\n","        ]\n","    )\n","\n","    trainset = MNIST(data_root, train=True, transform=transforms, download=True)\n","    testset = MNIST(data_root, train=False, transform=transforms, download=True)\n","\n","    cuda_args = {}\n","    if cuda_accel:\n","        cuda_args = {\"num_workers\": 1, \"pin_memory\": True}\n","\n","    trainloader = DataLoader(\n","        trainset, batch_size=batch_size_train, shuffle=True, **cuda_args\n","    )\n","    testloader = DataLoader(\n","        trainset, batch_size=batch_size_test, shuffle=False, **cuda_args\n","    )\n","\n","    return trainloader, testloader\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"0xgs3EZBUyJ4"},"outputs":[],"source":["# Train / Test tooling\n","\n","\n","def train_epoch(\n","    model, device, train_loader, loss_fn, optimizer, epoch, print_every_nep\n","):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = loss_fn(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % print_every_nep == 0:\n","            print(\n","                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n","                    epoch,\n","                    batch_idx * len(data),\n","                    len(train_loader.dataset),\n","                    100.0 * batch_idx / len(train_loader),\n","                    loss.item(),\n","                )\n","            )\n","\n","\n","def test(model, device, test_loader, loss_fn):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with th.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += loss_fn(\n","                output, target, reduction=\"sum\"\n","            ).item()  # sum up batch loss\n","            pred = output.argmax(\n","                dim=1, keepdim=True\n","            )  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print(\n","        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n","            test_loss,\n","            correct,\n","            len(test_loader.dataset),\n","            100.0 * correct / len(test_loader.dataset),\n","        )\n","    )\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"i2aclwpGUyOP"},"outputs":[],"source":["device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"PtJNzN33cX_e"},"outputs":[],"source":["# Hyperparameters & co.\n","\n","minibatch_size_train: int = 32  # (cfr. Masters & Luschi, 2018)\n","minibatch_size_test: int = 512\n","\n","nrepochs = 20\n","\n","lossfn = F.nll_loss\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"N8YTl9NTUySq"},"outputs":[],"source":["train_loader, test_loader = spawn_mnist_loaders(\n","    batch_size_train=minibatch_size_test,\n","    batch_size_test=minibatch_size_test,\n","    cuda_accel=True,\n",")\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"A5QVDGBfUyW7"},"outputs":[],"source":["model = FCBlock(28*28, [150, 40], 10, hactiv=lambda x: F.leaky_relu_(x, negative_slope=0.06), oactiv=lambda x: F.log_softmax(x, dim=1), bias=True).to(device)\n","optimizer = th.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, nesterov=True)\n","scheduler = StepLR(optimizer, step_size=2, gamma=0.7)  # or a lot of patience :)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cda0zuTDUyZQ","outputId":"27fbb0b9-0bc2-453b-a082-66a35674e9f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304295\n","Train Epoch: 1 [5120/60000 (8%)]\tLoss: 0.829657\n","Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.670204\n","Train Epoch: 1 [15360/60000 (25%)]\tLoss: 0.364339\n","Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.293912\n","Train Epoch: 1 [25600/60000 (42%)]\tLoss: 0.248731\n","Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.157993\n","Train Epoch: 1 [35840/60000 (59%)]\tLoss: 0.185503\n","Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.161975\n","Train Epoch: 1 [46080/60000 (76%)]\tLoss: 0.153820\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.197868\n","Train Epoch: 1 [56320/60000 (93%)]\tLoss: 0.199731\n","\n","Test set: Average loss: 0.1617, Accuracy: 56918/60000 (95%)\n","\n","\n","Test set: Average loss: 0.1617, Accuracy: 56918/60000 (95%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.143456\n","Train Epoch: 2 [5120/60000 (8%)]\tLoss: 0.177902\n","Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.168497\n","Train Epoch: 2 [15360/60000 (25%)]\tLoss: 0.182525\n","Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.159962\n","Train Epoch: 2 [25600/60000 (42%)]\tLoss: 0.126228\n","Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.145277\n","Train Epoch: 2 [35840/60000 (59%)]\tLoss: 0.146878\n","Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.088134\n","Train Epoch: 2 [46080/60000 (76%)]\tLoss: 0.134269\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.105493\n","Train Epoch: 2 [56320/60000 (93%)]\tLoss: 0.099883\n","\n","Test set: Average loss: 0.0819, Accuracy: 58541/60000 (98%)\n","\n","\n","Test set: Average loss: 0.0819, Accuracy: 58541/60000 (98%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.091712\n","Train Epoch: 3 [5120/60000 (8%)]\tLoss: 0.062578\n","Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.057294\n","Train Epoch: 3 [15360/60000 (25%)]\tLoss: 0.100857\n","Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.091403\n","Train Epoch: 3 [25600/60000 (42%)]\tLoss: 0.106477\n","Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.057572\n","Train Epoch: 3 [35840/60000 (59%)]\tLoss: 0.062746\n","Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.085763\n","Train Epoch: 3 [46080/60000 (76%)]\tLoss: 0.080712\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.057865\n","Train Epoch: 3 [56320/60000 (93%)]\tLoss: 0.072054\n","\n","Test set: Average loss: 0.0676, Accuracy: 58770/60000 (98%)\n","\n","\n","Test set: Average loss: 0.0676, Accuracy: 58770/60000 (98%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.093922\n","Train Epoch: 4 [5120/60000 (8%)]\tLoss: 0.073329\n","Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.047699\n","Train Epoch: 4 [15360/60000 (25%)]\tLoss: 0.038891\n","Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.059778\n","Train Epoch: 4 [25600/60000 (42%)]\tLoss: 0.058697\n","Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.070508\n","Train Epoch: 4 [35840/60000 (59%)]\tLoss: 0.040455\n","Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.045455\n","Train Epoch: 4 [46080/60000 (76%)]\tLoss: 0.086081\n","Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.057214\n","Train Epoch: 4 [56320/60000 (93%)]\tLoss: 0.067967\n","\n","Test set: Average loss: 0.0567, Accuracy: 58931/60000 (98%)\n","\n","\n","Test set: Average loss: 0.0567, Accuracy: 58931/60000 (98%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.076006\n","Train Epoch: 5 [5120/60000 (8%)]\tLoss: 0.044065\n","Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.040533\n","Train Epoch: 5 [15360/60000 (25%)]\tLoss: 0.032306\n","Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.066222\n","Train Epoch: 5 [25600/60000 (42%)]\tLoss: 0.038849\n","Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.023119\n","Train Epoch: 5 [35840/60000 (59%)]\tLoss: 0.061060\n","Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.049193\n","Train Epoch: 5 [46080/60000 (76%)]\tLoss: 0.041946\n","Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.040864\n","Train Epoch: 5 [56320/60000 (93%)]\tLoss: 0.033657\n","\n","Test set: Average loss: 0.0332, Accuracy: 59433/60000 (99%)\n","\n","\n","Test set: Average loss: 0.0332, Accuracy: 59433/60000 (99%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.044354\n","Train Epoch: 6 [5120/60000 (8%)]\tLoss: 0.025742\n","Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.037700\n","Train Epoch: 6 [15360/60000 (25%)]\tLoss: 0.055250\n","Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.032784\n","Train Epoch: 6 [25600/60000 (42%)]\tLoss: 0.024869\n","Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.038963\n","Train Epoch: 6 [35840/60000 (59%)]\tLoss: 0.025131\n","Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.050447\n","Train Epoch: 6 [46080/60000 (76%)]\tLoss: 0.044125\n","Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.036824\n","Train Epoch: 6 [56320/60000 (93%)]\tLoss: 0.051100\n","\n","Test set: Average loss: 0.0287, Accuracy: 59526/60000 (99%)\n","\n","\n","Test set: Average loss: 0.0287, Accuracy: 59526/60000 (99%)\n","\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.020755\n","Train Epoch: 7 [5120/60000 (8%)]\tLoss: 0.028850\n","Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.014682\n","Train Epoch: 7 [15360/60000 (25%)]\tLoss: 0.028575\n","Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.017527\n","Train Epoch: 7 [25600/60000 (42%)]\tLoss: 0.025128\n","Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.027048\n","Train Epoch: 7 [35840/60000 (59%)]\tLoss: 0.029815\n","Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.030194\n","Train Epoch: 7 [46080/60000 (76%)]\tLoss: 0.024123\n","Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.020697\n","Train Epoch: 7 [56320/60000 (93%)]\tLoss: 0.027606\n","\n","Test set: Average loss: 0.0204, Accuracy: 59721/60000 (100%)\n","\n","\n","Test set: Average loss: 0.0204, Accuracy: 59721/60000 (100%)\n","\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.020991\n","Train Epoch: 8 [5120/60000 (8%)]\tLoss: 0.018593\n","Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.035120\n","Train Epoch: 8 [15360/60000 (25%)]\tLoss: 0.013997\n","Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.030595\n","Train Epoch: 8 [25600/60000 (42%)]\tLoss: 0.019372\n","Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.019447\n","Train Epoch: 8 [35840/60000 (59%)]\tLoss: 0.009067\n","Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.017459\n","Train Epoch: 8 [46080/60000 (76%)]\tLoss: 0.010490\n","Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.021798\n","Train Epoch: 8 [56320/60000 (93%)]\tLoss: 0.026126\n","\n","Test set: Average loss: 0.0178, Accuracy: 59768/60000 (100%)\n","\n","\n","Test set: Average loss: 0.0178, Accuracy: 59768/60000 (100%)\n","\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.018502\n","Train Epoch: 9 [5120/60000 (8%)]\tLoss: 0.025963\n","Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.013711\n","Train Epoch: 9 [15360/60000 (25%)]\tLoss: 0.009497\n","Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.015942\n","Train Epoch: 9 [25600/60000 (42%)]\tLoss: 0.012580\n","Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.016554\n","Train Epoch: 9 [35840/60000 (59%)]\tLoss: 0.015371\n","Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.017320\n","Train Epoch: 9 [46080/60000 (76%)]\tLoss: 0.017269\n","Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.025638\n","Train Epoch: 9 [56320/60000 (93%)]\tLoss: 0.014654\n","\n","Test set: Average loss: 0.0154, Accuracy: 59820/60000 (100%)\n","\n","\n","Test set: Average loss: 0.0154, Accuracy: 59820/60000 (100%)\n","\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.009816\n","Train Epoch: 10 [5120/60000 (8%)]\tLoss: 0.013583\n","Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.011849\n","Train Epoch: 10 [15360/60000 (25%)]\tLoss: 0.012534\n","Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.021728\n","Train Epoch: 10 [25600/60000 (42%)]\tLoss: 0.028684\n","Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.024683\n","Train Epoch: 10 [35840/60000 (59%)]\tLoss: 0.020662\n","Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.009497\n","Train Epoch: 10 [46080/60000 (76%)]\tLoss: 0.012751\n","Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.030206\n","Train Epoch: 10 [56320/60000 (93%)]\tLoss: 0.009565\n","\n","Test set: Average loss: 0.0140, Accuracy: 59855/60000 (100%)\n","\n","\n","Test set: Average loss: 0.0140, Accuracy: 59855/60000 (100%)\n","\n","Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.008792\n","Train Epoch: 11 [5120/60000 (8%)]\tLoss: 0.014050\n","Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.011414\n","Train Epoch: 11 [15360/60000 (25%)]\tLoss: 0.012279\n","Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.030573\n","Train Epoch: 11 [25600/60000 (42%)]\tLoss: 0.010422\n","Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.015848\n","Train Epoch: 11 [35840/60000 (59%)]\tLoss: 0.010831\n","Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.014059\n","Train Epoch: 11 [46080/60000 (76%)]\tLoss: 0.011372\n","Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.012799\n","Train Epoch: 11 [56320/60000 (93%)]\tLoss: 0.015632\n","\n","Test set: Average loss: 0.0126, Accuracy: 59878/60000 (100%)\n","\n","\n","Test set: Average loss: 0.0126, Accuracy: 59878/60000 (100%)\n","\n","Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.008051\n","Train Epoch: 12 [5120/60000 (8%)]\tLoss: 0.005668\n","Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.007905\n","Train Epoch: 12 [15360/60000 (25%)]\tLoss: 0.016736\n","Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.008021\n","Train Epoch: 12 [25600/60000 (42%)]\tLoss: 0.018190\n","Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.016972\n","Train Epoch: 12 [35840/60000 (59%)]\tLoss: 0.010394\n","Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.011242\n","Train Epoch: 12 [46080/60000 (76%)]\tLoss: 0.016200\n","Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.007169\n","Train Epoch: 12 [56320/60000 (93%)]\tLoss: 0.009894\n","\n","Test set: Average loss: 0.0115, Accuracy: 59894/60000 (100%)\n","\n","\n","Test set: Average loss: 0.0115, Accuracy: 59894/60000 (100%)\n","\n","Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.007469\n","Train Epoch: 13 [5120/60000 (8%)]\tLoss: 0.006125\n","Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.007372\n","Train Epoch: 13 [15360/60000 (25%)]\tLoss: 0.025772\n","Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.007918\n","Train Epoch: 13 [25600/60000 (42%)]\tLoss: 0.012735\n","Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.008148\n","Train Epoch: 13 [35840/60000 (59%)]\tLoss: 0.009923\n","Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.009366\n","Train Epoch: 13 [46080/60000 (76%)]\tLoss: 0.009424\n","Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.008709\n","Train Epoch: 13 [56320/60000 (93%)]\tLoss: 0.010488\n","\n","Test set: Average loss: 0.0107, Accuracy: 59916/60000 (100%)\n","\n","\n","Test set: Average loss: 0.0107, Accuracy: 59916/60000 (100%)\n","\n","Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.008675\n","Train Epoch: 14 [5120/60000 (8%)]\tLoss: 0.006808\n","Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.010762\n","Train Epoch: 14 [15360/60000 (25%)]\tLoss: 0.008207\n","Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.008447\n","Train Epoch: 14 [25600/60000 (42%)]\tLoss: 0.008520\n","Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.005015\n","Train Epoch: 14 [35840/60000 (59%)]\tLoss: 0.005926\n","Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.015314\n","Train Epoch: 14 [46080/60000 (76%)]\tLoss: 0.011165\n","Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.007128\n","Train Epoch: 14 [56320/60000 (93%)]\tLoss: 0.008846\n","\n","Test set: Average loss: 0.0100, Accuracy: 59923/60000 (100%)\n","\n","\n","Test set: Average loss: 0.0100, Accuracy: 59923/60000 (100%)\n","\n","Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.008952\n","Train Epoch: 15 [5120/60000 (8%)]\tLoss: 0.009914\n","Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.013312\n","Train Epoch: 15 [15360/60000 (25%)]\tLoss: 0.003268\n","Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.006864\n","Train Epoch: 15 [25600/60000 (42%)]\tLoss: 0.023136\n","Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.011355\n","Train Epoch: 15 [35840/60000 (59%)]\tLoss: 0.005970\n","Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.005867\n","Train Epoch: 15 [46080/60000 (76%)]\tLoss: 0.006716\n","Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.008144\n","Train Epoch: 15 [56320/60000 (93%)]\tLoss: 0.016001\n","\n","Test set: Average loss: 0.0096, Accuracy: 59936/60000 (100%)\n","\n","\n","Test set: Average loss: 0.0096, Accuracy: 59936/60000 (100%)\n","\n","Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.003959\n","Train Epoch: 16 [5120/60000 (8%)]\tLoss: 0.006503\n","Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.011546\n","Train Epoch: 16 [15360/60000 (25%)]\tLoss: 0.018701\n","Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.010704\n","Train Epoch: 16 [25600/60000 (42%)]\tLoss: 0.018513\n","Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.008969\n","Train Epoch: 16 [35840/60000 (59%)]\tLoss: 0.007443\n","Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.004257\n","Train Epoch: 16 [46080/60000 (76%)]\tLoss: 0.007868\n","Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.008656\n","Train Epoch: 16 [56320/60000 (93%)]\tLoss: 0.007015\n","\n","Test set: Average loss: 0.0093, Accuracy: 59936/60000 (100%)\n","\n","\n","Test set: Average loss: 0.0093, Accuracy: 59936/60000 (100%)\n","\n","Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.022206\n","Train Epoch: 17 [5120/60000 (8%)]\tLoss: 0.012591\n","Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.015931\n","Train Epoch: 17 [15360/60000 (25%)]\tLoss: 0.016030\n","Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.019362\n","Train Epoch: 17 [25600/60000 (42%)]\tLoss: 0.010071\n","Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.006619\n","Train Epoch: 17 [35840/60000 (59%)]\tLoss: 0.005649\n","Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.014286\n","Train Epoch: 17 [46080/60000 (76%)]\tLoss: 0.020926\n","Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.006534\n","Train Epoch: 17 [56320/60000 (93%)]\tLoss: 0.006059\n","\n","Test set: Average loss: 0.0091, Accuracy: 59936/60000 (100%)\n","\n","\n","Test set: Average loss: 0.0091, Accuracy: 59936/60000 (100%)\n","\n","Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.006819\n","Train Epoch: 18 [5120/60000 (8%)]\tLoss: 0.006548\n","Train Epoch: 18 [10240/60000 (17%)]\tLoss: 0.010693\n","Train Epoch: 18 [15360/60000 (25%)]\tLoss: 0.003276\n","Train Epoch: 18 [20480/60000 (34%)]\tLoss: 0.004270\n","Train Epoch: 18 [25600/60000 (42%)]\tLoss: 0.006159\n","Train Epoch: 18 [30720/60000 (51%)]\tLoss: 0.006233\n","Train Epoch: 18 [35840/60000 (59%)]\tLoss: 0.008648\n","Train Epoch: 18 [40960/60000 (68%)]\tLoss: 0.009388\n","Train Epoch: 18 [46080/60000 (76%)]\tLoss: 0.017804\n","Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.006501\n","Train Epoch: 18 [56320/60000 (93%)]\tLoss: 0.006722\n","\n","Test set: Average loss: 0.0089, Accuracy: 59946/60000 (100%)\n","\n","\n","Test set: Average loss: 0.0089, Accuracy: 59946/60000 (100%)\n","\n","Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.011154\n","Train Epoch: 19 [5120/60000 (8%)]\tLoss: 0.009038\n","Train Epoch: 19 [10240/60000 (17%)]\tLoss: 0.027631\n","Train Epoch: 19 [15360/60000 (25%)]\tLoss: 0.006755\n","Train Epoch: 19 [20480/60000 (34%)]\tLoss: 0.022951\n","Train Epoch: 19 [25600/60000 (42%)]\tLoss: 0.017808\n","Train Epoch: 19 [30720/60000 (51%)]\tLoss: 0.006889\n","Train Epoch: 19 [35840/60000 (59%)]\tLoss: 0.008754\n","Train Epoch: 19 [40960/60000 (68%)]\tLoss: 0.006648\n","Train Epoch: 19 [46080/60000 (76%)]\tLoss: 0.009425\n","Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.023977\n","Train Epoch: 19 [56320/60000 (93%)]\tLoss: 0.007552\n","\n","Test set: Average loss: 0.0087, Accuracy: 59949/60000 (100%)\n","\n","\n","Test set: Average loss: 0.0087, Accuracy: 59949/60000 (100%)\n","\n","Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.008316\n","Train Epoch: 20 [5120/60000 (8%)]\tLoss: 0.007921\n","Train Epoch: 20 [10240/60000 (17%)]\tLoss: 0.005576\n","Train Epoch: 20 [15360/60000 (25%)]\tLoss: 0.006183\n","Train Epoch: 20 [20480/60000 (34%)]\tLoss: 0.012119\n","Train Epoch: 20 [25600/60000 (42%)]\tLoss: 0.008167\n","Train Epoch: 20 [30720/60000 (51%)]\tLoss: 0.014483\n","Train Epoch: 20 [35840/60000 (59%)]\tLoss: 0.010334\n","Train Epoch: 20 [40960/60000 (68%)]\tLoss: 0.004373\n","Train Epoch: 20 [46080/60000 (76%)]\tLoss: 0.015288\n","Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.007859\n","Train Epoch: 20 [56320/60000 (93%)]\tLoss: 0.007263\n","\n","Test set: Average loss: 0.0086, Accuracy: 59951/60000 (100%)\n","\n","\n","Test set: Average loss: 0.0086, Accuracy: 59951/60000 (100%)\n","\n"]}],"source":["for epoch in range(1, nrepochs + 1):\n","    train_epoch(\n","        model, device, train_loader, lossfn, optimizer, epoch, print_every_nep=10\n","    )\n","    test(model, device, test_loader, lossfn)\n","    test(model, device, test_loader, lossfn)\n","    scheduler.step()\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"softmax_mnist.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('RDDL': conda)","name":"python388jvsc74a57bd0eb8633c4d4e251251708d3c7ece77ee33d393b5bf4628cd3b0e51f052595f5d6"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}