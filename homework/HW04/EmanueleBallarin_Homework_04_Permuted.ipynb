{"cells":[{"cell_type":"markdown","metadata":{"id":"WZnV3RNgU0Mi"},"source":["# Deep Learning Homework \\#04 (*permuted MNIST*)\n","### Deep Learning Course $\\in$ DSSC @ UniTS (Spring 2021)  \n","\n","#### Submitted by [Emanuele Ballarin](mailto:emanuele@ballarin.cc)  "]},{"cell_type":"markdown","metadata":{},"source":["### Preliminaries:"]},{"cell_type":"markdown","metadata":{},"source":["#### Differences with the *normal MNIST* version:\n","\n","- **Batch size**. $1024$ instead of $512$. To further emphasize the *decision boundary* learning as opposed to commonalities (being the data essentially randomly-grouped);\n","\n","- **Training length**. $90$ epochs instead of $12$, since the problem is essentially harder (but with the *advantage* of not having anything else to change!). Note that this may take *a long time*!\n","\n","All the rest has been kept the same (with very minor exceptions *w.r.t* comments.)"]},{"cell_type":"markdown","metadata":{},"source":["#### Imports:\n","\n","We start off by importing all the libraries, modules, classes and functions we are going to use *today*..."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Ra6EQvDLUxqG"},"outputs":[],"source":["# Type hints\n","from torch import Tensor\n","\n","# Just to force-load MKL (if available)\n","import numpy as np\n","\n","# Mathematical functions\n","from math import sqrt as msqrt\n","\n","# Neural networks and friends\n","import torch as th\n","from torch.nn import Sequential, BatchNorm1d, Linear, LogSoftmax, Dropout\n","import torch.nn.functional as F\n","\n","# Optimization and scheduling\n","from torch.optim.lr_scheduler import StepLR, MultiStepLR\n","\n","# Bespoke Modules / Functions / Optimizers\n","from ebtorch.logging import AverageMeter\n","from ebtorch.nn import Mish, mishlayer_init\n","from ebtorch.optim import Lookahead\n","from madgrad.madgrad import MADGRAD as MadGrad\n","\n","# Model summarization\n","from torchinfo import summary\n","\n","# Dataset handling for PyTorch\n","import os\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import MNIST\n","from torchvision.transforms import ToTensor, Normalize, Compose, Lambda"]},{"cell_type":"markdown","metadata":{},"source":["### Request 1:\n","\n","Now that you have all the tools to train an MLP with high performance on MNIST, try reaching 0-loss (or 100% accuracy) on the training data (with a small epsilon, e.g. 99.99% training performance -- don't worry if you overfit!). The implementation is completely up to you. You just need to keep it an MLP without using fancy layers (e.g., keep the Linear layers, don't use `Conv1d` or something like this, don't use attention). You are free to use any LR scheduler or optimizer, any one of batchnorm/groupnorm, regularization methods... If you use something we haven't seen during lectures, please motivate your choice and explain (as briefly as possible) how it works."]},{"cell_type":"markdown","metadata":{},"source":["#### Comment:\n","\n","Nothing fancy here... just some DataSets/Loaders."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ANKugUNmUyFo"},"outputs":[],"source":["def spawn_mnist_loaders(\n","    data_root=\"datasets/\",\n","    batch_size_train=256,\n","    batch_size_test=512,\n","    cuda_accel=False,\n","    **kwargs\n","):\n","\n","    os.makedirs(data_root, exist_ok=True)\n","\n","    transforms = Compose(\n","        [\n","            ToTensor(),\n","            Normalize((0.1307,), (0.3081,)),  # usual normalization constants for MNIST\n","            Lambda(lambda x: th.flatten(x)),\n","        ]\n","    )\n","\n","    trainset = MNIST(data_root, train=True, transform=transforms, download=True)\n","    testset = MNIST(data_root, train=False, transform=transforms, download=True)\n","\n","    # Permute trainset.targets\n","    idx = th.randperm(trainset.targets.nelement())\n","    trainset.targets = trainset.targets.view(-1)[idx].view(trainset.targets.size())\n","\n","    cuda_args = {}\n","    if cuda_accel:\n","        cuda_args = {\"num_workers\": 1, \"pin_memory\": True}\n","\n","    trainloader = DataLoader(\n","        trainset, batch_size=batch_size_train, shuffle=True, **cuda_args\n","    )\n","    testloader = DataLoader(\n","        testset, batch_size=batch_size_test, shuffle=False, **cuda_args\n","    )\n","    tontrloader = DataLoader(   # tontr == test on train\n","        trainset, batch_size=batch_size_test, shuffle=False, **cuda_args\n","    )\n","\n","    return trainloader, testloader, tontrloader"]},{"cell_type":"markdown","metadata":{},"source":["#### Comment:\n","\n","Nothing fancy here, too. A standard training/testing loop."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"0xgs3EZBUyJ4"},"outputs":[],"source":["train_acc_avgmeter = AverageMeter(\"Training Loss\")\n","\n","def train_epoch(\n","    model, device, train_loader, loss_fn, optimizer, epoch, print_every_nep, inner_scheduler=None, quiet=False,\n","):\n","    train_acc_avgmeter.reset()\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = loss_fn(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if inner_scheduler is not None:\n","            inner_scheduler.step()\n","        \n","        train_acc_avgmeter.update(loss.item())\n","\n","        if not quiet and batch_idx % print_every_nep == 0:\n","            print(\n","                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tAvg. loss: {:.6f}\".format(\n","                    epoch,\n","                    batch_idx * len(data),\n","                    len(train_loader.dataset),\n","                    100.0 * batch_idx / len(train_loader),\n","                    train_acc_avgmeter.avg\n","                )\n","            )\n","\n","\n","def test(model, device, test_loader, loss_fn, quiet=False):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with th.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += loss_fn(\n","                output, target, reduction=\"sum\"\n","            ).item()  # sum up batch loss\n","            pred = output.argmax(\n","                dim=1, keepdim=True\n","            )  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    ltlds = len(test_loader.dataset)\n","\n","    test_loss /= ltlds\n","    \n","    if not quiet:\n","        print(\n","            \"Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n","                test_loss,\n","                correct,\n","                ltlds,\n","                100.0 * correct / ltlds,\n","            )\n","        )\n","    \n","    return test_loss, correct / ltlds"]},{"cell_type":"markdown","metadata":{},"source":["#### Comment:\n","\n","The *usual* specification of the device. Please, notece that the `MADGRAD` optimizer (used later on) requires a working *CUDA-capable* GPU and may not work otherwise. You may be interested in using `RAdam` instead."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"i2aclwpGUyOP"},"outputs":[],"source":["device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Comment:\n","\n","The choice of a $512$-element *batch size* is unusual. Here, the rationale may be summarized by noticing the peculiar relationship among *large batch size* (both as a regularizer and as a specific training device) and the deeper nature of learned features.  \n","\n","Small batch sizes ($<128$, for the dataset of interest), on the one hand:\n","\n","- Carry a considerable amount of *data sampling noise*, that smoothes the loss landcape and ease optimization with non-accelerated methods (e.g. SGD);\n","\n","- Carry a higher *signal-to-data* ratio, due to limited feature- and gradient- competition inside the batch;\n","\n","- Usually produce better generalization on both *testing* and *unseen* data. This may also be true for *synthetic whole-manifold* and *noise-corrupted* data;\n","\n","- Produce more unstable *training-set* loss and accuracy results;\n","\n","- Require longer or more-otherwise-regularized training for the same level of *training set accuuracy*;\n","\n","- Put the main training focus on learning *useful data-point features*, as opposed to pure decision boundaries.\n","\n","\n","On the other hand, larger batch sizes ($>128$, for the dataset of interest):\n","\n","- Carry a reduced amount of *data sampling noise*, making easier for an optimizer to find a single direction that minimizes (even though of a small amount) the whole-dataset loss. This regime may be particularly appealing for *accelerated* descent methods;\n","\n","- May carry a reduced *signal-to-data* ratio, also due to gradient competition inside the batch, thus requiring more finely-tuned *learning rate* and *loss function* choices. Or specifically-crafted robust-to-initialization optimizers;\n","\n","- May easily overfit the *training set* without significant generalization, especially without additional regularization devices;\n","\n","- Usually produce an approximation of *constantly-decreasing test-set loss* seen in *full-dataset GD*;\n","\n","- May speed-up training, with no guarantee - however - on how low will be the loss at convergence;\n","\n","- Put the main training focus on differences among data-points and decision boundaries, sometimes regardless of the most reasonable similarity-based representation.\n","\n","\n","As we can see, for our specific goal, the latter may be a favourable regime, provided that optimization is carried out thoughtfully."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"PtJNzN33cX_e"},"outputs":[],"source":["# Hyperparameters & co.\n","\n","minibatch_size_train: int = 1024 # I know it's high; I just want a \"little\" more stability\n","minibatch_size_test: int = 512\n","\n","nrepochs = 90   # This number os tuned to the minimum necessary for stable convergence to the result\n","\n","lossfn = F.nll_loss"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"N8YTl9NTUySq"},"outputs":[],"source":["train_loader, test_loader, test_on_train_loader = spawn_mnist_loaders(\n","    batch_size_train=minibatch_size_train,\n","    batch_size_test=minibatch_size_test,\n","    cuda_accel=bool(device == \"cuda\"),\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### Comment:\n","\n","The network is a good compromise between number of parameters, training performance, and a still *easily-steerable* parameter space.  \n","\n","In particular, though only $3$-layered, the MLP exploits *hyperfeaturization* (the choice to include hidden layers larger than input the size; still not close to $(\\# \\text{inputs} \\times \\# \\text{outputs})$) and presents in any case a relatively large (for the problem) number of parameters.  \n","\n","**About the optimizer**:\n","\n","- `Lookahead` optimizer. A fresh approach to *inner-loop optimization* applied to *neural network optimizers*. The optimizer copies the weights of the network (called *slow weights*), updates $k$ times such copy (called *fast weights*), according to the loss minimization criterion, with an auxiliary optimizer, and finally updates *once* the *slow weights* with a step in the direction of the sum of *fast weights* updates. Such optimization schedule is very robust to noise and to *ruggedness-induced jitter*.  \n","\n","- The `MADGRAD` optimizer, used (with maybe overkill attitude) as the *inner-loop optimizer* of the above, is a new, *doubly-averaged* adaptive (as in *AdaGrad*), *explicitly-momentumized* (as in *SGD with momentum*) optimizer by Facebook AI Research. It may be seen (with much approximation) as an attempt to combine *Adam-like* convergence speed and adaptiveness (successfully!), *SGD-like* generalizability (requires care!), and a tunable *momentum* (it's part of te design)."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["model = Sequential(\n","    # -> Input is here <-\n","\n","    # POST-INPUT BLOCK:\n","    Linear(in_features=28*28, out_features=1500, bias=True),    # Hyperfeaturize ~2*input\n","    Mish(),\n","\n","    # HIDDEN BLOCK:\n","    BatchNorm1d(num_features=1500, affine=True),\n","    Linear(in_features=1500, out_features=500, bias=True),      # Compress ~0.75*input\n","    Mish(),\n","\n","    # PRE-OUTPUT BLOCK:\n","    BatchNorm1d(num_features=500, affine=True),\n","    Linear(in_features=500, out_features=10, bias=True),        # To output\n","    LogSoftmax(dim=1)\n","\n","    # -> Output is here <-\n","        ).to(device)\n","\n","if device == \"cpu\":\n","    raise RuntimeError(\"The MADGRAD optimizer won't work without a GPU. You may want to use RAdam instead! ;)\")\n","\n","base_optimizer = MadGrad(model.parameters(), lr=0.00017)\n","optimizer      = Lookahead(base_optimizer, la_steps=4)\n","scheduler      = MultiStepLR(optimizer, milestones=[], gamma=0.4) # No dampening needed here!"]},{"cell_type":"markdown","metadata":{},"source":["#### Comment:\n","\n","A *weight-initialization* scheme that tries to evenly spread weights in such way to fully exploit the three different *regimes* offered by the `Mish` activation function (asymptotically zero, negative, asymptotically linear) with a *fanin/fanout*-inspired approach (as the one in *Xavier* or *Kaiming* initialization)."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Initialize weights and biases in the proper way ;)\n","for layr in model:\n","    mishlayer_init(layr)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":"=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─Linear: 1-1                            1,177,500\n├─Mish: 1-2                              --\n├─BatchNorm1d: 1-3                       3,000\n├─Linear: 1-4                            750,500\n├─Mish: 1-5                              --\n├─BatchNorm1d: 1-6                       1,000\n├─Linear: 1-7                            5,010\n├─LogSoftmax: 1-8                        --\n=================================================================\nTotal params: 1,937,010\nTrainable params: 1,937,010\nNon-trainable params: 0\n================================================================="},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["summary(model)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TRAINING...\n","Train Epoch: 1 [0/60000 (0%)]\tAvg. loss: 2.829545\n","Train Epoch: 1 [15360/60000 (25%)]\tAvg. loss: 2.633122\n","Train Epoch: 1 [30720/60000 (51%)]\tAvg. loss: 2.600081\n","Train Epoch: 1 [46080/60000 (76%)]\tAvg. loss: 2.574432\n","\n","ON TRAINING SET:\n","Average loss: 2.3659, Accuracy: 8430/60000 (14%)\n","\n","ON TEST SET:\n","Average loss: 2.4880, Accuracy: 960/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 2 [0/60000 (0%)]\tAvg. loss: 2.355809\n","Train Epoch: 2 [15360/60000 (25%)]\tAvg. loss: 2.352004\n","Train Epoch: 2 [30720/60000 (51%)]\tAvg. loss: 2.353787\n","Train Epoch: 2 [46080/60000 (76%)]\tAvg. loss: 2.355460\n","\n","ON TRAINING SET:\n","Average loss: 2.2313, Accuracy: 11100/60000 (18%)\n","\n","ON TEST SET:\n","Average loss: 2.4167, Accuracy: 1014/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 3 [0/60000 (0%)]\tAvg. loss: 2.230685\n","Train Epoch: 3 [15360/60000 (25%)]\tAvg. loss: 2.226574\n","Train Epoch: 3 [30720/60000 (51%)]\tAvg. loss: 2.235333\n","Train Epoch: 3 [46080/60000 (76%)]\tAvg. loss: 2.245528\n","\n","ON TRAINING SET:\n","Average loss: 2.1439, Accuracy: 13698/60000 (23%)\n","\n","ON TEST SET:\n","Average loss: 2.3159, Accuracy: 1350/10000 (14%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 4 [0/60000 (0%)]\tAvg. loss: 2.146974\n","Train Epoch: 4 [15360/60000 (25%)]\tAvg. loss: 2.144077\n","Train Epoch: 4 [30720/60000 (51%)]\tAvg. loss: 2.159066\n","Train Epoch: 4 [46080/60000 (76%)]\tAvg. loss: 2.174627\n","\n","ON TRAINING SET:\n","Average loss: 2.0618, Accuracy: 16202/60000 (27%)\n","\n","ON TEST SET:\n","Average loss: 2.3875, Accuracy: 1272/10000 (13%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 5 [0/60000 (0%)]\tAvg. loss: 2.051428\n","Train Epoch: 5 [15360/60000 (25%)]\tAvg. loss: 2.073435\n","Train Epoch: 5 [30720/60000 (51%)]\tAvg. loss: 2.084896\n","Train Epoch: 5 [46080/60000 (76%)]\tAvg. loss: 2.102977\n","\n","ON TRAINING SET:\n","Average loss: 1.9797, Accuracy: 18656/60000 (31%)\n","\n","ON TEST SET:\n","Average loss: 2.4760, Accuracy: 1103/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 6 [0/60000 (0%)]\tAvg. loss: 1.992233\n","Train Epoch: 6 [15360/60000 (25%)]\tAvg. loss: 1.998455\n","Train Epoch: 6 [30720/60000 (51%)]\tAvg. loss: 2.016511\n","Train Epoch: 6 [46080/60000 (76%)]\tAvg. loss: 2.032103\n","\n","ON TRAINING SET:\n","Average loss: 1.8917, Accuracy: 21126/60000 (35%)\n","\n","ON TEST SET:\n","Average loss: 2.4763, Accuracy: 1095/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 7 [0/60000 (0%)]\tAvg. loss: 1.946275\n","Train Epoch: 7 [15360/60000 (25%)]\tAvg. loss: 1.912188\n","Train Epoch: 7 [30720/60000 (51%)]\tAvg. loss: 1.924299\n","Train Epoch: 7 [46080/60000 (76%)]\tAvg. loss: 1.944657\n","\n","ON TRAINING SET:\n","Average loss: 1.7985, Accuracy: 23629/60000 (39%)\n","\n","ON TEST SET:\n","Average loss: 2.6188, Accuracy: 898/10000 (9%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 8 [0/60000 (0%)]\tAvg. loss: 1.784129\n","Train Epoch: 8 [15360/60000 (25%)]\tAvg. loss: 1.804875\n","Train Epoch: 8 [30720/60000 (51%)]\tAvg. loss: 1.824782\n","Train Epoch: 8 [46080/60000 (76%)]\tAvg. loss: 1.850564\n","\n","ON TRAINING SET:\n","Average loss: 1.6872, Accuracy: 26438/60000 (44%)\n","\n","ON TEST SET:\n","Average loss: 2.5257, Accuracy: 1190/10000 (12%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 9 [0/60000 (0%)]\tAvg. loss: 1.671138\n","Train Epoch: 9 [15360/60000 (25%)]\tAvg. loss: 1.715115\n","Train Epoch: 9 [30720/60000 (51%)]\tAvg. loss: 1.730934\n","Train Epoch: 9 [46080/60000 (76%)]\tAvg. loss: 1.750979\n","\n","ON TRAINING SET:\n","Average loss: 1.5658, Accuracy: 29797/60000 (50%)\n","\n","ON TEST SET:\n","Average loss: 2.6758, Accuracy: 919/10000 (9%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 10 [0/60000 (0%)]\tAvg. loss: 1.594730\n","Train Epoch: 10 [15360/60000 (25%)]\tAvg. loss: 1.578735\n","Train Epoch: 10 [30720/60000 (51%)]\tAvg. loss: 1.610055\n","Train Epoch: 10 [46080/60000 (76%)]\tAvg. loss: 1.642021\n","\n","ON TRAINING SET:\n","Average loss: 1.4426, Accuracy: 32867/60000 (55%)\n","\n","ON TEST SET:\n","Average loss: 2.6794, Accuracy: 1230/10000 (12%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 11 [0/60000 (0%)]\tAvg. loss: 1.501898\n","Train Epoch: 11 [15360/60000 (25%)]\tAvg. loss: 1.464185\n","Train Epoch: 11 [30720/60000 (51%)]\tAvg. loss: 1.488251\n","Train Epoch: 11 [46080/60000 (76%)]\tAvg. loss: 1.520473\n","\n","ON TRAINING SET:\n","Average loss: 1.3218, Accuracy: 35523/60000 (59%)\n","\n","ON TEST SET:\n","Average loss: 2.8465, Accuracy: 1074/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 12 [0/60000 (0%)]\tAvg. loss: 1.300051\n","Train Epoch: 12 [15360/60000 (25%)]\tAvg. loss: 1.339557\n","Train Epoch: 12 [30720/60000 (51%)]\tAvg. loss: 1.368850\n","Train Epoch: 12 [46080/60000 (76%)]\tAvg. loss: 1.396421\n","\n","ON TRAINING SET:\n","Average loss: 1.1963, Accuracy: 38209/60000 (64%)\n","\n","ON TEST SET:\n","Average loss: 2.9407, Accuracy: 1062/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 13 [0/60000 (0%)]\tAvg. loss: 1.233236\n","Train Epoch: 13 [15360/60000 (25%)]\tAvg. loss: 1.224691\n","Train Epoch: 13 [30720/60000 (51%)]\tAvg. loss: 1.246021\n","Train Epoch: 13 [46080/60000 (76%)]\tAvg. loss: 1.278412\n","\n","ON TRAINING SET:\n","Average loss: 1.0864, Accuracy: 40861/60000 (68%)\n","\n","ON TEST SET:\n","Average loss: 3.0312, Accuracy: 1009/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 14 [0/60000 (0%)]\tAvg. loss: 1.078147\n","Train Epoch: 14 [15360/60000 (25%)]\tAvg. loss: 1.111064\n","Train Epoch: 14 [30720/60000 (51%)]\tAvg. loss: 1.129366\n","Train Epoch: 14 [46080/60000 (76%)]\tAvg. loss: 1.159850\n","\n","ON TRAINING SET:\n","Average loss: 0.9682, Accuracy: 43241/60000 (72%)\n","\n","ON TEST SET:\n","Average loss: 3.1835, Accuracy: 1074/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 15 [0/60000 (0%)]\tAvg. loss: 0.971623\n","Train Epoch: 15 [15360/60000 (25%)]\tAvg. loss: 0.991140\n","Train Epoch: 15 [30720/60000 (51%)]\tAvg. loss: 1.011101\n","Train Epoch: 15 [46080/60000 (76%)]\tAvg. loss: 1.042795\n","\n","ON TRAINING SET:\n","Average loss: 0.8685, Accuracy: 45161/60000 (75%)\n","\n","ON TEST SET:\n","Average loss: 3.3817, Accuracy: 1026/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 16 [0/60000 (0%)]\tAvg. loss: 0.851344\n","Train Epoch: 16 [15360/60000 (25%)]\tAvg. loss: 0.868488\n","Train Epoch: 16 [30720/60000 (51%)]\tAvg. loss: 0.898585\n","Train Epoch: 16 [46080/60000 (76%)]\tAvg. loss: 0.927256\n","\n","ON TRAINING SET:\n","Average loss: 0.7544, Accuracy: 47619/60000 (79%)\n","\n","ON TEST SET:\n","Average loss: 3.4290, Accuracy: 1240/10000 (12%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 17 [0/60000 (0%)]\tAvg. loss: 0.747303\n","Train Epoch: 17 [15360/60000 (25%)]\tAvg. loss: 0.773647\n","Train Epoch: 17 [30720/60000 (51%)]\tAvg. loss: 0.790625\n","Train Epoch: 17 [46080/60000 (76%)]\tAvg. loss: 0.812507\n","\n","ON TRAINING SET:\n","Average loss: 0.6498, Accuracy: 49839/60000 (83%)\n","\n","ON TEST SET:\n","Average loss: 3.5956, Accuracy: 1002/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 18 [0/60000 (0%)]\tAvg. loss: 0.641335\n","Train Epoch: 18 [15360/60000 (25%)]\tAvg. loss: 0.677570\n","Train Epoch: 18 [30720/60000 (51%)]\tAvg. loss: 0.694487\n","Train Epoch: 18 [46080/60000 (76%)]\tAvg. loss: 0.714375\n","\n","ON TRAINING SET:\n","Average loss: 0.5714, Accuracy: 51079/60000 (85%)\n","\n","ON TEST SET:\n","Average loss: 3.8425, Accuracy: 929/10000 (9%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 19 [0/60000 (0%)]\tAvg. loss: 0.570752\n","Train Epoch: 19 [15360/60000 (25%)]\tAvg. loss: 0.595061\n","Train Epoch: 19 [30720/60000 (51%)]\tAvg. loss: 0.605918\n","Train Epoch: 19 [46080/60000 (76%)]\tAvg. loss: 0.624386\n","\n","ON TRAINING SET:\n","Average loss: 0.5003, Accuracy: 52318/60000 (87%)\n","\n","ON TEST SET:\n","Average loss: 3.8778, Accuracy: 1028/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 20 [0/60000 (0%)]\tAvg. loss: 0.458611\n","Train Epoch: 20 [15360/60000 (25%)]\tAvg. loss: 0.521302\n","Train Epoch: 20 [30720/60000 (51%)]\tAvg. loss: 0.534436\n","Train Epoch: 20 [46080/60000 (76%)]\tAvg. loss: 0.548595\n","\n","ON TRAINING SET:\n","Average loss: 0.4365, Accuracy: 53327/60000 (89%)\n","\n","ON TEST SET:\n","Average loss: 4.0646, Accuracy: 983/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 21 [0/60000 (0%)]\tAvg. loss: 0.440558\n","Train Epoch: 21 [15360/60000 (25%)]\tAvg. loss: 0.462578\n","Train Epoch: 21 [30720/60000 (51%)]\tAvg. loss: 0.471096\n","Train Epoch: 21 [46080/60000 (76%)]\tAvg. loss: 0.479785\n","\n","ON TRAINING SET:\n","Average loss: 0.3810, Accuracy: 54264/60000 (90%)\n","\n","ON TEST SET:\n","Average loss: 4.0972, Accuracy: 1170/10000 (12%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 22 [0/60000 (0%)]\tAvg. loss: 0.357142\n","Train Epoch: 22 [15360/60000 (25%)]\tAvg. loss: 0.406697\n","Train Epoch: 22 [30720/60000 (51%)]\tAvg. loss: 0.415382\n","Train Epoch: 22 [46080/60000 (76%)]\tAvg. loss: 0.423334\n","\n","ON TRAINING SET:\n","Average loss: 0.3364, Accuracy: 55094/60000 (92%)\n","\n","ON TEST SET:\n","Average loss: 4.2752, Accuracy: 1068/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 23 [0/60000 (0%)]\tAvg. loss: 0.312092\n","Train Epoch: 23 [15360/60000 (25%)]\tAvg. loss: 0.351911\n","Train Epoch: 23 [30720/60000 (51%)]\tAvg. loss: 0.358757\n","Train Epoch: 23 [46080/60000 (76%)]\tAvg. loss: 0.370148\n","\n","ON TRAINING SET:\n","Average loss: 0.3051, Accuracy: 55368/60000 (92%)\n","\n","ON TEST SET:\n","Average loss: 4.4759, Accuracy: 1007/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 24 [0/60000 (0%)]\tAvg. loss: 0.300221\n","Train Epoch: 24 [15360/60000 (25%)]\tAvg. loss: 0.317938\n","Train Epoch: 24 [30720/60000 (51%)]\tAvg. loss: 0.324924\n","Train Epoch: 24 [46080/60000 (76%)]\tAvg. loss: 0.330144\n","\n","ON TRAINING SET:\n","Average loss: 0.2714, Accuracy: 55846/60000 (93%)\n","\n","ON TEST SET:\n","Average loss: 4.5882, Accuracy: 1105/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 25 [0/60000 (0%)]\tAvg. loss: 0.258202\n","Train Epoch: 25 [15360/60000 (25%)]\tAvg. loss: 0.285608\n","Train Epoch: 25 [30720/60000 (51%)]\tAvg. loss: 0.292175\n","Train Epoch: 25 [46080/60000 (76%)]\tAvg. loss: 0.296277\n","\n","ON TRAINING SET:\n","Average loss: 0.2334, Accuracy: 56522/60000 (94%)\n","\n","ON TEST SET:\n","Average loss: 4.7102, Accuracy: 991/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 26 [0/60000 (0%)]\tAvg. loss: 0.229357\n","Train Epoch: 26 [15360/60000 (25%)]\tAvg. loss: 0.260180\n","Train Epoch: 26 [30720/60000 (51%)]\tAvg. loss: 0.261953\n","Train Epoch: 26 [46080/60000 (76%)]\tAvg. loss: 0.266544\n","\n","ON TRAINING SET:\n","Average loss: 0.2157, Accuracy: 56624/60000 (94%)\n","\n","ON TEST SET:\n","Average loss: 4.6240, Accuracy: 1187/10000 (12%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 27 [0/60000 (0%)]\tAvg. loss: 0.247504\n","Train Epoch: 27 [15360/60000 (25%)]\tAvg. loss: 0.232890\n","Train Epoch: 27 [30720/60000 (51%)]\tAvg. loss: 0.238748\n","Train Epoch: 27 [46080/60000 (76%)]\tAvg. loss: 0.242807\n","\n","ON TRAINING SET:\n","Average loss: 0.2011, Accuracy: 56823/60000 (95%)\n","\n","ON TEST SET:\n","Average loss: 4.7559, Accuracy: 1157/10000 (12%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 28 [0/60000 (0%)]\tAvg. loss: 0.188306\n","Train Epoch: 28 [15360/60000 (25%)]\tAvg. loss: 0.204118\n","Train Epoch: 28 [30720/60000 (51%)]\tAvg. loss: 0.211221\n","Train Epoch: 28 [46080/60000 (76%)]\tAvg. loss: 0.217291\n","\n","ON TRAINING SET:\n","Average loss: 0.1783, Accuracy: 57255/60000 (95%)\n","\n","ON TEST SET:\n","Average loss: 4.9419, Accuracy: 1063/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 29 [0/60000 (0%)]\tAvg. loss: 0.165704\n","Train Epoch: 29 [15360/60000 (25%)]\tAvg. loss: 0.190064\n","Train Epoch: 29 [30720/60000 (51%)]\tAvg. loss: 0.195467\n","Train Epoch: 29 [46080/60000 (76%)]\tAvg. loss: 0.197092\n","\n","ON TRAINING SET:\n","Average loss: 0.1656, Accuracy: 57269/60000 (95%)\n","\n","ON TEST SET:\n","Average loss: 5.1092, Accuracy: 1062/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 30 [0/60000 (0%)]\tAvg. loss: 0.181331\n","Train Epoch: 30 [15360/60000 (25%)]\tAvg. loss: 0.173121\n","Train Epoch: 30 [30720/60000 (51%)]\tAvg. loss: 0.179950\n","Train Epoch: 30 [46080/60000 (76%)]\tAvg. loss: 0.183224\n","\n","ON TRAINING SET:\n","Average loss: 0.1561, Accuracy: 57475/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 5.1359, Accuracy: 1031/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 31 [0/60000 (0%)]\tAvg. loss: 0.171201\n","Train Epoch: 31 [15360/60000 (25%)]\tAvg. loss: 0.160698\n","Train Epoch: 31 [30720/60000 (51%)]\tAvg. loss: 0.168293\n","Train Epoch: 31 [46080/60000 (76%)]\tAvg. loss: 0.175099\n","\n","ON TRAINING SET:\n","Average loss: 0.1483, Accuracy: 57583/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 5.2548, Accuracy: 1017/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 32 [0/60000 (0%)]\tAvg. loss: 0.163844\n","Train Epoch: 32 [15360/60000 (25%)]\tAvg. loss: 0.155620\n","Train Epoch: 32 [30720/60000 (51%)]\tAvg. loss: 0.164461\n","Train Epoch: 32 [46080/60000 (76%)]\tAvg. loss: 0.167346\n","\n","ON TRAINING SET:\n","Average loss: 0.1406, Accuracy: 57675/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 5.2499, Accuracy: 1138/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 33 [0/60000 (0%)]\tAvg. loss: 0.138987\n","Train Epoch: 33 [15360/60000 (25%)]\tAvg. loss: 0.146413\n","Train Epoch: 33 [30720/60000 (51%)]\tAvg. loss: 0.150150\n","Train Epoch: 33 [46080/60000 (76%)]\tAvg. loss: 0.154219\n","\n","ON TRAINING SET:\n","Average loss: 0.1233, Accuracy: 58042/60000 (97%)\n","\n","ON TEST SET:\n","Average loss: 5.3022, Accuracy: 1082/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 34 [0/60000 (0%)]\tAvg. loss: 0.126291\n","Train Epoch: 34 [15360/60000 (25%)]\tAvg. loss: 0.137108\n","Train Epoch: 34 [30720/60000 (51%)]\tAvg. loss: 0.143245\n","Train Epoch: 34 [46080/60000 (76%)]\tAvg. loss: 0.148791\n","\n","ON TRAINING SET:\n","Average loss: 0.1205, Accuracy: 58025/60000 (97%)\n","\n","ON TEST SET:\n","Average loss: 5.4193, Accuracy: 1028/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 35 [0/60000 (0%)]\tAvg. loss: 0.120902\n","Train Epoch: 35 [15360/60000 (25%)]\tAvg. loss: 0.131936\n","Train Epoch: 35 [30720/60000 (51%)]\tAvg. loss: 0.138919\n","Train Epoch: 35 [46080/60000 (76%)]\tAvg. loss: 0.145235\n","\n","ON TRAINING SET:\n","Average loss: 0.1219, Accuracy: 57955/60000 (97%)\n","\n","ON TEST SET:\n","Average loss: 5.4641, Accuracy: 1107/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 36 [0/60000 (0%)]\tAvg. loss: 0.111444\n","Train Epoch: 36 [15360/60000 (25%)]\tAvg. loss: 0.130448\n","Train Epoch: 36 [30720/60000 (51%)]\tAvg. loss: 0.132958\n","Train Epoch: 36 [46080/60000 (76%)]\tAvg. loss: 0.137795\n","\n","ON TRAINING SET:\n","Average loss: 0.1194, Accuracy: 58062/60000 (97%)\n","\n","ON TEST SET:\n","Average loss: 5.5012, Accuracy: 1109/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 37 [0/60000 (0%)]\tAvg. loss: 0.107746\n","Train Epoch: 37 [15360/60000 (25%)]\tAvg. loss: 0.124912\n","Train Epoch: 37 [30720/60000 (51%)]\tAvg. loss: 0.130185\n","Train Epoch: 37 [46080/60000 (76%)]\tAvg. loss: 0.133016\n","\n","ON TRAINING SET:\n","Average loss: 0.1093, Accuracy: 58250/60000 (97%)\n","\n","ON TEST SET:\n","Average loss: 5.6037, Accuracy: 1079/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 38 [0/60000 (0%)]\tAvg. loss: 0.117463\n","Train Epoch: 38 [15360/60000 (25%)]\tAvg. loss: 0.115689\n","Train Epoch: 38 [30720/60000 (51%)]\tAvg. loss: 0.119017\n","Train Epoch: 38 [46080/60000 (76%)]\tAvg. loss: 0.125008\n","\n","ON TRAINING SET:\n","Average loss: 0.1005, Accuracy: 58443/60000 (97%)\n","\n","ON TEST SET:\n","Average loss: 5.6949, Accuracy: 1047/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 39 [0/60000 (0%)]\tAvg. loss: 0.109535\n","Train Epoch: 39 [15360/60000 (25%)]\tAvg. loss: 0.112121\n","Train Epoch: 39 [30720/60000 (51%)]\tAvg. loss: 0.117504\n","Train Epoch: 39 [46080/60000 (76%)]\tAvg. loss: 0.122171\n","\n","ON TRAINING SET:\n","Average loss: 0.0945, Accuracy: 58544/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 5.7676, Accuracy: 1042/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 40 [0/60000 (0%)]\tAvg. loss: 0.083327\n","Train Epoch: 40 [15360/60000 (25%)]\tAvg. loss: 0.104812\n","Train Epoch: 40 [30720/60000 (51%)]\tAvg. loss: 0.110351\n","Train Epoch: 40 [46080/60000 (76%)]\tAvg. loss: 0.112593\n","\n","ON TRAINING SET:\n","Average loss: 0.0917, Accuracy: 58623/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 5.7007, Accuracy: 1066/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 41 [0/60000 (0%)]\tAvg. loss: 0.112069\n","Train Epoch: 41 [15360/60000 (25%)]\tAvg. loss: 0.103994\n","Train Epoch: 41 [30720/60000 (51%)]\tAvg. loss: 0.109978\n","Train Epoch: 41 [46080/60000 (76%)]\tAvg. loss: 0.113806\n","\n","ON TRAINING SET:\n","Average loss: 0.0956, Accuracy: 58507/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 5.8274, Accuracy: 1060/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 42 [0/60000 (0%)]\tAvg. loss: 0.079744\n","Train Epoch: 42 [15360/60000 (25%)]\tAvg. loss: 0.112846\n","Train Epoch: 42 [30720/60000 (51%)]\tAvg. loss: 0.114811\n","Train Epoch: 42 [46080/60000 (76%)]\tAvg. loss: 0.119482\n","\n","ON TRAINING SET:\n","Average loss: 0.1003, Accuracy: 58511/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 5.7638, Accuracy: 1098/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 43 [0/60000 (0%)]\tAvg. loss: 0.127696\n","Train Epoch: 43 [15360/60000 (25%)]\tAvg. loss: 0.116716\n","Train Epoch: 43 [30720/60000 (51%)]\tAvg. loss: 0.118389\n","Train Epoch: 43 [46080/60000 (76%)]\tAvg. loss: 0.125351\n","\n","ON TRAINING SET:\n","Average loss: 0.1098, Accuracy: 58312/60000 (97%)\n","\n","ON TEST SET:\n","Average loss: 5.8650, Accuracy: 1144/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 44 [0/60000 (0%)]\tAvg. loss: 0.105702\n","Train Epoch: 44 [15360/60000 (25%)]\tAvg. loss: 0.134927\n","Train Epoch: 44 [30720/60000 (51%)]\tAvg. loss: 0.144784\n","Train Epoch: 44 [46080/60000 (76%)]\tAvg. loss: 0.156299\n","\n","ON TRAINING SET:\n","Average loss: 0.1741, Accuracy: 57089/60000 (95%)\n","\n","ON TEST SET:\n","Average loss: 5.9998, Accuracy: 1044/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 45 [0/60000 (0%)]\tAvg. loss: 0.145899\n","Train Epoch: 45 [15360/60000 (25%)]\tAvg. loss: 0.206240\n","Train Epoch: 45 [30720/60000 (51%)]\tAvg. loss: 0.225131\n","Train Epoch: 45 [46080/60000 (76%)]\tAvg. loss: 0.254455\n","\n","ON TRAINING SET:\n","Average loss: 0.2789, Accuracy: 54707/60000 (91%)\n","\n","ON TEST SET:\n","Average loss: 5.9647, Accuracy: 1106/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 46 [0/60000 (0%)]\tAvg. loss: 0.355456\n","Train Epoch: 46 [15360/60000 (25%)]\tAvg. loss: 0.390907\n","Train Epoch: 46 [30720/60000 (51%)]\tAvg. loss: 0.436399\n","Train Epoch: 46 [46080/60000 (76%)]\tAvg. loss: 0.488428\n","\n","ON TRAINING SET:\n","Average loss: 0.4664, Accuracy: 50655/60000 (84%)\n","\n","ON TEST SET:\n","Average loss: 5.8354, Accuracy: 1071/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 47 [0/60000 (0%)]\tAvg. loss: 0.434820\n","Train Epoch: 47 [15360/60000 (25%)]\tAvg. loss: 0.563780\n","Train Epoch: 47 [30720/60000 (51%)]\tAvg. loss: 0.585395\n","Train Epoch: 47 [46080/60000 (76%)]\tAvg. loss: 0.608466\n","\n","ON TRAINING SET:\n","Average loss: 0.4149, Accuracy: 51871/60000 (86%)\n","\n","ON TEST SET:\n","Average loss: 5.6203, Accuracy: 996/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 48 [0/60000 (0%)]\tAvg. loss: 0.341902\n","Train Epoch: 48 [15360/60000 (25%)]\tAvg. loss: 0.441195\n","Train Epoch: 48 [30720/60000 (51%)]\tAvg. loss: 0.444494\n","Train Epoch: 48 [46080/60000 (76%)]\tAvg. loss: 0.445239\n","\n","ON TRAINING SET:\n","Average loss: 0.2621, Accuracy: 55149/60000 (92%)\n","\n","ON TEST SET:\n","Average loss: 5.5752, Accuracy: 1049/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 49 [0/60000 (0%)]\tAvg. loss: 0.229118\n","Train Epoch: 49 [15360/60000 (25%)]\tAvg. loss: 0.274133\n","Train Epoch: 49 [30720/60000 (51%)]\tAvg. loss: 0.275815\n","Train Epoch: 49 [46080/60000 (76%)]\tAvg. loss: 0.270669\n","\n","ON TRAINING SET:\n","Average loss: 0.1497, Accuracy: 57814/60000 (96%)\n","\n","ON TEST SET:\n","Average loss: 5.5484, Accuracy: 1006/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 50 [0/60000 (0%)]\tAvg. loss: 0.168349\n","Train Epoch: 50 [15360/60000 (25%)]\tAvg. loss: 0.162610\n","Train Epoch: 50 [30720/60000 (51%)]\tAvg. loss: 0.162053\n","Train Epoch: 50 [46080/60000 (76%)]\tAvg. loss: 0.161057\n","\n","ON TRAINING SET:\n","Average loss: 0.0949, Accuracy: 58765/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 5.6006, Accuracy: 1041/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 51 [0/60000 (0%)]\tAvg. loss: 0.085908\n","Train Epoch: 51 [15360/60000 (25%)]\tAvg. loss: 0.106471\n","Train Epoch: 51 [30720/60000 (51%)]\tAvg. loss: 0.104458\n","Train Epoch: 51 [46080/60000 (76%)]\tAvg. loss: 0.104283\n","\n","ON TRAINING SET:\n","Average loss: 0.0674, Accuracy: 59079/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 5.7375, Accuracy: 1065/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 52 [0/60000 (0%)]\tAvg. loss: 0.054356\n","Train Epoch: 52 [15360/60000 (25%)]\tAvg. loss: 0.078201\n","Train Epoch: 52 [30720/60000 (51%)]\tAvg. loss: 0.077688\n","Train Epoch: 52 [46080/60000 (76%)]\tAvg. loss: 0.079951\n","\n","ON TRAINING SET:\n","Average loss: 0.0619, Accuracy: 59070/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 5.7772, Accuracy: 1034/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 53 [0/60000 (0%)]\tAvg. loss: 0.056932\n","Train Epoch: 53 [15360/60000 (25%)]\tAvg. loss: 0.069132\n","Train Epoch: 53 [30720/60000 (51%)]\tAvg. loss: 0.070393\n","Train Epoch: 53 [46080/60000 (76%)]\tAvg. loss: 0.071985\n","\n","ON TRAINING SET:\n","Average loss: 0.0580, Accuracy: 59077/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 5.8833, Accuracy: 989/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 54 [0/60000 (0%)]\tAvg. loss: 0.058241\n","Train Epoch: 54 [15360/60000 (25%)]\tAvg. loss: 0.062611\n","Train Epoch: 54 [30720/60000 (51%)]\tAvg. loss: 0.063494\n","Train Epoch: 54 [46080/60000 (76%)]\tAvg. loss: 0.065103\n","\n","ON TRAINING SET:\n","Average loss: 0.0526, Accuracy: 59134/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 5.9084, Accuracy: 1023/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 55 [0/60000 (0%)]\tAvg. loss: 0.053375\n","Train Epoch: 55 [15360/60000 (25%)]\tAvg. loss: 0.056410\n","Train Epoch: 55 [30720/60000 (51%)]\tAvg. loss: 0.058078\n","Train Epoch: 55 [46080/60000 (76%)]\tAvg. loss: 0.061128\n","\n","ON TRAINING SET:\n","Average loss: 0.0419, Accuracy: 59371/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 5.8756, Accuracy: 1076/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 56 [0/60000 (0%)]\tAvg. loss: 0.032914\n","Train Epoch: 56 [15360/60000 (25%)]\tAvg. loss: 0.048539\n","Train Epoch: 56 [30720/60000 (51%)]\tAvg. loss: 0.049977\n","Train Epoch: 56 [46080/60000 (76%)]\tAvg. loss: 0.052950\n","\n","ON TRAINING SET:\n","Average loss: 0.0463, Accuracy: 59246/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 6.0167, Accuracy: 1048/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 57 [0/60000 (0%)]\tAvg. loss: 0.036807\n","Train Epoch: 57 [15360/60000 (25%)]\tAvg. loss: 0.051534\n","Train Epoch: 57 [30720/60000 (51%)]\tAvg. loss: 0.049618\n","Train Epoch: 57 [46080/60000 (76%)]\tAvg. loss: 0.050180\n","\n","ON TRAINING SET:\n","Average loss: 0.0398, Accuracy: 59367/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 5.9936, Accuracy: 1047/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 58 [0/60000 (0%)]\tAvg. loss: 0.027444\n","Train Epoch: 58 [15360/60000 (25%)]\tAvg. loss: 0.046762\n","Train Epoch: 58 [30720/60000 (51%)]\tAvg. loss: 0.045919\n","Train Epoch: 58 [46080/60000 (76%)]\tAvg. loss: 0.046959\n","\n","ON TRAINING SET:\n","Average loss: 0.0358, Accuracy: 59469/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 5.9381, Accuracy: 1130/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 59 [0/60000 (0%)]\tAvg. loss: 0.032076\n","Train Epoch: 59 [15360/60000 (25%)]\tAvg. loss: 0.037591\n","Train Epoch: 59 [30720/60000 (51%)]\tAvg. loss: 0.038621\n","Train Epoch: 59 [46080/60000 (76%)]\tAvg. loss: 0.039351\n","\n","ON TRAINING SET:\n","Average loss: 0.0308, Accuracy: 59532/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 6.1686, Accuracy: 1029/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 60 [0/60000 (0%)]\tAvg. loss: 0.042073\n","Train Epoch: 60 [15360/60000 (25%)]\tAvg. loss: 0.035721\n","Train Epoch: 60 [30720/60000 (51%)]\tAvg. loss: 0.036113\n","Train Epoch: 60 [46080/60000 (76%)]\tAvg. loss: 0.037683\n","\n","ON TRAINING SET:\n","Average loss: 0.0350, Accuracy: 59453/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 6.0725, Accuracy: 1048/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 61 [0/60000 (0%)]\tAvg. loss: 0.033665\n","Train Epoch: 61 [15360/60000 (25%)]\tAvg. loss: 0.036418\n","Train Epoch: 61 [30720/60000 (51%)]\tAvg. loss: 0.036395\n","Train Epoch: 61 [46080/60000 (76%)]\tAvg. loss: 0.037986\n","\n","ON TRAINING SET:\n","Average loss: 0.0300, Accuracy: 59538/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 6.1180, Accuracy: 1038/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 62 [0/60000 (0%)]\tAvg. loss: 0.029707\n","Train Epoch: 62 [15360/60000 (25%)]\tAvg. loss: 0.031872\n","Train Epoch: 62 [30720/60000 (51%)]\tAvg. loss: 0.031990\n","Train Epoch: 62 [46080/60000 (76%)]\tAvg. loss: 0.033688\n","\n","ON TRAINING SET:\n","Average loss: 0.0236, Accuracy: 59687/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 6.0900, Accuracy: 1086/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 63 [0/60000 (0%)]\tAvg. loss: 0.037267\n","Train Epoch: 63 [15360/60000 (25%)]\tAvg. loss: 0.031925\n","Train Epoch: 63 [30720/60000 (51%)]\tAvg. loss: 0.030389\n","Train Epoch: 63 [46080/60000 (76%)]\tAvg. loss: 0.031286\n","\n","ON TRAINING SET:\n","Average loss: 0.0223, Accuracy: 59698/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 6.2081, Accuracy: 1062/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 64 [0/60000 (0%)]\tAvg. loss: 0.023848\n","Train Epoch: 64 [15360/60000 (25%)]\tAvg. loss: 0.029690\n","Train Epoch: 64 [30720/60000 (51%)]\tAvg. loss: 0.030009\n","Train Epoch: 64 [46080/60000 (76%)]\tAvg. loss: 0.029539\n","\n","ON TRAINING SET:\n","Average loss: 0.0194, Accuracy: 59761/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.2907, Accuracy: 1032/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 65 [0/60000 (0%)]\tAvg. loss: 0.024100\n","Train Epoch: 65 [15360/60000 (25%)]\tAvg. loss: 0.025055\n","Train Epoch: 65 [30720/60000 (51%)]\tAvg. loss: 0.025420\n","Train Epoch: 65 [46080/60000 (76%)]\tAvg. loss: 0.025862\n","\n","ON TRAINING SET:\n","Average loss: 0.0221, Accuracy: 59684/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 6.2994, Accuracy: 1037/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 66 [0/60000 (0%)]\tAvg. loss: 0.022256\n","Train Epoch: 66 [15360/60000 (25%)]\tAvg. loss: 0.025406\n","Train Epoch: 66 [30720/60000 (51%)]\tAvg. loss: 0.027282\n","Train Epoch: 66 [46080/60000 (76%)]\tAvg. loss: 0.027748\n","\n","ON TRAINING SET:\n","Average loss: 0.0193, Accuracy: 59758/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.2836, Accuracy: 1079/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 67 [0/60000 (0%)]\tAvg. loss: 0.019543\n","Train Epoch: 67 [15360/60000 (25%)]\tAvg. loss: 0.021791\n","Train Epoch: 67 [30720/60000 (51%)]\tAvg. loss: 0.022991\n","Train Epoch: 67 [46080/60000 (76%)]\tAvg. loss: 0.023489\n","\n","ON TRAINING SET:\n","Average loss: 0.0193, Accuracy: 59726/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.3209, Accuracy: 1025/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 68 [0/60000 (0%)]\tAvg. loss: 0.014898\n","Train Epoch: 68 [15360/60000 (25%)]\tAvg. loss: 0.020821\n","Train Epoch: 68 [30720/60000 (51%)]\tAvg. loss: 0.020733\n","Train Epoch: 68 [46080/60000 (76%)]\tAvg. loss: 0.021013\n","\n","ON TRAINING SET:\n","Average loss: 0.0150, Accuracy: 59819/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.3356, Accuracy: 1081/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 69 [0/60000 (0%)]\tAvg. loss: 0.016480\n","Train Epoch: 69 [15360/60000 (25%)]\tAvg. loss: 0.021400\n","Train Epoch: 69 [30720/60000 (51%)]\tAvg. loss: 0.024694\n","Train Epoch: 69 [46080/60000 (76%)]\tAvg. loss: 0.024638\n","\n","ON TRAINING SET:\n","Average loss: 0.0168, Accuracy: 59777/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.4109, Accuracy: 1044/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 70 [0/60000 (0%)]\tAvg. loss: 0.027508\n","Train Epoch: 70 [15360/60000 (25%)]\tAvg. loss: 0.023926\n","Train Epoch: 70 [30720/60000 (51%)]\tAvg. loss: 0.024354\n","Train Epoch: 70 [46080/60000 (76%)]\tAvg. loss: 0.025870\n","\n","ON TRAINING SET:\n","Average loss: 0.0178, Accuracy: 59765/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.4789, Accuracy: 1039/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 71 [0/60000 (0%)]\tAvg. loss: 0.013918\n","Train Epoch: 71 [15360/60000 (25%)]\tAvg. loss: 0.020131\n","Train Epoch: 71 [30720/60000 (51%)]\tAvg. loss: 0.022376\n","Train Epoch: 71 [46080/60000 (76%)]\tAvg. loss: 0.022951\n","\n","ON TRAINING SET:\n","Average loss: 0.0174, Accuracy: 59754/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.5533, Accuracy: 994/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 72 [0/60000 (0%)]\tAvg. loss: 0.017640\n","Train Epoch: 72 [15360/60000 (25%)]\tAvg. loss: 0.020420\n","Train Epoch: 72 [30720/60000 (51%)]\tAvg. loss: 0.021855\n","Train Epoch: 72 [46080/60000 (76%)]\tAvg. loss: 0.021535\n","\n","ON TRAINING SET:\n","Average loss: 0.0146, Accuracy: 59804/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.3659, Accuracy: 1099/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 73 [0/60000 (0%)]\tAvg. loss: 0.009371\n","Train Epoch: 73 [15360/60000 (25%)]\tAvg. loss: 0.015872\n","Train Epoch: 73 [30720/60000 (51%)]\tAvg. loss: 0.015959\n","Train Epoch: 73 [46080/60000 (76%)]\tAvg. loss: 0.015994\n","\n","ON TRAINING SET:\n","Average loss: 0.0101, Accuracy: 59906/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.4296, Accuracy: 1069/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 74 [0/60000 (0%)]\tAvg. loss: 0.010733\n","Train Epoch: 74 [15360/60000 (25%)]\tAvg. loss: 0.013165\n","Train Epoch: 74 [30720/60000 (51%)]\tAvg. loss: 0.013459\n","Train Epoch: 74 [46080/60000 (76%)]\tAvg. loss: 0.013348\n","\n","ON TRAINING SET:\n","Average loss: 0.0080, Accuracy: 59933/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.4416, Accuracy: 1080/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 75 [0/60000 (0%)]\tAvg. loss: 0.006723\n","Train Epoch: 75 [15360/60000 (25%)]\tAvg. loss: 0.010203\n","Train Epoch: 75 [30720/60000 (51%)]\tAvg. loss: 0.010837\n","Train Epoch: 75 [46080/60000 (76%)]\tAvg. loss: 0.010776\n","\n","ON TRAINING SET:\n","Average loss: 0.0081, Accuracy: 59913/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.5415, Accuracy: 1035/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 76 [0/60000 (0%)]\tAvg. loss: 0.006771\n","Train Epoch: 76 [15360/60000 (25%)]\tAvg. loss: 0.008745\n","Train Epoch: 76 [30720/60000 (51%)]\tAvg. loss: 0.009324\n","Train Epoch: 76 [46080/60000 (76%)]\tAvg. loss: 0.009105\n","\n","ON TRAINING SET:\n","Average loss: 0.0053, Accuracy: 59974/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.5293, Accuracy: 1055/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 77 [0/60000 (0%)]\tAvg. loss: 0.005843\n","Train Epoch: 77 [15360/60000 (25%)]\tAvg. loss: 0.005917\n","Train Epoch: 77 [30720/60000 (51%)]\tAvg. loss: 0.005708\n","Train Epoch: 77 [46080/60000 (76%)]\tAvg. loss: 0.005509\n","\n","ON TRAINING SET:\n","Average loss: 0.0033, Accuracy: 59990/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.5884, Accuracy: 1043/10000 (10%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 78 [0/60000 (0%)]\tAvg. loss: 0.006195\n","Train Epoch: 78 [15360/60000 (25%)]\tAvg. loss: 0.004424\n","Train Epoch: 78 [30720/60000 (51%)]\tAvg. loss: 0.004117\n","Train Epoch: 78 [46080/60000 (76%)]\tAvg. loss: 0.004056\n","\n","ON TRAINING SET:\n","Average loss: 0.0025, Accuracy: 59997/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.5951, Accuracy: 1056/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 79 [0/60000 (0%)]\tAvg. loss: 0.002853\n","Train Epoch: 79 [15360/60000 (25%)]\tAvg. loss: 0.003107\n","Train Epoch: 79 [30720/60000 (51%)]\tAvg. loss: 0.003036\n","Train Epoch: 79 [46080/60000 (76%)]\tAvg. loss: 0.003000\n","\n","ON TRAINING SET:\n","Average loss: 0.0019, Accuracy: 59999/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.5955, Accuracy: 1082/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 80 [0/60000 (0%)]\tAvg. loss: 0.002563\n","Train Epoch: 80 [15360/60000 (25%)]\tAvg. loss: 0.002568\n","Train Epoch: 80 [30720/60000 (51%)]\tAvg. loss: 0.002460\n","Train Epoch: 80 [46080/60000 (76%)]\tAvg. loss: 0.002430\n","\n","ON TRAINING SET:\n","Average loss: 0.0016, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.6047, Accuracy: 1089/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 81 [0/60000 (0%)]\tAvg. loss: 0.001957\n","Train Epoch: 81 [15360/60000 (25%)]\tAvg. loss: 0.002124\n","Train Epoch: 81 [30720/60000 (51%)]\tAvg. loss: 0.002122\n","Train Epoch: 81 [46080/60000 (76%)]\tAvg. loss: 0.002120\n","\n","ON TRAINING SET:\n","Average loss: 0.0015, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.6062, Accuracy: 1083/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 82 [0/60000 (0%)]\tAvg. loss: 0.001977\n","Train Epoch: 82 [15360/60000 (25%)]\tAvg. loss: 0.002004\n","Train Epoch: 82 [30720/60000 (51%)]\tAvg. loss: 0.002007\n","Train Epoch: 82 [46080/60000 (76%)]\tAvg. loss: 0.002010\n","\n","ON TRAINING SET:\n","Average loss: 0.0014, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.6352, Accuracy: 1080/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 83 [0/60000 (0%)]\tAvg. loss: 0.001779\n","Train Epoch: 83 [15360/60000 (25%)]\tAvg. loss: 0.001938\n","Train Epoch: 83 [30720/60000 (51%)]\tAvg. loss: 0.001926\n","Train Epoch: 83 [46080/60000 (76%)]\tAvg. loss: 0.001936\n","\n","ON TRAINING SET:\n","Average loss: 0.0014, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.6365, Accuracy: 1072/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 84 [0/60000 (0%)]\tAvg. loss: 0.001799\n","Train Epoch: 84 [15360/60000 (25%)]\tAvg. loss: 0.001806\n","Train Epoch: 84 [30720/60000 (51%)]\tAvg. loss: 0.001818\n","Train Epoch: 84 [46080/60000 (76%)]\tAvg. loss: 0.001827\n","\n","ON TRAINING SET:\n","Average loss: 0.0013, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.6612, Accuracy: 1074/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 85 [0/60000 (0%)]\tAvg. loss: 0.001887\n","Train Epoch: 85 [15360/60000 (25%)]\tAvg. loss: 0.001686\n","Train Epoch: 85 [30720/60000 (51%)]\tAvg. loss: 0.001708\n","Train Epoch: 85 [46080/60000 (76%)]\tAvg. loss: 0.001742\n","\n","ON TRAINING SET:\n","Average loss: 0.0012, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.6677, Accuracy: 1065/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 86 [0/60000 (0%)]\tAvg. loss: 0.001736\n","Train Epoch: 86 [15360/60000 (25%)]\tAvg. loss: 0.001728\n","Train Epoch: 86 [30720/60000 (51%)]\tAvg. loss: 0.001700\n","Train Epoch: 86 [46080/60000 (76%)]\tAvg. loss: 0.001696\n","\n","ON TRAINING SET:\n","Average loss: 0.0012, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.6908, Accuracy: 1064/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 87 [0/60000 (0%)]\tAvg. loss: 0.001626\n","Train Epoch: 87 [15360/60000 (25%)]\tAvg. loss: 0.001576\n","Train Epoch: 87 [30720/60000 (51%)]\tAvg. loss: 0.001596\n","Train Epoch: 87 [46080/60000 (76%)]\tAvg. loss: 0.001629\n","\n","ON TRAINING SET:\n","Average loss: 0.0012, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.6800, Accuracy: 1069/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 88 [0/60000 (0%)]\tAvg. loss: 0.001390\n","Train Epoch: 88 [15360/60000 (25%)]\tAvg. loss: 0.001511\n","Train Epoch: 88 [30720/60000 (51%)]\tAvg. loss: 0.001552\n","Train Epoch: 88 [46080/60000 (76%)]\tAvg. loss: 0.001567\n","\n","ON TRAINING SET:\n","Average loss: 0.0011, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.7081, Accuracy: 1075/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 89 [0/60000 (0%)]\tAvg. loss: 0.001487\n","Train Epoch: 89 [15360/60000 (25%)]\tAvg. loss: 0.001488\n","Train Epoch: 89 [30720/60000 (51%)]\tAvg. loss: 0.001527\n","Train Epoch: 89 [46080/60000 (76%)]\tAvg. loss: 0.001528\n","\n","ON TRAINING SET:\n","Average loss: 0.0011, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.7112, Accuracy: 1091/10000 (11%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 90 [0/60000 (0%)]\tAvg. loss: 0.001447\n","Train Epoch: 90 [15360/60000 (25%)]\tAvg. loss: 0.001454\n","Train Epoch: 90 [30720/60000 (51%)]\tAvg. loss: 0.001456\n","Train Epoch: 90 [46080/60000 (76%)]\tAvg. loss: 0.001463\n","\n","ON TRAINING SET:\n","Average loss: 0.0011, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 6.7395, Accuracy: 1071/10000 (11%)\n","\n","\n","\n"]}],"source":["for epoch in range(1, nrepochs + 1):\n","\n","    # Training\n","    print(\"TRAINING...\")\n","    train_epoch(\n","        model, device, train_loader, lossfn, optimizer, epoch, print_every_nep=15, inner_scheduler=None, quiet=False,\n","    )\n","\n","    # Tweaks for the Lookahead optimizer (before testing)\n","    if isinstance(optimizer, Lookahead):\n","        optimizer._backup_and_load_cache()  # I.e.: use slow weights for testing -->\n","\n","    # Testing: on training and testing set\n","    print(\"\\nON TRAINING SET:\")\n","    _ = test(model, device, test_on_train_loader, lossfn, quiet=False)\n","    print(\"\\nON TEST SET:\")\n","    _ = test(model, device, test_loader, lossfn, quiet=False)\n","    print(\"\\n\\n\")\n","\n","    # Tweaks for the Lookahead optimizer (after testing)\n","    if isinstance(optimizer, Lookahead):\n","        optimizer._clear_and_load_backup()  # <-- I.e.: use slow weights for testing\n","    \n","    # Scheduling step (outer)\n","    scheduler.step()"]},{"cell_type":"markdown","metadata":{},"source":["#### Final comment\n","\n","Upon completion of the task, we can notice that the *test-set accuracy* of the trained *neural network* is $\\approx 10\\%$. This is indeed the result we expect from theory, as the classification task at hand required to separate (exactly) the dataset in ($10$ in total) classes with a *one-versus-all ratio* of $\\frac{1}{10}$.  \n","\n","The result is in sharp contrast with the expected (and, actually, verified - just not in the notebook!) behaviour -- in terms of *test-set accuracy* -- of the same classification task (which is equivalent to the *unpermuted dataset*) with deterministically-permuted labels, which is in fact $\\approx 0$."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"softmax_mnist.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('RDDL': conda)","name":"python388jvsc74a57bd0eb8633c4d4e251251708d3c7ece77ee33d393b5bf4628cd3b0e51f052595f5d6"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}