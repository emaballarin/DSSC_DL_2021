{"cells":[{"cell_type":"markdown","metadata":{"id":"WZnV3RNgU0Mi"},"source":["# Deep Learning Homework \\#04 (*normal MNIST*)\n","### Deep Learning Course $\\in$ DSSC @ UniTS (Spring 2021)  \n","\n","#### Submitted by [Emanuele Ballarin](mailto:emanuele@ballarin.cc)  "]},{"cell_type":"markdown","metadata":{},"source":["### Preliminaries:"]},{"cell_type":"markdown","metadata":{},"source":["#### Imports:\n","\n","We start off by importing all the libraries, modules, classes and functions we are going to use *today*..."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Ra6EQvDLUxqG"},"outputs":[],"source":["# Type hints\n","from torch import Tensor\n","\n","# Just to force-load MKL (if available)\n","import numpy as np\n","\n","# Mathematical functions\n","from math import sqrt as msqrt\n","\n","# Neural networks and friends\n","import torch as th\n","from torch.nn import Sequential, BatchNorm1d, Linear, LogSoftmax, Dropout\n","import torch.nn.functional as F\n","\n","# Optimization and scheduling\n","from torch.optim.lr_scheduler import StepLR, MultiStepLR\n","\n","# Bespoke Modules / Functions / Optimizers\n","from ebtorch.logging import AverageMeter\n","from ebtorch.nn import Mish, mishlayer_init\n","from ebtorch.optim import Lookahead\n","from madgrad.madgrad import MADGRAD as MadGrad\n","\n","# Model summarization\n","from torchinfo import summary\n","\n","# Dataset handling for PyTorch\n","import os\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import MNIST\n","from torchvision.transforms import ToTensor, Normalize, Compose, Lambda"]},{"cell_type":"markdown","metadata":{},"source":["### Request 1:\n","\n","Now that you have all the tools to train an MLP with high performance on MNIST, try reaching 0-loss (or 100% accuracy) on the training data (with a small epsilon, e.g. 99.99% training performance -- don't worry if you overfit!). The implementation is completely up to you. You just need to keep it an MLP without using fancy layers (e.g., keep the Linear layers, don't use `Conv1d` or something like this, don't use attention). You are free to use any LR scheduler or optimizer, any one of batchnorm/groupnorm, regularization methods... If you use something we haven't seen during lectures, please motivate your choice and explain (as briefly as possible) how it works."]},{"cell_type":"markdown","metadata":{},"source":["#### Comment:\n","\n","Nothing fancy here... just some DataSets/Loaders."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ANKugUNmUyFo"},"outputs":[],"source":["def spawn_mnist_loaders(\n","    data_root=\"datasets/\",\n","    batch_size_train=256,\n","    batch_size_test=512,\n","    cuda_accel=False,\n","    **kwargs\n","):\n","\n","    os.makedirs(data_root, exist_ok=True)\n","\n","    transforms = Compose(\n","        [\n","            ToTensor(),\n","            Normalize((0.1307,), (0.3081,)),  # usual normalization constants for MNIST\n","            Lambda(lambda x: th.flatten(x)),\n","        ]\n","    )\n","\n","    trainset = MNIST(data_root, train=True, transform=transforms, download=True)\n","    testset = MNIST(data_root, train=False, transform=transforms, download=True)\n","\n","    cuda_args = {}\n","    if cuda_accel:\n","        cuda_args = {\"num_workers\": 1, \"pin_memory\": True}\n","\n","    trainloader = DataLoader(\n","        trainset, batch_size=batch_size_train, shuffle=True, **cuda_args\n","    )\n","    testloader = DataLoader(\n","        testset, batch_size=batch_size_test, shuffle=False, **cuda_args\n","    )\n","    tontrloader = DataLoader(   # tontr == test on train\n","        trainset, batch_size=batch_size_test, shuffle=False, **cuda_args\n","    )\n","\n","    return trainloader, testloader, tontrloader"]},{"cell_type":"markdown","metadata":{},"source":["#### Comment:\n","\n","Nothing fancy here, too. A standard training/testing loop."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"0xgs3EZBUyJ4"},"outputs":[],"source":["train_acc_avgmeter = AverageMeter(\"Training Loss\")\n","\n","def train_epoch(\n","    model, device, train_loader, loss_fn, optimizer, epoch, print_every_nep, inner_scheduler=None, quiet=False,\n","):\n","    train_acc_avgmeter.reset()\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = loss_fn(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if inner_scheduler is not None:\n","            inner_scheduler.step()\n","        \n","        train_acc_avgmeter.update(loss.item())\n","\n","        if not quiet and batch_idx % print_every_nep == 0:\n","            print(\n","                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tAvg. loss: {:.6f}\".format(\n","                    epoch,\n","                    batch_idx * len(data),\n","                    len(train_loader.dataset),\n","                    100.0 * batch_idx / len(train_loader),\n","                    train_acc_avgmeter.avg\n","                )\n","            )\n","\n","\n","def test(model, device, test_loader, loss_fn, quiet=False):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with th.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += loss_fn(\n","                output, target, reduction=\"sum\"\n","            ).item()  # sum up batch loss\n","            pred = output.argmax(\n","                dim=1, keepdim=True\n","            )  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    ltlds = len(test_loader.dataset)\n","\n","    test_loss /= ltlds\n","    \n","    if not quiet:\n","        print(\n","            \"Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n","                test_loss,\n","                correct,\n","                ltlds,\n","                100.0 * correct / ltlds,\n","            )\n","        )\n","    \n","    return test_loss, correct / ltlds"]},{"cell_type":"markdown","metadata":{},"source":["#### Comment:\n","\n","The *usual* specification of the device. Please, notece that the `MADGRAD` optimizer (used later on) requires a working *CUDA-capable* GPU and may not work otherwise. You may be interested in using `RAdam` instead."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"i2aclwpGUyOP"},"outputs":[],"source":["device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Comment:\n","\n","The choice of a $512$-element *batch size* is unusual. Here, the rationale may be summarized by noticing the peculiar relationship among *large batch size* (both as a regularizer and as a specific training device) and the deeper nature of learned features.  \n","\n","Small batch sizes ($<128$, for the dataset of interest), on the one hand:\n","\n","- Carry a considerable amount of *data sampling noise*, that smoothes the loss landcape and ease optimization with non-accelerated methods (e.g. SGD);\n","\n","- Carry a higher *signal-to-data* ratio, due to limited feature- and gradient- competition inside the batch;\n","\n","- Usually produce better generalization on both *testing* and *unseen* data. This may also be true for *synthetic whole-manifold* and *noise-corrupted* data;\n","\n","- Produce more unstable *training-set* loss and accuracy results;\n","\n","- Require longer or more-otherwise-regularized training for the same level of *training set accuuracy*;\n","\n","- Put the main training focus on learning *useful data-point features*, as opposed to pure decision boundaries.\n","\n","\n","On the other hand, larger batch sizes ($>128$, for the dataset of interest):\n","\n","- Carry a reduced amount of *data sampling noise*, making easier for an optimizer to find a single direction that minimizes (even though of a small amount) the whole-dataset loss. This regime may be particularly appealing for *accelerated* descent methods;\n","\n","- May carry a reduced *signal-to-data* ratio, also due to gradient competition inside the batch, thus requiring more finely-tuned *learning rate* and *loss function* choices. Or specifically-crafted robust-to-initialization optimizers;\n","\n","- May easily overfit the *training set* without significant generalization, especially without additional regularization devices;\n","\n","- Usually produce an approximation of *constantly-decreasing test-set loss* seen in *full-dataset GD*;\n","\n","- May speed-up training, with no guarantee - however - on how low will be the loss at convergence;\n","\n","- Put the main training focus on differences among data-points and decision boundaries, sometimes regardless of the most reasonable similarity-based representation.\n","\n","\n","As we can see, for our specific goal, the latter may be a favourable regime, provided that optimization is carried out thoughtfully."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"PtJNzN33cX_e"},"outputs":[],"source":["# Hyperparameters & co.\n","\n","minibatch_size_train: int = 512 # I know it's high; I just want a \"little\" more stability\n","minibatch_size_test: int = 512\n","\n","nrepochs = 12   # This number os tuned to the minimum necessary for stable convergence to the result\n","\n","lossfn = F.nll_loss"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"N8YTl9NTUySq"},"outputs":[],"source":["train_loader, test_loader, test_on_train_loader = spawn_mnist_loaders(\n","    batch_size_train=minibatch_size_train,\n","    batch_size_test=minibatch_size_test,\n","    cuda_accel=bool(device == \"cuda\"),\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### Comment:\n","\n","The network is a good compromise between number of parameters, training performance, and a still *easily-steerable* parameter space.  \n","\n","In particular, though only $3$-layered, the MLP exploits *hyperfeaturization* (the choice to include hidden layers larger than input the size; still not close to $(\\# \\text{inputs} \\times \\# \\text{outputs})$) and presents in any case a relatively large (for the problem) number of parameters.  \n","\n","**About the optimizer**:\n","\n","- `Lookahead` optimizer. A fresh approach to *inner-loop optimization* applied to *neural network optimizers*. The optimizer copies the weights of the network (called *slow weights*), updates $k$ times such copy (called *fast weights*), according to the loss minimization criterion, with an auxiliary optimizer, and finally updates *once* the *slow weights* with a step in the direction of the sum of *fast weights* updates. Such optimization schedule is very robust to noise and to *ruggedness-induced jitter*.  \n","\n","- The `MADGRAD` optimizer, used (with maybe overkill attitude) as the *inner-loop optimizer* of the above, is a new, *doubly-averaged* adaptive (as in *AdaGrad*), *explicitly-momentumized* (as in *SGD with momentum*) optimizer by Facebook AI Research. It may be seen (with much approximation) as an attempt to combine *Adam-like* convergence speed and adaptiveness (successfully!), *SGD-like* generalizability (requires care!), and a tunable *momentum* (it's part of te design)."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["model = Sequential(\n","    # -> Input is here <-\n","\n","    # POST-INPUT BLOCK:\n","    Linear(in_features=28*28, out_features=1500, bias=True),    # Hyperfeaturize ~2*input\n","    Mish(),\n","\n","    # HIDDEN BLOCK:\n","    BatchNorm1d(num_features=1500, affine=True),\n","    Linear(in_features=1500, out_features=500, bias=True),      # Compress ~0.75*input\n","    Mish(),\n","\n","    # PRE-OUTPUT BLOCK:\n","    BatchNorm1d(num_features=500, affine=True),\n","    Linear(in_features=500, out_features=10, bias=True),        # To output\n","    LogSoftmax(dim=1)\n","\n","    # -> Output is here <-\n","        ).to(device)\n","\n","if device == \"cpu\":\n","    raise RuntimeError(\"The MADGRAD optimizer won't work without a GPU. You may want to use RAdam instead! ;)\")\n","\n","base_optimizer = MadGrad(model.parameters(), lr=0.00017)\n","optimizer      = Lookahead(base_optimizer, la_steps=4)\n","scheduler      = MultiStepLR(optimizer, milestones=[10, 11], gamma=0.4) # Just to dampen jitter in case perfect accuracy is reached"]},{"cell_type":"markdown","metadata":{},"source":["#### Comment:\n","\n","A *weight-initialization* scheme that tries to evenly spread weights in such way to fully exploit the three different *regimes* offered by the `Mish` activation function (asymptotically zero, negative, asymptotically linear) with a *fanin/fanout*-inspired approach (as the one in *Xavier* or *Kaiming* initialization)."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Initialize weights and biases in the proper way ;)\n","for layr in model:\n","    mishlayer_init(layr)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":"=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─Linear: 1-1                            1,177,500\n├─Mish: 1-2                              --\n├─BatchNorm1d: 1-3                       3,000\n├─Linear: 1-4                            750,500\n├─Mish: 1-5                              --\n├─BatchNorm1d: 1-6                       1,000\n├─Linear: 1-7                            5,010\n├─LogSoftmax: 1-8                        --\n=================================================================\nTotal params: 1,937,010\nTrainable params: 1,937,010\nNon-trainable params: 0\n================================================================="},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["summary(model)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TRAINING...\n","Train Epoch: 1 [0/60000 (0%)]\tAvg. loss: 3.071268\n","Train Epoch: 1 [7680/60000 (13%)]\tAvg. loss: 1.157837\n","Train Epoch: 1 [15360/60000 (25%)]\tAvg. loss: 0.749150\n","Train Epoch: 1 [23040/60000 (38%)]\tAvg. loss: 0.577069\n","Train Epoch: 1 [30720/60000 (51%)]\tAvg. loss: 0.477688\n","Train Epoch: 1 [38400/60000 (64%)]\tAvg. loss: 0.415793\n","Train Epoch: 1 [46080/60000 (76%)]\tAvg. loss: 0.370885\n","Train Epoch: 1 [53760/60000 (89%)]\tAvg. loss: 0.336213\n","\n","ON TRAINING SET:\n","Average loss: 0.0841, Accuracy: 58680/60000 (98%)\n","\n","ON TEST SET:\n","Average loss: 0.1060, Accuracy: 9687/10000 (97%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 2 [0/60000 (0%)]\tAvg. loss: 0.057759\n","Train Epoch: 2 [7680/60000 (13%)]\tAvg. loss: 0.083856\n","Train Epoch: 2 [15360/60000 (25%)]\tAvg. loss: 0.083331\n","Train Epoch: 2 [23040/60000 (38%)]\tAvg. loss: 0.080269\n","Train Epoch: 2 [30720/60000 (51%)]\tAvg. loss: 0.077920\n","Train Epoch: 2 [38400/60000 (64%)]\tAvg. loss: 0.076626\n","Train Epoch: 2 [46080/60000 (76%)]\tAvg. loss: 0.073931\n","Train Epoch: 2 [53760/60000 (89%)]\tAvg. loss: 0.074531\n","\n","ON TRAINING SET:\n","Average loss: 0.0367, Accuracy: 59567/60000 (99%)\n","\n","ON TEST SET:\n","Average loss: 0.0756, Accuracy: 9774/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 3 [0/60000 (0%)]\tAvg. loss: 0.037090\n","Train Epoch: 3 [7680/60000 (13%)]\tAvg. loss: 0.036789\n","Train Epoch: 3 [15360/60000 (25%)]\tAvg. loss: 0.038432\n","Train Epoch: 3 [23040/60000 (38%)]\tAvg. loss: 0.037233\n","Train Epoch: 3 [30720/60000 (51%)]\tAvg. loss: 0.037669\n","Train Epoch: 3 [38400/60000 (64%)]\tAvg. loss: 0.037018\n","Train Epoch: 3 [46080/60000 (76%)]\tAvg. loss: 0.037348\n","Train Epoch: 3 [53760/60000 (89%)]\tAvg. loss: 0.037115\n","\n","ON TRAINING SET:\n","Average loss: 0.0179, Accuracy: 59859/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 0.0675, Accuracy: 9783/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 4 [0/60000 (0%)]\tAvg. loss: 0.015619\n","Train Epoch: 4 [7680/60000 (13%)]\tAvg. loss: 0.019979\n","Train Epoch: 4 [15360/60000 (25%)]\tAvg. loss: 0.019345\n","Train Epoch: 4 [23040/60000 (38%)]\tAvg. loss: 0.018831\n","Train Epoch: 4 [30720/60000 (51%)]\tAvg. loss: 0.018799\n","Train Epoch: 4 [38400/60000 (64%)]\tAvg. loss: 0.018671\n","Train Epoch: 4 [46080/60000 (76%)]\tAvg. loss: 0.018324\n","Train Epoch: 4 [53760/60000 (89%)]\tAvg. loss: 0.018509\n","\n","ON TRAINING SET:\n","Average loss: 0.0091, Accuracy: 59961/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 0.0610, Accuracy: 9807/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 5 [0/60000 (0%)]\tAvg. loss: 0.006782\n","Train Epoch: 5 [7680/60000 (13%)]\tAvg. loss: 0.010216\n","Train Epoch: 5 [15360/60000 (25%)]\tAvg. loss: 0.009870\n","Train Epoch: 5 [23040/60000 (38%)]\tAvg. loss: 0.009680\n","Train Epoch: 5 [30720/60000 (51%)]\tAvg. loss: 0.009483\n","Train Epoch: 5 [38400/60000 (64%)]\tAvg. loss: 0.009527\n","Train Epoch: 5 [46080/60000 (76%)]\tAvg. loss: 0.009502\n","Train Epoch: 5 [53760/60000 (89%)]\tAvg. loss: 0.009624\n","\n","ON TRAINING SET:\n","Average loss: 0.0058, Accuracy: 59965/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 0.0612, Accuracy: 9795/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 6 [0/60000 (0%)]\tAvg. loss: 0.009247\n","Train Epoch: 6 [7680/60000 (13%)]\tAvg. loss: 0.007023\n","Train Epoch: 6 [15360/60000 (25%)]\tAvg. loss: 0.006519\n","Train Epoch: 6 [23040/60000 (38%)]\tAvg. loss: 0.006048\n","Train Epoch: 6 [30720/60000 (51%)]\tAvg. loss: 0.005817\n","Train Epoch: 6 [38400/60000 (64%)]\tAvg. loss: 0.005558\n","Train Epoch: 6 [46080/60000 (76%)]\tAvg. loss: 0.005342\n","Train Epoch: 6 [53760/60000 (89%)]\tAvg. loss: 0.005315\n","\n","ON TRAINING SET:\n","Average loss: 0.0029, Accuracy: 59989/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 0.0588, Accuracy: 9821/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 7 [0/60000 (0%)]\tAvg. loss: 0.003512\n","Train Epoch: 7 [7680/60000 (13%)]\tAvg. loss: 0.003473\n","Train Epoch: 7 [15360/60000 (25%)]\tAvg. loss: 0.003067\n","Train Epoch: 7 [23040/60000 (38%)]\tAvg. loss: 0.002893\n","Train Epoch: 7 [30720/60000 (51%)]\tAvg. loss: 0.002883\n","Train Epoch: 7 [38400/60000 (64%)]\tAvg. loss: 0.002916\n","Train Epoch: 7 [46080/60000 (76%)]\tAvg. loss: 0.002854\n","Train Epoch: 7 [53760/60000 (89%)]\tAvg. loss: 0.002847\n","\n","ON TRAINING SET:\n","Average loss: 0.0019, Accuracy: 59996/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 0.0569, Accuracy: 9832/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 8 [0/60000 (0%)]\tAvg. loss: 0.001678\n","Train Epoch: 8 [7680/60000 (13%)]\tAvg. loss: 0.002503\n","Train Epoch: 8 [15360/60000 (25%)]\tAvg. loss: 0.002204\n","Train Epoch: 8 [23040/60000 (38%)]\tAvg. loss: 0.002071\n","Train Epoch: 8 [30720/60000 (51%)]\tAvg. loss: 0.001970\n","Train Epoch: 8 [38400/60000 (64%)]\tAvg. loss: 0.001926\n","Train Epoch: 8 [46080/60000 (76%)]\tAvg. loss: 0.001881\n","Train Epoch: 8 [53760/60000 (89%)]\tAvg. loss: 0.001814\n","\n","ON TRAINING SET:\n","Average loss: 0.0010, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 0.0571, Accuracy: 9833/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 9 [0/60000 (0%)]\tAvg. loss: 0.001148\n","Train Epoch: 9 [7680/60000 (13%)]\tAvg. loss: 0.001262\n","Train Epoch: 9 [15360/60000 (25%)]\tAvg. loss: 0.001205\n","Train Epoch: 9 [23040/60000 (38%)]\tAvg. loss: 0.001221\n","Train Epoch: 9 [30720/60000 (51%)]\tAvg. loss: 0.001180\n","Train Epoch: 9 [38400/60000 (64%)]\tAvg. loss: 0.001175\n","Train Epoch: 9 [46080/60000 (76%)]\tAvg. loss: 0.001178\n","Train Epoch: 9 [53760/60000 (89%)]\tAvg. loss: 0.001174\n","\n","ON TRAINING SET:\n","Average loss: 0.0008, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 0.0572, Accuracy: 9831/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 10 [0/60000 (0%)]\tAvg. loss: 0.000891\n","Train Epoch: 10 [7680/60000 (13%)]\tAvg. loss: 0.000865\n","Train Epoch: 10 [15360/60000 (25%)]\tAvg. loss: 0.000876\n","Train Epoch: 10 [23040/60000 (38%)]\tAvg. loss: 0.000886\n","Train Epoch: 10 [30720/60000 (51%)]\tAvg. loss: 0.000905\n","Train Epoch: 10 [38400/60000 (64%)]\tAvg. loss: 0.000897\n","Train Epoch: 10 [46080/60000 (76%)]\tAvg. loss: 0.000903\n","Train Epoch: 10 [53760/60000 (89%)]\tAvg. loss: 0.000896\n","\n","ON TRAINING SET:\n","Average loss: 0.0006, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 0.0567, Accuracy: 9841/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 11 [0/60000 (0%)]\tAvg. loss: 0.000593\n","Train Epoch: 11 [7680/60000 (13%)]\tAvg. loss: 0.000731\n","Train Epoch: 11 [15360/60000 (25%)]\tAvg. loss: 0.000726\n","Train Epoch: 11 [23040/60000 (38%)]\tAvg. loss: 0.000714\n","Train Epoch: 11 [30720/60000 (51%)]\tAvg. loss: 0.000712\n","Train Epoch: 11 [38400/60000 (64%)]\tAvg. loss: 0.000709\n","Train Epoch: 11 [46080/60000 (76%)]\tAvg. loss: 0.000702\n","Train Epoch: 11 [53760/60000 (89%)]\tAvg. loss: 0.000698\n","\n","ON TRAINING SET:\n","Average loss: 0.0005, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 0.0570, Accuracy: 9835/10000 (98%)\n","\n","\n","\n","TRAINING...\n","Train Epoch: 12 [0/60000 (0%)]\tAvg. loss: 0.000565\n","Train Epoch: 12 [7680/60000 (13%)]\tAvg. loss: 0.000646\n","Train Epoch: 12 [15360/60000 (25%)]\tAvg. loss: 0.000630\n","Train Epoch: 12 [23040/60000 (38%)]\tAvg. loss: 0.000638\n","Train Epoch: 12 [30720/60000 (51%)]\tAvg. loss: 0.000636\n","Train Epoch: 12 [38400/60000 (64%)]\tAvg. loss: 0.000638\n","Train Epoch: 12 [46080/60000 (76%)]\tAvg. loss: 0.000644\n","Train Epoch: 12 [53760/60000 (89%)]\tAvg. loss: 0.000648\n","\n","ON TRAINING SET:\n","Average loss: 0.0005, Accuracy: 60000/60000 (100%)\n","\n","ON TEST SET:\n","Average loss: 0.0569, Accuracy: 9838/10000 (98%)\n","\n","\n","\n"]}],"source":["for epoch in range(1, nrepochs + 1):\n","\n","    # Training\n","    print(\"TRAINING...\")\n","    train_epoch(\n","        model, device, train_loader, lossfn, optimizer, epoch, print_every_nep=15, inner_scheduler=None, quiet=False,\n","    )\n","\n","    # Tweaks for the Lookahead optimizer (before testing)\n","    if isinstance(optimizer, Lookahead):\n","        optimizer._backup_and_load_cache()  # I.e.: use slow weights for testing -->\n","\n","    # Testing: on training and testing set\n","    print(\"\\nON TRAINING SET:\")\n","    _ = test(model, device, test_on_train_loader, lossfn, quiet=False)\n","    print(\"\\nON TEST SET:\")\n","    _ = test(model, device, test_loader, lossfn, quiet=False)\n","    print(\"\\n\\n\")\n","\n","    # Tweaks for the Lookahead optimizer (after testing)\n","    if isinstance(optimizer, Lookahead):\n","        optimizer._clear_and_load_backup()  # <-- I.e.: use slow weights for testing\n","    \n","    # Scheduling step (outer)\n","    scheduler.step()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"softmax_mnist.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('RDDL': conda)","name":"python388jvsc74a57bd0eb8633c4d4e251251708d3c7ece77ee33d393b5bf4628cd3b0e51f052595f5d6"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}