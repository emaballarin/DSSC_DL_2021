{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Learning course - LAB 4\n",
    "\n",
    "## A tour of the optimizers in PyTorch\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recap from previous Lab\n",
    "\n",
    "* We experimented with building a Multilayer Perceptron (MLP) trained on the MNIST dataset using _vanilla_ Stochastic Gradient Descent (SGD) and constructing a training loop that lets us track loss and accuracy as training goes on\n",
    "* We saw how we can analyze parameters and gradients of this MLP as training is operated\n",
    "* We explored how to add regularization to our network and loss function to increase generalization or speed up the training\n",
    "\n",
    "### Agenda for today\n",
    "\n",
    "* Today we will be taking a quick tour of the `torch.optim` library, having a look at some optimizers which are more advanced than vanilla SGD\n",
    "* in addition to that, we will be exploring how to toggle the hyperparameters (chiefly, the learning rate) of the optimizer as the training is operated"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scripts.architectures import MLP # I have pasted the code for the MLP with regularization in this script, no need to redefine it\n",
    "from scripts.train_utils import AverageMeter, accuracy\n",
    "from scripts import mnist"
   ]
  },
  {
   "source": [
    "### Exploring optimizers in PyTorch\n",
    "\n",
    "PT optimizers can be found in the `torch.optim` library.\n",
    "\n",
    "We'll take a look at some of those, namely:\n",
    "\n",
    "* SGD with momentum\n",
    "* RMSProp\n",
    "* Adam\n",
    "\n",
    "If you're a fan of optimizers, you can yourself have a look at the plethora of optimizers in the `optim` library on the [official docs](https://pytorch.org/docs/stable/optim.html).\n",
    "\n",
    "#### SGD w/ momentum\n",
    "\n",
    "Adding a momentum term helps SGD optimize faster in some situations where the optimum is situated in _valleys_ which are way steeper along sime directions w.r.t. others.\n",
    "\n",
    "![](img/sgd_momentum.jpg)\n",
    "\n",
    "*Image from Deep Learning book (Goodfellow et al.) - chapter 8.3.2*\n",
    "\n",
    "The gradient is updated via the following quantity $M$:\n",
    "\n",
    "$\\mathbf{M} \\leftarrow \\mu\\cdot\\mathbf{M} + \\text{lr} \\cdot \\mathbf{G}$\n",
    "\n",
    "$\\mathbf{\\Theta} \\leftarrow \\mathbf{\\Theta} - \\mathbf{M}$\n",
    "\n",
    "where $\\mathbf{G}$ is the gradient and $\\mu$ is the momentum term (usually picked $\\rightarrow 1$).\n",
    "\n",
    "Actually, SGD with momentum is part of vanilla SGD in PT. Indeed, one of its arguments is `momentum`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .1\n",
    "wd = 5e-4\n",
    "momentum = .9\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model = MLP()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=momentum)"
   ]
  },
  {
   "source": [
    "Let us also recover the training and testing routines we defined during the last lab (without the trajectory):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance): # note: I've added a generic performance to replace accuracy\n",
    "    for X, y in dataloader:\n",
    "        # 1. reset the gradients previously accumulated by the optimizer\n",
    "        #    this will avoid re-using gradients from previous loops\n",
    "        optimizer.zero_grad() \n",
    "        # 2. get the predictions from the current state of the model\n",
    "        #    this is the forward pass\n",
    "        y_hat = model(X)\n",
    "        # 3. calculate the loss on the current mini-batch\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        # 4. execute the backward pass given the current loss\n",
    "        loss.backward()\n",
    "        # 5. update the value of the params\n",
    "        optimizer.step()\n",
    "        # 6. calculate the accuracy for this mini-batch\n",
    "        acc = performance(y_hat, y)\n",
    "        # 7. update the loss and accuracy AverageMeter\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy):\n",
    "\n",
    "    # create the folder for the checkpoints (if it's not None)\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # epoch loop\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        # produce checkpoint dictionary -- but only if the name and folder of the checkpoint are not None\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg\n",
    "\n",
    "def test_model(model, dataloader, performance=accuracy, loss_fn=None):\n",
    "    # create an AverageMeter for the loss if passed\n",
    "    if loss_fn is not None:\n",
    "        loss_meter = AverageMeter()\n",
    "    \n",
    "    performance_meter = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y_hat = model(X)\n",
    "            loss = loss_fn(y_hat, y) if loss_fn is not None else None\n",
    "            acc = performance(y_hat, y)\n",
    "            if loss_fn is not None:\n",
    "                loss_meter.update(loss.item(), X.shape[0])\n",
    "            performance_meter.update(acc, X.shape[0])\n",
    "    # get final performances\n",
    "    fin_loss = loss_meter.sum if loss_fn is not None else None\n",
    "    fin_perf = performance_meter.avg\n",
    "    print(f\"TESTING - loss {fin_loss if fin_loss is not None else '--'} - performance {fin_perf}\")\n",
    "    return fin_loss, fin_perf"
   ]
  },
  {
   "source": [
    "and recover the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, _, _ = mnist.get_data()"
   ]
  },
  {
   "source": [
    "Let's train the network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, trainloader, loss_fn, optimizer, num_epochs)"
   ]
  },
  {
   "source": [
    "by adding the momentum term, we already saw a small increase in training accuracy. Let's test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, testloader)"
   ]
  },
  {
   "source": [
    "#### RMSProp and the LR sensitivity \"dilemma\"\n",
    "\n",
    "With RMSProp we want to tackle a problem with SGD/SGD+momentum, which is related to the fact that, with SGD, there seems to be a deal of _sensitivity_ towards some specific _directions_ (read, parameters, since each parameter of the model represent a dimension in the optimization space).\n",
    "\n",
    "RMSProp tries to tackle this issue by introducing an _adaptive rule_ for updating the learning rate parameter-wise in each step. \n",
    "In particular:\n",
    "* it keeps track of the _history_ of the squared gradient via an exponentially decaying running average: \n",
    "  $\\mathbf{R} \\leftarrow \\rho\\mathbf{R} + (1-\\rho) \\mathbf{G}^2$\n",
    "  * (the _decay_ is controlled by a hyperparameter $\\rho \\in (0,1)$, usually 0.9)\n",
    "* The parameter update is \n",
    "  * directly proportional to the learning rate\n",
    "  * directly proportional to the gradient for this step\n",
    "  * inversely proportional to the gradient average\n",
    "    * i.e., the direct effect of the gradient is _mitigated_ by dividing it with the accumulated average gradient\n",
    "\n",
    "The formula for the update is:\n",
    "\n",
    "$ \\Theta \\leftarrow \\Theta + \\frac{\\text{lr}}{\\sqrt{\\epsilon + \\mathbf{R}}} \\odot \\mathbf{G}$\n",
    "\n",
    "where:\n",
    "* $\\epsilon$ is a small constant for numerical stability\n",
    "* $\\mathbf{R}$ is the squared gradient running averate (which depends upon $\\rho$)\n",
    "* $\\mathbf{G}$ is the gradient for the current step\n",
    "* $\\odot$ indicates the Hadamard matrix product (el.-by-el. product)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP() # always remember to reinstantiate the net between tries\n",
    "rmsprop = torch.optim.RMSprop(model.parameters()) # let's use the default hyperparams (lr=.01, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, trainloader, loss_fn, rmsprop, num_epochs)\n",
    "test_model(model, testloader)"
   ]
  },
  {
   "source": [
    "#### ADAM\n",
    "\n",
    "Adam is an extension of RMSProp where we try to implement momentum-like mechanics as well.\n",
    "\n",
    "Instead of adding one single momentum term, we add two of them:\n",
    "\n",
    "$\\mathbf{M} \\leftarrow (\\beta_1 \\mathbf{M} + (1 - \\beta_1)\\mathbf{G})~/~(1-\\beta_1^t)$\n",
    "\n",
    "$\\mathbf{V} \\leftarrow (\\beta_2 \\mathbf{V} + (1 - \\beta_2)\\mathbf{G}^2)~/~(1-\\beta_2^t)$\n",
    "\n",
    "where $t$ is the training iteration.\n",
    "\n",
    "The two terms are running averages (with a so called *bias correction* at the denominator) of the gradient and its square respectively.\n",
    "\n",
    "These terms are then incorporated into the parameters update formula:\n",
    "\n",
    "$\\mathbf{\\Theta} \\leftarrow \\mathbf{\\Theta} + \\text{lr}\\frac{\\mathbf{M}}{\\sqrt{\\mathbf{V}} + \\epsilon} \\cdot \\mathbf{G}$\n",
    "\n",
    "Notice the similarities between Adam and RMSProp."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "adam = torch.optim.Adam(model.parameters()) # we keep the default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, trainloader, loss_fn, adam, num_epochs)\n",
    "test_model(model, testloader)"
   ]
  },
  {
   "source": [
    "The literature is loaded with SGD variants for optimization: Adagrad, AdaMax, Nadam, AdamW... You can use one of them of your own choice in your exercises, (provided you can explain the concept behind it during the exam).\n",
    "\n",
    "#### Additional reading\n",
    "\n",
    "If you're interested in SGD variants, you may check out [this blog post](https://ruder.io/optimizing-gradient-descent/index.html) which, in my opinion, does a good job in summarising and presenting recent work in the field."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Modifying the optimizer hyperparameters\n",
    "\n",
    "One thing we might be interested in doing is to modify the hyperparameters of our optimizer mid-training.\n",
    "\n",
    "The parameters of the optimizer are contained:\n",
    "* in its `state_dict`\n",
    "* under the `param_groups`\n",
    "\n",
    "we will see how to work with the latter.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\n1\n"
     ]
    }
   ],
   "source": [
    "print(type(optimizer.param_groups))\n",
    "print(len(optimizer.param_groups))"
   ]
  },
  {
   "source": [
    "The `param_groups` represent groups of parameters for which given conditions apply.\n",
    "\n",
    "Here we have only one group, corresponding to the params of our MLP network.\n",
    "\n",
    "Let us see how this group is composed:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'dict'>\ndict_keys(['params', 'lr', 'momentum', 'dampening', 'weight_decay', 'nesterov'])\n"
     ]
    }
   ],
   "source": [
    "print(type(optimizer.param_groups[0]))\n",
    "print(optimizer.param_groups[0].keys())"
   ]
  },
  {
   "source": [
    "To the surprise of no-one, the parameters of the MLP are stored under the `params` key.\n",
    "\n",
    "The other keys represent the _conditions_ that apply to these parameter group.\n",
    "\n",
    "Toggling one of these hyperparameters can be done in that way."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0][\"momentum\"] = 0.8 # -> from now on, the momentum will be decreased a little bit"
   ]
  },
  {
   "source": [
    "even if it's better to be general: if we're willing to do a global update for that optimizer, we better do it for all groups."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pg in optimizer.param_groups:\n",
    "    pg[\"momentum\"] = .8"
   ]
  },
  {
   "source": [
    "Let us suppose we wish to use a different momentum or learning rate for each layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_diff = torch.optim.SGD(\n",
    "    [\n",
    "        {\"params\": model.layers[:6].parameters()},\n",
    "        {\"params\": model.layers[6:].parameters()}\n",
    "    ],\n",
    "    lr=.1, weight_decay=5e-4, momentum=.9\n",
    ")"
   ]
  },
  {
   "source": [
    "We have split the params of our MLP in two groups (the first 6 layers and the remaining ones). Let's check that we have >1 `param_group`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "len(optimizer_diff.param_groups)"
   ]
  },
  {
   "source": [
    "Now, suppose we might want to have a different weight decay in the second group: we only need to toggle it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_diff.param_groups[1][\"weight_decay\"] = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lr \t 0.1\nmomentum \t 0.9\ndampening \t 0\nweight_decay \t 0.0005\nnesterov \t False\nlr \t 0.1\nmomentum \t 0.9\ndampening \t 0\nweight_decay \t 0.001\nnesterov \t False\n"
     ]
    }
   ],
   "source": [
    "_ = [[print(hyp, \"\\t\", val) for hyp, val in pg.items() if hyp!=\"params\"] for pg in optimizer_diff.param_groups]"
   ]
  },
  {
   "source": [
    "We also could've done this like so:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "weight_decay \t 0.0005\nlr \t 0.1\nmomentum \t 0.9\ndampening \t 0\nnesterov \t False\nweight_decay \t 0.001\nlr \t 0.1\nmomentum \t 0.9\ndampening \t 0\nnesterov \t False\n"
     ]
    }
   ],
   "source": [
    "optimizer_diff = torch.optim.SGD(\n",
    "    [\n",
    "        {\"params\": model.layers[:6].parameters(), \"weight_decay\": 5e-4},\n",
    "        {\"params\": model.layers[6:].parameters(), \"weight_decay\": 1e-3}\n",
    "    ],\n",
    "     momentum=.9, lr=.1\n",
    ")\n",
    "\n",
    "_ = [[print(hyp, \"\\t\", val) for hyp, val in pg.items() if hyp!=\"params\"] for pg in optimizer_diff.param_groups]"
   ]
  },
  {
   "source": [
    "### The Learning Rate dilemma in Deep Learning\n",
    "\n",
    "static nature of the learning rate (LR):\n",
    "* if the LR is too high, we'll notice a sharp increase in accuracy with a relatively quick plateu corresponding to non-optimal solutions.\n",
    "  * this is because we'll likely miss local optima because our step in the parameter space is too large\n",
    "* if the LR is too low, training will be excruciatingly low and we'll likely get stuck in very bad local optima, being unable to get out of them because the step in the parameter space is too low to get out of these _valleys_\n",
    "\n",
    "An _ideal_ solution would be to keep a _high enough_ LR until we find a _good enough_ portion of the parameter space, then decrease progressively the LR in order to carefully explore these areas for good optima.\n",
    "\n",
    "Mid-training learning rate toggling is called in a variety of terms: **learning rate decay**, **learning rate annealing**, **learning rate scheduling**...\n",
    "\n",
    "The simplest idea to implement this is a **stepwise** learning rate annealing:\n",
    "\n",
    "![](https://miro.medium.com/max/864/1*VQkTnjr2VJOz0R2m4hDucQ.jpeg)\n",
    "\n",
    "*picture from [towardsdatascience.com](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1?gi=aea0860a2f14).*\n",
    "\n",
    "In our MLP trained with SGD + momentum, we wish to train for 15 epochs and decrease the lr by a factor of 1/10 **before** epoch 7 and 12.\n",
    "\n",
    "Let us recover our training loop and update it accordingly:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy):\n",
    "\n",
    "    # create the folder for the checkpoints (if it's not None)\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # epoch loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch in (6, 11):\n",
    "            for pg in optimizer.param_groups:\n",
    "                pg[\"lr\"] *= .1\n",
    "\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        # produce checkpoint dictionary -- but only if the name and folder of the checkpoint are not None\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg"
   ]
  },
  {
   "source": [
    "PyTorch has a tool additional to the optimizer, the **`lr_scheduler`**.\n",
    "\n",
    "The closest thing to the one above is the **StepLR**, which decays the lr by `gamma` each `step_size` epochs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=.1, weight_decay=5e-4, momentum=.9)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy, lr_scheduler=None, epoch_start_scheduler=1):\n",
    "    # added lr_scheduler\n",
    "\n",
    "    # create the folder for the checkpoints (if it's not None)\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # epoch loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        # added print for LR\n",
    "        print(f\"Epoch {epoch+1} --- learning rate {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        # produce checkpoint dictionary -- but only if the name and folder of the checkpoint are not None\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            if epoch >= epoch_start_scheduler:\n",
    "                lr_scheduler.step()\n",
    "            # or you can use a MultiStepLR with milestones=[6, 11] thus deleting the `if` construct for the epoch\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 --- learning rate 0.10000\n",
      "Epoch 1 completed. Loss - total: 24835.74279499054 - average: 0.41392904658317564; Performance: 0.8763166666666666\n",
      "Epoch 2 --- learning rate 0.10000\n",
      "Epoch 2 completed. Loss - total: 15146.720636367798 - average: 0.2524453439394633; Performance: 0.9264\n",
      "Epoch 3 --- learning rate 0.10000\n",
      "Epoch 3 completed. Loss - total: 13459.693778038025 - average: 0.2243282296339671; Performance: 0.9339666666666666\n",
      "Epoch 4 --- learning rate 0.10000\n",
      "Epoch 4 completed. Loss - total: 12862.99659538269 - average: 0.2143832765897115; Performance: 0.9365166666666667\n",
      "Epoch 5 --- learning rate 0.10000\n",
      "Epoch 5 completed. Loss - total: 12256.855070352554 - average: 0.20428091783920924; Performance: 0.9390333333333334\n",
      "Epoch 6 --- learning rate 0.10000\n",
      "Epoch 6 completed. Loss - total: 11845.358766078949 - average: 0.19742264610131582; Performance: 0.9412666666666667\n",
      "Epoch 7 --- learning rate 0.01000\n",
      "Epoch 7 completed. Loss - total: 9745.373826026917 - average: 0.1624228971004486; Performance: 0.9513333333333334\n",
      "Epoch 8 --- learning rate 0.01000\n",
      "Epoch 8 completed. Loss - total: 8947.71988773346 - average: 0.14912866479555767; Performance: 0.9555\n",
      "Epoch 9 --- learning rate 0.01000\n",
      "Epoch 9 completed. Loss - total: 8606.213770866394 - average: 0.14343689618110655; Performance: 0.95725\n",
      "Epoch 10 --- learning rate 0.01000\n",
      "Epoch 10 completed. Loss - total: 8713.001657009125 - average: 0.1452166942834854; Performance: 0.9568666666666666\n",
      "Epoch 11 --- learning rate 0.01000\n",
      "Epoch 11 completed. Loss - total: 8473.267280578613 - average: 0.1412211213429769; Performance: 0.9576\n",
      "Epoch 12 --- learning rate 0.00100\n",
      "Epoch 12 completed. Loss - total: 8222.787214756012 - average: 0.13704645357926687; Performance: 0.9590666666666666\n",
      "Epoch 13 --- learning rate 0.00100\n",
      "Epoch 13 completed. Loss - total: 8011.786520957947 - average: 0.1335297753492991; Performance: 0.9607333333333333\n",
      "Epoch 14 --- learning rate 0.00100\n",
      "Epoch 14 completed. Loss - total: 7994.584562063217 - average: 0.13324307603438695; Performance: 0.9607666666666667\n",
      "Epoch 15 --- learning rate 0.00100\n",
      "Epoch 15 completed. Loss - total: 7881.568531513214 - average: 0.13135947552522023; Performance: 0.9606666666666667\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7881.568531513214, 0.9606666666666667)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "train_model(model, trainloader, loss_fn, optimizer, 15, lr_scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss -- - performance 0.9719666666666666\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(None, 0.9719666666666666)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "test_model(model, testloader)"
   ]
  },
  {
   "source": [
    "#### Other techniques\n",
    "\n",
    "1. Exponential Annealing\n",
    "\n",
    "![](https://miro.medium.com/max/432/1*iSZv0xuVCsCCK7Z4UiXf2g.jpeg)\n",
    "\n",
    "*picture from [towardsdatascience.com](towardsdatascience.com).*\n",
    "\n",
    "2. Cosine Annealing\n",
    "\n",
    "![](https://miro.medium.com/max/1266/1*2NAuh6DbcrrMv4Voq5yG9A.png)\n",
    "\n",
    "*picture from [towardsdatascience.com](towardsdatascience.com).*\n",
    "\n",
    "3. Triangular Annealing\n",
    "\n",
    "![](img/lr_tri.jpg)\n",
    "\n",
    "*picture from [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/pdf/1801.06146.pdf).*\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Warm-up\n",
    "\n",
    "Warm-up is a techinque which is centered on the idea that, before we start the actual training, the network has to be _warmed-up_ with some iterations of training at an ever-increasing LR, till we hit the target LR $\\eta$.\n",
    "\n",
    "A simple implementation of this (which resembles the ascending phase of the triangular schedule above) could be to:\n",
    "* warm up for $U$ iterations\n",
    "* increase the LR by a fraction $\\frac{\\eta}{U}$.\n",
    "\n",
    "So, at iteration $u\\in\\{1,\\dots,U\\}$, the LR is $u\\frac{\\eta}{U}$.\n",
    "\n",
    "Hence, the triangular annealing above could be thought of as a composition of\n",
    "\n",
    "1. Linear warm-up, and\n",
    "2. Linear annealing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### LR schedule cycling\n",
    "\n",
    "The aforementioned schedules can be cycled multiple times during the same training, giving rise to shapes like the following:\n",
    "\n",
    "![](https://miro.medium.com/max/890/1*xaQVSxG_13E7ZhwPPvPNhw.png)\n",
    "\n",
    "*picture from [towardsdatascience.com](towardsdatascience.com)*\n",
    "\n",
    "![](https://pyimagesearch.com/wp-content/uploads/2019/07/keras_clr_triangular.png)\n",
    "\n",
    "*picture from [pyimagesearch.com](https://www.pyimagesearch.com/2019/07/29/cyclical-learning-rates-with-keras-and-deep-learning/)*\n",
    "\n",
    "This, coupled with $E_{opt}$ early stopping, might actually give the optimizer multiple end points from which to choose our best model. Each time the LR gets \"bumped up\", we get a \"fresh restart\" from a possibly more favorable initialization, in the hope of getting closer and closer to a good local optimum.\n",
    "\n",
    "On to something a bit more complex:\n",
    "\n",
    "![](https://www.pyimagesearch.com/wp-content/uploads/2019/07/keras_clr_exp_range.png)\n",
    "\n",
    "*picture from [pyimagesearch.com](https://www.pyimagesearch.com/2019/07/29/cyclical-learning-rates-with-keras-and-deep-learning/)*\n",
    "\n",
    "the maximum LR gets decayed as well in a \"logarithmic\" way. We can have a similar figure with the cosine annealing as well.\n",
    "\n",
    "Further watch (a bit older, from 2018): [2](https://www.youtube.com/watch?v=kbe_tNGoBHI)\n",
    "\n",
    "Further read, an argument proposing an alternative to LR annealing: [Don't Decay the Learning Rate, Increase the Batch Size](https://arxiv.org/abs/1711.00489). The batch size is not solely related to the available memory for training: a batch size too high will often result in the optimizer getting stuck in bad areas -- indeed, often networks trained with full-batch gradient descent act badly. The **batch size is then another regularizer** and finding its optimal value is dataset- and architecture-dependent. According to the paper above, increasing the batch size as the network approaches a good solution might be a good idea -- I won't take it for granted though, as a lot of research, being based upon experimental proofs, often presents results specific to a given dataset or architecture but depicting them as \"general\"; moreover, _clickbait titles_ are more and more present even for research papers that should instead be based upon humble claims and careful enucleation of limitations.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Defining a custom optimizer\n",
    "\n",
    "To define a custom optimizer, we essentially need to build a class inheriting from `torch.optim.Optimizer` and having the following methods:\n",
    "* `__init__`\n",
    "* `step`\n",
    "\n",
    "Let's see an example below:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD_Alternate(torch.optim.Optimizer):\n",
    "    '''\n",
    "    An alternative of torch.optim.SGD with momentum and no weight decay\n",
    "    '''\n",
    "    def __init__(self, parameters, lr:float, momentum:float):\n",
    "        # we skip check on the args values\n",
    "        # initialize superclass with\n",
    "            # parameters\n",
    "            # hyperparameters as dict\n",
    "        super().__init__(\n",
    "            parameters,\n",
    "            {\n",
    "                \"lr\": lr,\n",
    "                \"momentum\": momentum\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @torch.no_grad() # -> decorator specifying that all the computations here are done without keeping the gradient\n",
    "    def step(self):\n",
    "        for pg in self.param_groups: # note: param_groups is already here bc it's defined in the father class\n",
    "            for param in pg[\"params\"]:\n",
    "                if param.grad is not None: # param update operated only on those params having gradient\n",
    "                    \n",
    "                    # now we wish to recover the momentum buffer (which in the equation of SGD with momentum is indicated with M)\n",
    "                    # each parameter within the optimizer group indexes a dictionary where the values are a group of entities\n",
    "                    # which are crucial for the optimizer to work\n",
    "                    state = self.state[param]\n",
    "                    # we use .get so it does not throw an exception in case of nonexistence of the key in the dict\n",
    "                    M = state.get(\"momentum_buffer\")\n",
    "\n",
    "                    if M is not None:\n",
    "                        # if the buffer exists -> update it\n",
    "                        # M = self.momentum * M + self.lr * param.grad\n",
    "                        # ↓ more advanced notion than ↑\n",
    "                        M.mul_(pg[\"momentum\"]).add_(param.grad * pg[\"lr\"])\n",
    "                    else:\n",
    "                        # if the buffer does not exist, we must calculate the momentum and then assign it to the dict\n",
    "                        # we don't use pg[\"momentum\"] as we don't have previous values for it.\n",
    "                        M = pg[\"lr\"] * param.grad\n",
    "                        state[\"momentum_buffer\"] = M\n",
    "                    # now we can update the params by subtracting the buffer M\n",
    "                    param.sub_(M)\n"
   ]
  },
  {
   "source": [
    "Let us verify that it works by training on a few epochs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP()\n",
    "optim_alt = SGD_Alternate(net.parameters(), lr=.1, momentum=.9)\n",
    "train_model(net, trainloader, loss_fn, optim_alt, 3)"
   ]
  },
  {
   "source": [
    "#### References for this chapter\n",
    "\n",
    "[Writing Your Own Optimizers in PyTorch, by Daniel McNeela](https://mcneela.github.io/machine_learning/2019/09/03/Writing-Your-Own-Optimizers-In-Pytorch.html)\n",
    "\n",
    "[Official PT source for SGD](https://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Homework\n",
    "\n",
    "1. Now that you have all the tools to train an MLP with high performance on MNIST, try reaching 0-loss on the training data (with a small epsilon, e.g. 99.99% training performance -- don't worry if you overfit!).\n",
    "The implementation is completely up to you. You just need to keep it an MLP without using fancy layers (e.g., keep the `Linear` layers, don't use `Conv1d` or something like this, don't use attention). You are free to use any LR scheduler or optimizer, any one of batchnorm/groupnorm, regularization methods... If you use something we haven't seen during lectures, please motivate your choice and explain (as briefly as possible) how it works.\n",
    "2. Try reaching 0-loss on the training data with **permuted labels**. Assess the model on the test data (without permuted labels) and comment. Help yourself with [3](https://arxiv.org/abs/1611.03530).\n",
    "*Tip*: To permute the labels, act on the `trainset.targets` with an appropriate torch function.\n",
    "Then, you can pass this \"permuted\" `Dataset` to a `DataLoader` like so: `trainloader_permuted = torch.utils.data.DataLoader(trainset_permuted, batch_size=batch_size_train, shuffle=True)`. You can now use this `DataLoader` inside the training function.\n",
    "Additional view for motivating this exercise: [\"The statistical significance perfect linear separation\", by Jared Tanner (Oxford U.)](https://www.youtube.com/watch?v=vl2QsVWEqdA)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### References\n",
    "\n",
    "[1](https://www.deeplearningbook.org/) LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. nature, 521(7553), 436-444.\n",
    "\n",
    "[2](https://www.youtube.com/watch?v=kbe_tNGoBHI) State-of-the-art Learning Rate Schedules. Apache MXNet. YouTube.\n",
    "\n",
    "[3](https://arxiv.org/abs/1611.03530) Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2016). Understanding deep learning requires rethinking generalization.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}