{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap from previous Lab\n",
    "\n",
    "* We \"closed the loop\" on our first MultiLayer Perceptron (MLP), exploring how the training routine is implemented in PyTorch (PT):\n",
    "\n",
    "    * we saw how to use built-in loss functions in PT and we learned how to construct custom losses based upon tensor methods\n",
    "    * moreover, we also saw how to use vanilla Stochastic Gradient Descent (SGD) in conjunction with backpropagation to enable the parameters updating in our MLP\n",
    "\n",
    "### Agenda for today\n",
    "\n",
    "* The main topic of our lecture is **regularization**\n",
    "* First of all, though, we will implement a framework for monitoring the parameters during training (the so called *trajectory*), a simple research exercize\n",
    "* On to regularization, we will see how to utilize various way to infuse regularization into our MLP training, still with an eye on the trajectory:\n",
    "\n",
    "  * L2 regularization (aka *weight decay*)\n",
    "  * dropout\n",
    "  * normalization layers (not really regularization, but they fit well in this lab)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining parameters mid-training â€“ the trajectory\n",
    "\n",
    "We already covered how to save the \"snapshot\" of the parameters via the `state_dict` during the previous Lab.\n",
    "We can use the same method to recover the trajectory during our training, although a more useful alternative is `model.named_parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from scripts import mnist\n",
    "from scripts.train_utils import accuracy, AverageMeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us quickly recover the stuff we implemented during Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flat = nn.Flatten()\n",
    "        self.h1 = nn.Linear(28*28, 16)\n",
    "        self.h2 = nn.Linear(16, 32)\n",
    "        self.h3 = nn.Linear(32, 24)\n",
    "        self.out = nn.Linear(24, 10)\n",
    "    \n",
    "    def forward(self, X, activ_hidden=nn.functional.relu):\n",
    "        out = self.flat(X)\n",
    "        out = activ_hidden(self.h1(out))\n",
    "        out = activ_hidden(self.h2(out))\n",
    "        out = activ_hidden(self.h3(out))\n",
    "        out = self.out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what `model.named_parameters()` is and how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "h1.weight \t torch.Size([16, 784])\nh1.bias \t torch.Size([16])\nh2.weight \t torch.Size([32, 16])\nh2.bias \t torch.Size([32])\nh3.weight \t torch.Size([24, 32])\nh3.bias \t torch.Size([24])\nout.weight \t torch.Size([10, 24])\nout.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, \"\\t\", param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add a small piece of code to our `train_epoch` from last Lab to implement the tracking of the trajectory: more specifically, we're interested in the L2-norm of the parameters during each training iteration.\n",
    "\n",
    "NB:\n",
    "* Each **epoch** is composed of **training iterations**:\n",
    "  * a training iteration coincides with the forward/backward pass on a single batch\n",
    "  * an epoch is completed when the whole of the dataset has been seen from the network during training. After an epoch has ended, we reshuffle the batches and begin a new epoch\n",
    "\n",
    "As we can see above, parameters are stored in a (lazy) list of tuples. If we want to calculate the norm, we can't do it on such structure.\n",
    "What we need to do is:\n",
    "* \"flatten\" all the list in a single tensor (a vector)\n",
    "  * first, we must create this tensor, and to do so we have to know the number of parameters that we will need to fit into it\n",
    "* calculate the norm of this vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the norm of the parameters, we're interested in the **norm of the gradients**.\n",
    "\n",
    "Gradients of a layer may be accessed via `tensor.grad` where `tensor` is the tensor associated to the parameters of a given layer.\n",
    "\n",
    "Let's try to call it now on our `named_parameters`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "h1.weight \t None\nh1.bias \t None\nh2.weight \t None\nh2.bias \t None\nh3.weight \t None\nh3.bias \t None\nout.weight \t None\nout.bias \t None\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, \"\\t\", param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all the gradients are `None`\n",
    "\n",
    "**Q**: who knows why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_and_gradients_norm(named_parameters):\n",
    "    square_norms_params = []\n",
    "    square_norms_grads = []\n",
    "\n",
    "    for _, param in named_parameters:\n",
    "\n",
    "        # Q: what is this and why did I write it here?\n",
    "        if param.requires_grad:\n",
    "            square_norms_params.append((param ** 2).sum())\n",
    "            square_norms_grads.append((param.grad ** 2).sum())\n",
    "    \n",
    "    norm_params = sum(square_norms_params).sqrt().item()\n",
    "    norm_grads = sum(square_norms_grads).sqrt().item()\n",
    "\n",
    "    return norm_params, norm_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, trajectory, device): # note: I've added a generic performance to replace accuracy and the device\n",
    "    for X, y in dataloader:\n",
    "        # TRANSFER X AND y TO GPU IF SPECIFIED\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # ... like last time\n",
    "        optimizer.zero_grad() \n",
    "        y_hat = model(X)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = performance(y_hat, y)\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])\n",
    "\n",
    "        if trajectory is not None:\n",
    "\n",
    "            params_norm, gradients_norm = get_params_and_gradients_norm(model.named_parameters())\n",
    "            trajectory[\"parameters\"].append(params_norm)\n",
    "            trajectory[\"gradients\"].append(gradients_norm)\n",
    "\n",
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy, trajectory=None, device=None): # note: I've added a generic performance to replace accuracy and an object where to store the trajectory and the device on which to run our training\n",
    "\n",
    "    # establish device\n",
    "    if device is None:\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    # create the folder for the checkpoints (if it's not None)\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # epoch loop\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, trajectory, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        # produce checkpoint dictionary -- but only if the name and folder of the checkpoint are not None\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg, trajectory\n",
    "\n",
    "def test_model(model, dataloader, performance=accuracy, loss_fn=None, device=None):\n",
    "    # establish device\n",
    "    if device is None:\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # create an AverageMeter for the loss if passed\n",
    "    if loss_fn is not None:\n",
    "        loss_meter = AverageMeter()\n",
    "    \n",
    "    performance_meter = AverageMeter()\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_hat = model(X)\n",
    "            loss = loss_fn(y_hat, y) if loss_fn is not None else None\n",
    "            acc = performance(y_hat, y)\n",
    "            if loss_fn is not None:\n",
    "                loss_meter.update(loss.item(), X.shape[0])\n",
    "            performance_meter.update(acc, X.shape[0])\n",
    "    # get final performances\n",
    "    fin_loss = loss_meter.sum if loss_fn is not None else None\n",
    "    fin_perf = performance_meter.avg\n",
    "    print(f\"TESTING - loss {fin_loss if fin_loss is not None else '--'} - performance {fin_perf}\")\n",
    "    return fin_loss, fin_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size_train = 256\n",
    "minibatch_size_test = 512\n",
    "\n",
    "trainloader, testloader, trainset, testset = mnist.get_data(batch_size_train=minibatch_size_test, batch_size_test=minibatch_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.1\n",
    "num_epochs = 5\n",
    "\n",
    "model = MLP()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate)\n",
    "\n",
    "trajectory = {\"parameters\": [], \"gradients\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training on cpu\n",
      "Epoch 1 completed. Loss - total: 80854.3089466095 - average: 1.347571815776825; Performance: 0.5584666666348775\n",
      "Epoch 2 completed. Loss - total: 24299.805331230164 - average: 0.40499675552050274; Performance: 0.8776833333333334\n",
      "Epoch 3 completed. Loss - total: 18296.16684103012 - average: 0.3049361140171687; Performance: 0.9090333333651225\n",
      "Epoch 4 completed. Loss - total: 15596.531733512878 - average: 0.259942195558548; Performance: 0.9216833333333333\n",
      "Epoch 5 completed. Loss - total: 14754.555691242218 - average: 0.24590926152070364; Performance: 0.9278666666666666\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc, trajectory = train_model(model, trainloader, loss_fn, optimizer, num_epochs, trajectory=trajectory, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(trajectory, ylim=(0,9)):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, sharey=True)\n",
    "    ax1.set_ylim(*ylim)\n",
    "    ax1.plot(trajectory[\"parameters\"])\n",
    "    ax1.set_title(\"Norm of parameters\")\n",
    "    ax2.plot(trajectory[\"gradients\"])\n",
    "    ax2.set_title(\"Norm of gradients\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 367.772627 263.63625\" width=\"367.772627pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-03-23T12:52:12.540335</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 263.63625 \nL 367.772627 263.63625 \nL 367.772627 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 20.5625 239.758125 \nL 172.744318 239.758125 \nL 172.744318 22.318125 \nL 20.5625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m1ecba439cc\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"27.479855\" xlink:href=\"#m1ecba439cc\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(24.298605 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"74.456802\" xlink:href=\"#m1ecba439cc\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <g transform=\"translate(64.913052 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"121.433748\" xlink:href=\"#m1ecba439cc\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 400 -->\n      <g transform=\"translate(111.889998 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"168.410695\" xlink:href=\"#m1ecba439cc\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 600 -->\n      <g transform=\"translate(158.866945 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"me28ad32677\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me28ad32677\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0 -->\n      <g transform=\"translate(7.2 243.557344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me28ad32677\" y=\"215.598125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1 -->\n      <g transform=\"translate(7.2 219.397344)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me28ad32677\" y=\"191.438125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 2 -->\n      <g transform=\"translate(7.2 195.237344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me28ad32677\" y=\"167.278125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 3 -->\n      <g transform=\"translate(7.2 171.077344)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me28ad32677\" y=\"143.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 4 -->\n      <g transform=\"translate(7.2 146.917344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me28ad32677\" y=\"118.958125\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 5 -->\n      <g transform=\"translate(7.2 122.757344)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me28ad32677\" y=\"94.798125\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 6 -->\n      <g transform=\"translate(7.2 98.597344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me28ad32677\" y=\"70.638125\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 7 -->\n      <g transform=\"translate(7.2 74.437344)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me28ad32677\" y=\"46.478125\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 8 -->\n      <g transform=\"translate(7.2 50.277344)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#me28ad32677\" y=\"22.318125\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 9 -->\n      <g transform=\"translate(7.2 26.117344)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-57\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p09c1d13181)\" d=\"M 27.479855 114.078101 \nL 28.889164 114.060383 \nL 30.063587 113.891517 \nL 31.238011 113.484133 \nL 32.17755 112.915614 \nL 33.117089 112.021609 \nL 34.056628 110.704494 \nL 34.996167 108.886831 \nL 35.935706 106.611866 \nL 38.049668 100.562788 \nL 39.224092 97.170034 \nL 39.928746 95.907711 \nL 40.398516 94.762527 \nL 41.807824 92.011113 \nL 42.982248 89.102561 \nL 43.452017 88.522602 \nL 44.391556 87.063591 \nL 45.09621 85.837419 \nL 45.800865 84.476539 \nL 46.035749 84.206513 \nL 46.505519 83.368566 \nL 46.975288 82.744299 \nL 47.445058 82.599165 \nL 47.679942 82.80295 \nL 48.149712 82.119123 \nL 48.854366 80.98221 \nL 49.089251 80.798771 \nL 49.55902 80.082699 \nL 50.02879 79.474699 \nL 50.498559 78.827506 \nL 51.438098 78.015146 \nL 51.672983 78.06465 \nL 51.907868 77.984813 \nL 52.377637 77.390027 \nL 53.082291 76.840332 \nL 54.961369 74.889378 \nL 55.431139 74.609513 \nL 55.900908 74.184802 \nL 56.840447 73.550502 \nL 57.310216 73.215489 \nL 58.48464 72.587144 \nL 58.95441 72.327071 \nL 59.424179 71.977439 \nL 60.363718 71.175367 \nL 62.477681 69.918481 \nL 63.652104 69.287867 \nL 64.121874 69.10384 \nL 67.41026 67.496516 \nL 68.349799 67.111032 \nL 69.524222 66.524921 \nL 69.993992 66.393289 \nL 70.228877 66.391331 \nL 71.4033 65.84235 \nL 71.87307 65.705246 \nL 78.449842 63.255009 \nL 79.389381 62.998508 \nL 79.624266 63.050453 \nL 80.32892 62.773008 \nL 81.033574 62.510378 \nL 81.503344 62.368665 \nL 81.973113 62.342664 \nL 82.677768 62.133684 \nL 83.147537 61.978528 \nL 85.966154 60.974664 \nL 86.905693 60.686205 \nL 87.845232 60.296206 \nL 90.898733 59.564027 \nL 91.603387 59.255442 \nL 98.415045 57.515645 \nL 98.884814 57.531935 \nL 102.1732 56.76644 \nL 102.408085 56.838638 \nL 104.052278 56.44592 \nL 104.287163 56.467475 \nL 104.991817 56.234106 \nL 106.166241 55.927906 \nL 106.870895 55.679849 \nL 108.749973 55.297533 \nL 109.219742 55.238641 \nL 110.394166 54.898939 \nL 111.333705 54.762768 \nL 111.56859 54.661378 \nL 111.803474 54.683808 \nL 112.038359 54.566588 \nL 112.508129 54.53698 \nL 113.682552 54.18789 \nL 114.856976 54.036696 \nL 115.326745 53.811323 \nL 116.501169 53.338768 \nL 117.205823 53.181273 \nL 117.910477 53.000852 \nL 118.850016 52.816952 \nL 119.084901 52.879036 \nL 119.554671 52.679698 \nL 120.02444 52.583296 \nL 121.198864 52.166212 \nL 125.191904 51.309164 \nL 125.896558 51.178281 \nL 126.366328 50.98187 \nL 127.540751 50.9067 \nL 128.95006 50.649726 \nL 129.654714 50.407959 \nL 130.124483 50.289391 \nL 130.594253 50.116044 \nL 131.064022 49.997073 \nL 131.768677 49.847204 \nL 132.238446 49.864946 \nL 133.177985 49.694697 \nL 133.647754 49.661876 \nL 134.352409 49.515786 \nL 135.526832 49.333222 \nL 136.231487 49.224447 \nL 136.701256 49.130118 \nL 137.40591 49.002011 \nL 137.87568 48.900194 \nL 138.110564 49.094462 \nL 138.345449 48.893143 \nL 138.580334 49.274572 \nL 138.815219 49.349558 \nL 140.224527 48.625997 \nL 143.512913 47.615913 \nL 144.217567 47.416115 \nL 144.687337 47.359331 \nL 145.157106 47.21839 \nL 145.626876 47.16921 \nL 146.566415 46.826559 \nL 147.505954 46.687301 \nL 147.975723 46.66949 \nL 148.445493 46.526695 \nL 149.385032 46.394556 \nL 151.498994 45.948395 \nL 152.438533 45.896922 \nL 156.901343 45.143719 \nL 157.840882 44.946605 \nL 159.25019 44.680622 \nL 163.008346 43.87249 \nL 163.947885 43.800557 \nL 164.887424 43.625032 \nL 165.826963 43.688947 \nL 165.826963 43.688947 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 20.5625 239.758125 \nL 20.5625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 172.744318 239.758125 \nL 172.744318 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 20.5625 239.758125 \nL 172.744318 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 20.5625 22.318125 \nL 172.744318 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_15\">\n    <!-- Norm of parameters -->\n    <g transform=\"translate(35.888409 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"74.804688\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"135.986328\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"175.349609\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"272.761719\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"304.548828\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"365.730469\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"400.935547\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"432.722656\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"496.199219\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"557.478516\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"598.591797\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"659.871094\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"757.283203\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"818.806641\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"858.015625\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"919.539062\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"960.652344\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 203.180682 239.758125 \nL 355.3625 239.758125 \nL 355.3625 22.318125 \nL 203.180682 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_5\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"210.098037\" xlink:href=\"#m1ecba439cc\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0 -->\n      <g transform=\"translate(206.916787 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"257.074984\" xlink:href=\"#m1ecba439cc\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 200 -->\n      <g transform=\"translate(247.531234 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"304.05193\" xlink:href=\"#m1ecba439cc\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 400 -->\n      <g transform=\"translate(294.50818 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"351.028877\" xlink:href=\"#m1ecba439cc\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 600 -->\n      <g transform=\"translate(341.485127 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_11\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#me28ad32677\" y=\"239.758125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#me28ad32677\" y=\"215.598125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#me28ad32677\" y=\"191.438125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#me28ad32677\" y=\"167.278125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#me28ad32677\" y=\"143.118125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#me28ad32677\" y=\"118.958125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#me28ad32677\" y=\"94.798125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#me28ad32677\" y=\"70.638125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#me28ad32677\" y=\"46.478125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#me28ad32677\" y=\"22.318125\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_30\">\n    <path clip-path=\"url(#p6d1a8dcbfb)\" d=\"M 210.098037 234.721863 \nL 210.332922 235.484802 \nL 210.567807 235.317298 \nL 210.802691 235.505925 \nL 211.037576 235.438987 \nL 211.272461 235.727517 \nL 211.507346 234.980952 \nL 211.74223 234.74868 \nL 211.977115 233.441794 \nL 212.212 233.61023 \nL 212.681769 233.177422 \nL 212.916654 233.46874 \nL 213.386423 232.75573 \nL 213.621308 232.831101 \nL 214.091078 232.157388 \nL 215.265501 229.646083 \nL 215.500386 228.269661 \nL 215.735271 228.423562 \nL 215.970156 227.631666 \nL 216.20504 227.953418 \nL 216.439925 227.482168 \nL 216.67481 225.535627 \nL 216.909694 225.731625 \nL 217.379464 223.45945 \nL 217.849233 224.737307 \nL 218.319003 221.629679 \nL 218.553888 221.836402 \nL 218.788772 222.420744 \nL 219.023657 222.577394 \nL 219.258542 222.514864 \nL 219.728311 220.936594 \nL 219.963196 216.689866 \nL 220.198081 214.639908 \nL 220.432965 214.825577 \nL 220.66785 216.22328 \nL 220.902735 215.37454 \nL 221.372504 205.7273 \nL 222.077159 161.520518 \nL 222.312043 161.210459 \nL 222.546928 184.655181 \nL 222.781813 185.299349 \nL 223.721352 148.92145 \nL 224.895775 217.361546 \nL 225.13066 217.494841 \nL 225.365545 210.698154 \nL 225.60043 194.791344 \nL 225.835314 167.252964 \nL 226.070199 166.405876 \nL 226.539968 170.904132 \nL 226.774853 186.749098 \nL 227.244623 182.374438 \nL 227.479507 190.076433 \nL 227.714392 188.678434 \nL 227.949277 176.981009 \nL 228.184162 175.138415 \nL 228.888816 211.972265 \nL 229.358585 203.044182 \nL 229.59347 187.156948 \nL 230.063239 122.893351 \nL 230.298124 146.24277 \nL 230.533009 191.696861 \nL 230.767894 213.863066 \nL 231.237663 204.697821 \nL 231.472548 206.177672 \nL 231.707433 203.55732 \nL 231.942317 203.833591 \nL 232.177202 209.347308 \nL 232.412087 211.585975 \nL 232.646972 210.23118 \nL 232.881856 214.232084 \nL 233.116741 215.453452 \nL 233.351626 211.506211 \nL 233.821395 177.041093 \nL 234.05628 151.082698 \nL 234.291165 152.634419 \nL 234.760934 201.408089 \nL 234.995819 191.769163 \nL 235.465588 183.657568 \nL 235.935358 201.882734 \nL 236.170242 202.700126 \nL 236.405127 204.081555 \nL 236.640012 201.540982 \nL 237.109781 189.671606 \nL 237.344666 177.345047 \nL 237.579551 191.786936 \nL 237.814436 194.352541 \nL 238.284205 215.476933 \nL 238.51909 216.563458 \nL 238.753975 215.799702 \nL 238.988859 215.978193 \nL 239.223744 212.024167 \nL 239.693513 218.512659 \nL 240.398168 188.484598 \nL 240.867937 154.311793 \nL 241.102822 172.458422 \nL 241.572591 221.887898 \nL 241.807476 221.044338 \nL 242.042361 221.531163 \nL 242.277246 222.913737 \nL 242.9819 198.553164 \nL 243.216784 197.470308 \nL 243.921439 208.829665 \nL 244.156323 202.420134 \nL 244.391208 202.889273 \nL 244.860978 223.811094 \nL 245.565632 203.599669 \nL 246.035401 213.122394 \nL 246.505171 205.649379 \nL 246.740055 203.536857 \nL 246.97494 204.670115 \nL 247.209825 201.398216 \nL 247.44471 193.808557 \nL 247.679594 196.450844 \nL 248.149364 209.990557 \nL 248.384249 219.824656 \nL 248.619133 221.720074 \nL 248.854018 217.12702 \nL 249.088903 203.883474 \nL 249.323788 205.271738 \nL 249.558672 201.107159 \nL 249.793557 188.315167 \nL 250.263326 215.592106 \nL 250.498211 218.429039 \nL 250.733096 214.167526 \nL 250.967981 202.03368 \nL 251.202865 200.522707 \nL 251.672635 190.310302 \nL 252.142404 213.096191 \nL 252.377289 216.718412 \nL 252.612174 216.537146 \nL 252.847059 207.105691 \nL 253.081943 206.357984 \nL 253.316828 200.97271 \nL 253.551713 205.188209 \nL 253.786597 211.676332 \nL 254.021482 210.162816 \nL 254.256367 210.891425 \nL 254.491252 206.329721 \nL 254.726136 206.382977 \nL 254.961021 210.905512 \nL 255.195906 209.561025 \nL 255.430791 209.846581 \nL 255.665675 212.273981 \nL 255.90056 221.0352 \nL 256.135445 222.333415 \nL 256.370329 221.165939 \nL 256.605214 214.755844 \nL 256.840099 217.313585 \nL 257.074984 221.812381 \nL 257.309868 223.584697 \nL 257.779638 215.994848 \nL 258.249407 219.662948 \nL 258.484292 224.516262 \nL 258.719177 223.23368 \nL 258.954062 218.991528 \nL 259.188946 217.94456 \nL 259.658716 223.945621 \nL 259.8936 219.111245 \nL 260.128485 207.716551 \nL 260.833139 219.954238 \nL 261.068024 226.456109 \nL 261.537794 211.407694 \nL 262.007563 204.369078 \nL 262.242448 190.088201 \nL 262.712217 213.023549 \nL 262.947102 211.22471 \nL 263.416871 215.029816 \nL 263.651756 219.758576 \nL 263.886641 221.962198 \nL 264.121526 218.149604 \nL 264.35641 218.645693 \nL 264.591295 215.884022 \nL 264.82618 216.581806 \nL 265.061065 215.9738 \nL 265.530834 177.540594 \nL 266.000604 197.124394 \nL 266.235488 208.823142 \nL 266.940142 223.230049 \nL 267.175027 222.313257 \nL 267.409912 221.910954 \nL 267.644797 222.241823 \nL 268.114566 224.698435 \nL 268.349451 212.12038 \nL 268.584336 207.895661 \nL 268.81922 212.874648 \nL 269.054105 221.563785 \nL 269.28899 219.162182 \nL 269.523875 219.533709 \nL 269.758759 223.627776 \nL 269.993644 221.069067 \nL 270.228529 222.866539 \nL 270.463413 221.019055 \nL 270.698298 207.45075 \nL 270.933183 200.337465 \nL 271.168068 203.835687 \nL 271.402952 204.865014 \nL 271.637837 205.13875 \nL 271.872722 209.040229 \nL 272.107607 218.329535 \nL 272.577376 225.437969 \nL 272.812261 226.363727 \nL 273.28203 216.575336 \nL 273.516915 224.377995 \nL 273.7518 226.07626 \nL 274.221569 211.525127 \nL 274.456454 214.657857 \nL 274.691339 221.954787 \nL 274.926223 221.745761 \nL 275.161108 223.347874 \nL 275.395993 226.597804 \nL 275.630878 220.253936 \nL 275.865762 218.463036 \nL 276.100647 221.193124 \nL 276.570416 217.04209 \nL 276.805301 217.050336 \nL 277.040186 216.648185 \nL 277.275071 214.775777 \nL 277.509955 203.090609 \nL 277.74484 207.540597 \nL 277.979725 218.022622 \nL 278.21461 216.297297 \nL 278.449494 208.294762 \nL 278.684379 212.311039 \nL 278.919264 221.097466 \nL 279.154149 216.310717 \nL 279.389033 214.839289 \nL 279.623918 215.275762 \nL 279.858803 222.043759 \nL 280.328572 203.26637 \nL 280.563457 208.064758 \nL 280.798342 220.294523 \nL 281.268111 210.485375 \nL 281.737881 222.791091 \nL 281.972765 222.817134 \nL 282.20765 224.542063 \nL 282.67742 210.71953 \nL 282.912304 219.732734 \nL 283.382074 210.038465 \nL 283.616958 215.247033 \nL 284.321613 219.183524 \nL 284.556497 217.93952 \nL 284.791382 218.47914 \nL 285.026267 220.477963 \nL 285.261152 225.513005 \nL 285.496036 220.607278 \nL 285.730921 227.337679 \nL 285.965806 225.773537 \nL 286.200691 226.25945 \nL 286.435575 221.616174 \nL 286.67046 222.785845 \nL 287.140229 216.01529 \nL 287.609999 216.689166 \nL 287.844884 213.251443 \nL 288.079768 213.740926 \nL 288.314653 210.997427 \nL 288.549538 211.237861 \nL 288.784423 222.982243 \nL 289.019307 219.626565 \nL 289.254192 219.007206 \nL 289.489077 213.8498 \nL 289.723962 214.673554 \nL 289.958846 208.373029 \nL 290.6635 226.706412 \nL 290.898385 224.156229 \nL 291.13327 218.180098 \nL 291.368155 222.150107 \nL 291.603039 221.7918 \nL 291.837924 216.36235 \nL 292.307694 223.411654 \nL 292.542578 223.811423 \nL 292.777463 223.921431 \nL 293.012348 211.322167 \nL 293.247232 208.613715 \nL 293.482117 214.37875 \nL 293.717002 223.824917 \nL 294.186771 221.351159 \nL 294.656541 226.210294 \nL 294.891426 220.957671 \nL 295.12631 218.289255 \nL 295.361195 212.079721 \nL 295.59608 211.496444 \nL 295.830965 205.728878 \nL 296.065849 190.693666 \nL 296.300734 198.34523 \nL 296.535619 212.304311 \nL 296.770503 214.343912 \nL 297.005388 222.888985 \nL 297.240273 222.154159 \nL 297.475158 215.501227 \nL 297.710042 205.330766 \nL 297.944927 211.450489 \nL 298.179812 207.324697 \nL 298.414697 210.965118 \nL 298.884466 226.043368 \nL 299.119351 221.409677 \nL 299.354236 222.457304 \nL 299.58912 228.804924 \nL 299.824005 224.142969 \nL 300.05889 223.525577 \nL 300.528659 208.354982 \nL 300.763544 194.731904 \nL 300.998429 193.864799 \nL 301.233313 212.225682 \nL 301.468198 221.524246 \nL 301.703083 214.946325 \nL 301.937968 219.025482 \nL 302.172852 218.724177 \nL 302.407737 220.892919 \nL 302.642622 215.070149 \nL 302.877507 212.930364 \nL 303.112391 219.721925 \nL 303.582161 222.765802 \nL 303.817045 220.213827 \nL 304.05193 223.02653 \nL 304.286815 222.849069 \nL 304.5217 217.897902 \nL 304.756584 221.506549 \nL 304.991469 220.710017 \nL 305.226354 223.184923 \nL 305.696123 204.290316 \nL 305.931008 209.303064 \nL 306.400778 226.806769 \nL 306.635662 224.990155 \nL 306.870547 222.039495 \nL 307.105432 223.671171 \nL 307.340316 218.900169 \nL 307.575201 219.668646 \nL 308.279855 208.480459 \nL 308.51474 216.839812 \nL 308.749625 218.554138 \nL 308.98451 222.155298 \nL 309.219394 219.423699 \nL 309.454279 220.596561 \nL 309.689164 213.13822 \nL 310.158933 210.135285 \nL 310.628703 224.529716 \nL 310.863587 223.562675 \nL 311.098472 225.15281 \nL 311.333357 221.079595 \nL 311.568242 221.034952 \nL 311.803126 222.38048 \nL 312.038011 227.081602 \nL 312.272896 228.013867 \nL 312.507781 225.963631 \nL 312.742665 225.374405 \nL 312.97755 216.21555 \nL 313.212435 222.277171 \nL 313.682204 225.771677 \nL 313.917089 225.731828 \nL 314.151974 217.819322 \nL 314.386858 223.651783 \nL 314.621743 224.998018 \nL 314.856628 227.45774 \nL 315.091513 220.168022 \nL 315.326397 224.029074 \nL 315.561282 224.184499 \nL 315.796167 223.907255 \nL 316.031052 222.436914 \nL 316.265936 222.208825 \nL 316.500821 223.26539 \nL 316.735706 227.143389 \nL 316.97059 222.295454 \nL 317.205475 222.542376 \nL 317.44036 222.995769 \nL 317.675245 216.665849 \nL 317.910129 217.392247 \nL 318.145014 219.967722 \nL 318.379899 221.062075 \nL 318.614784 220.040046 \nL 318.849668 220.313137 \nL 319.084553 216.864388 \nL 319.319438 226.996442 \nL 319.554323 224.269207 \nL 319.789207 224.032469 \nL 320.024092 223.0079 \nL 320.258977 224.48934 \nL 320.493861 225.144857 \nL 320.728746 185.730315 \nL 320.963631 98.931766 \nL 321.198516 92.923643 \nL 321.4334 167.961756 \nL 321.90317 213.947239 \nL 322.138055 225.58867 \nL 322.372939 220.525967 \nL 322.607824 226.852404 \nL 322.842709 224.23968 \nL 323.077594 223.328042 \nL 323.312478 217.222426 \nL 323.547363 218.049716 \nL 323.782248 224.931312 \nL 324.017132 221.760084 \nL 324.486902 226.944392 \nL 324.721787 227.948606 \nL 324.956671 226.449819 \nL 325.191556 225.998417 \nL 325.426441 224.842873 \nL 325.661326 226.512407 \nL 325.89621 224.043785 \nL 326.131095 219.621404 \nL 326.36598 221.449273 \nL 326.600865 225.455254 \nL 326.835749 225.870225 \nL 327.070634 223.389666 \nL 327.305519 226.387037 \nL 327.540403 222.201432 \nL 327.775288 222.048536 \nL 328.010173 222.271 \nL 328.245058 224.183714 \nL 328.479942 227.465551 \nL 328.949712 224.697651 \nL 329.184597 221.168456 \nL 329.419481 220.59844 \nL 329.654366 224.368484 \nL 329.889251 216.498527 \nL 330.124135 225.08567 \nL 330.35902 216.988631 \nL 330.593905 221.582201 \nL 330.82879 228.661957 \nL 331.063674 222.651933 \nL 331.298559 221.214111 \nL 331.533444 220.546242 \nL 331.768329 226.546064 \nL 332.003213 227.725477 \nL 332.238098 220.35151 \nL 332.472983 220.075693 \nL 332.707868 218.125169 \nL 332.942752 217.892194 \nL 333.647406 224.400834 \nL 333.882291 224.152672 \nL 334.352061 222.607977 \nL 334.586945 224.410402 \nL 334.82183 219.331103 \nL 335.056715 219.720679 \nL 335.2916 225.404164 \nL 335.761369 209.898555 \nL 335.996254 218.980306 \nL 336.231139 221.243655 \nL 336.466023 221.296152 \nL 336.700908 220.847282 \nL 336.935793 225.413927 \nL 337.170677 219.40944 \nL 337.405562 218.876729 \nL 337.640447 219.996526 \nL 337.875332 219.766303 \nL 338.110216 222.840142 \nL 338.345101 223.560676 \nL 338.579986 223.407296 \nL 338.814871 226.30572 \nL 339.049755 225.281104 \nL 339.28464 215.486484 \nL 339.519525 218.759361 \nL 339.75441 225.375275 \nL 339.989294 222.988834 \nL 340.224179 215.710302 \nL 340.459064 213.009929 \nL 340.693948 223.930447 \nL 340.928833 226.271755 \nL 341.163718 225.400006 \nL 341.398603 225.612088 \nL 341.633487 220.027646 \nL 342.103257 227.214163 \nL 342.338142 226.235292 \nL 342.807911 220.266757 \nL 343.042796 213.752107 \nL 343.277681 219.2252 \nL 343.512565 219.708892 \nL 343.74745 213.498376 \nL 343.982335 224.093204 \nL 344.217219 221.81259 \nL 344.452104 214.473266 \nL 344.686989 211.776825 \nL 344.921874 211.58198 \nL 345.391643 206.954645 \nL 345.626528 215.444855 \nL 345.861413 218.530452 \nL 346.331182 210.628807 \nL 346.800952 221.770497 \nL 347.035836 224.836533 \nL 347.270721 221.890465 \nL 347.74049 225.561369 \nL 348.21026 221.405416 \nL 348.445145 211.760034 \nL 348.445145 211.760034 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 203.180682 239.758125 \nL 203.180682 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 355.3625 239.758125 \nL 355.3625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 203.180682 239.758125 \nL 355.3625 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 203.180682 22.318125 \nL 355.3625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_20\">\n    <!-- Norm of gradients -->\n    <g transform=\"translate(224.907841 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"74.804688\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"135.986328\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"175.349609\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"272.761719\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"304.548828\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"365.730469\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"400.935547\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"432.722656\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"496.199219\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"537.3125\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"598.591797\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"662.068359\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"689.851562\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"751.375\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"814.753906\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"853.962891\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p09c1d13181\">\n   <rect height=\"217.44\" width=\"152.181818\" x=\"20.5625\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p6d1a8dcbfb\">\n   <rect height=\"217.44\" width=\"152.181818\" x=\"203.180682\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plot_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a similar trend to what [3](https://arxiv.org/abs/2002.10365) observed:\n",
    "\n",
    "1. in the very first iterations, gradients are very large and parameters quickly grow\n",
    "2. gradients quickly diminishes, while parameters growth tends to slow down\n",
    "3. gradients reach a \"stationary state\" (with some noise added), while parameters grow even slower\n",
    "\n",
    "5 epochs of training is very little. If you want (_not a homework_), you may try increasing the number of epochs and see what happens.\n",
    "\n",
    "of course, the analysis in [3](https://arxiv.org/abs/2002.10365) is much finer than ours and considers much more quantities and experiments than ours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Regularization in DL comes in the form of different tools. We can have:\n",
    "\n",
    "1. Penalty terms in loss functions (e.g. L1 and L2 norm regularization) which introduce bias in our parameters by actively reducing the magnitude of some weights:\n",
    "    * L1 norm regularization is also called LASSO regularization\n",
    "    * L2 norm regularization is also called Ridge regularization or **weight decay**\n",
    "    * they were originally implemented in linear regression models as a way to infuse *inductive bias* in models originally thought to rely on the complete unbiasedness on training data\n",
    "2. Normalization layers which normalize the incoming information s.t. their mean is zero and standard deviation one. It comes in different flavors:\n",
    "    * batch normalization or batchnorm (the most common technique)\n",
    "    * group normalization or groupnorm\n",
    "    * there are more possibilities, for additional info on these, please check [this lecture by Aaron Defazio, NYU](https://atcold.github.io/pytorch-Deep-Learning/en/week05/05-2/)\n",
    "3. Dropout, a technique [patented by Google](https://patents.google.com/patent/US9406017B2/en) which consists in randomly *dropping* some neurons from a given layer to prevent overfitting.\n",
    "4. Early stopping, which we'll see later on during this Lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight decay or L2 norm/Ridge regularization\n",
    "\n",
    "Weight Decay (WD) is a simple technique which *appends* a penalty term to the loss function equation. The term is based upon the L2 norm of the weights.\n",
    "\n",
    "Given our original loss function $\\mathcal{L}_0 (\\hat{y}, y)$ and our parameter vector $\\Theta$, our new loss will be:\n",
    "\n",
    "$\\mathcal{L}_0 (\\hat{y}, y) + \\frac{1}{2}\\cdot\\lambda \\cdot \\vert\\vert \\Theta \\vert\\vert_2^2$\n",
    "\n",
    "the parameter $\\lambda$ (also called weight decay) controls the strenght of the regularization. $\\lambda$ too high means that the model will not concentrate well enough on the original objective ($\\mathcal{L}_0$), hence it will not perform well. Usually, good values form $\\lambda$ fall into the interval $[5\\cdot 10^{-4}, 1\\cdot 10^{-4}]$.\n",
    "\n",
    "In PT, instead of inserting our penalty term in the loss function, we specify the weight decay parameter in our optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 5e-2 # Note: this parameter is purposedly too high, to show the \"extreme\" effect of regularization on the trajectory\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 norm regularization\n",
    "\n",
    "L1 norm regularization is analogous to weight decay. The equation is:\n",
    "\n",
    "$\\mathcal{L}_0 (\\hat{y}, y) + \\lambda \\cdot \\vert\\vert \\Theta \\vert\\vert_1$\n",
    "\n",
    "where $\\vert\\vert x \\vert\\vert_1 = \\sum_{j=1}^d \\vert x_j \\vert$\n",
    "\n",
    "unlike weight decay, to my knowledge PT does not provide a built-in for L1 reg. You need to define a custom loss function for this task (**homework**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batchnorm\n",
    "\n",
    "Batch Normalization is not really a regularization technique. It operates in such a way that the mean and standard deviation of the incoming batches of data are approximately 0 and 1 respectively.\n",
    "\n",
    "The ultimate goal of batchnorm is not to normalize each batch, but estimate one vector for mean (a running mean) and one for std (a running std) for the whole dataset and to normalize w.r.t. these. So, these become new parameters of the network. They are not adjusted via backprop but they get adjusted each time the layer *sees* another batch of data.\n",
    "\n",
    "![](https://miro.medium.com/max/474/1*QQ2Q5rVBtLv7b3yGhO0flg.png)\n",
    "\n",
    "*picture from [towardsdatascience.com](https://towardsdatascience.com/batch-normalisation-explained-5f4bd9de5feb)*\n",
    "\n",
    "When the network is evaluated on test data, the running mean and std must not be adjusted, hence PT has implemented a \"switch\", which we saw during the previous Lab, to tell the network when to adjust and not adjust these two parameters. The switch is triggered via `model.train()` and `model.eval()` (or equivalently `model.train(False)`).\n",
    "\n",
    "In PT, the batch normalization is found as a regular layer under within the `torch.nn` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=16), # we specify the dimensionality of the incoming data\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=32),\n",
    "            nn.Linear(32, 24),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=24),\n",
    "            nn.Linear(24, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** (for the most skilled students): why didn't we apply batchnorm for the first layer?\n",
    "\n",
    "By peeking at the PT docs, we can see that actually the batchnorm layers have much more hyperparameters which we can play with if we wanted to:\n",
    "\n",
    "![](img/bn_docs.jpg)\n",
    "\n",
    "*from [PyTorch docs](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)*\n",
    "\n",
    "In addition to what we say till now, there is still some debate in the DL community on whether batchnorm or other normalization techniques help optimization. The claims in the original paper [1](https://arxiv.org/abs/1502.03167) of \"reducing internal covariate shift\" was confuted in successive works such as [2](https://arxiv.org/abs/1805.11604.pdf), which claims that it \"makes the optimization landscape significantly smoother\". Another things to consider is that, since the data is distributed in a small intervall around 0, there's also a better numerical stability added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n",
    "\n",
    "Dropout acts by removing (i.e. *zeroing-out*) a random subset of the neurons in a given layer for each forward pass.\n",
    "\n",
    "It has one hyperparameter ($p$), which is the fraction of neurons to be dropped out.\n",
    "\n",
    "During training, each time a layer with backprop produces an output, a fraction $p$ of that output gets discarded. This helps in such a way that co-dependence between neurons gets *forgotten* by the network. To say it in simple terms, it forces each neuron to be independent from the output of other neurons within the same layer.\n",
    "\n",
    "For the same reason as in batchnorm, since dropout has to apply only during training, we must be careful in activating the switch `model.eval()` when testing our network.\n",
    "\n",
    "In PT, we find Dropout as a module of `torch.nn`. Instead of placing if *before* the layer (as in batchnorm), we place it *after* the layer (the reason being, the layer produces an output, a portion $p$ of that output gets discarded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_BN_Drop(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # nn.BatchNorm1d(num_features=16),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=.2), # we add a dropout here. it's referred to the previous layer (with 32 neurons)\n",
    "            # usually with MLP p = .3, with CNN p = .5\n",
    "\n",
    "            nn.BatchNorm1d(num_features=32),\n",
    "            nn.Linear(32, 24),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=24),\n",
    "            nn.Linear(24, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_BN_Drop()\n",
    "weight_decay = 5e-2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "\n",
    "trajectory_reg = {\"parameters\": [], \"gradients\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training on cpu\n",
      "Epoch 1 completed. Loss - total: 56373.67782974243 - average: 0.9395612971623739; Performance: 0.7734166666666666\n",
      "Epoch 2 completed. Loss - total: 31922.05287361145 - average: 0.5320342145601908; Performance: 0.8911666666984558\n",
      "Epoch 3 completed. Loss - total: 29244.092144966125 - average: 0.4874015357494354; Performance: 0.9040833333015442\n",
      "Epoch 4 completed. Loss - total: 28621.994693756104 - average: 0.47703324489593507; Performance: 0.9071499999682109\n",
      "Epoch 5 completed. Loss - total: 28253.929012298584 - average: 0.4708988168716431; Performance: 0.9091000000317891\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc, trajectory_reg = train_model(model, trainloader, loss_fn, optimizer, num_epochs, trajectory=trajectory_reg, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 367.772627 263.63625\" width=\"367.772627pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-03-23T13:14:16.181599</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 263.63625 \nL 367.772627 263.63625 \nL 367.772627 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 20.5625 239.758125 \nL 172.744318 239.758125 \nL 172.744318 22.318125 \nL 20.5625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc2ccd245fa\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"27.479855\" xlink:href=\"#mc2ccd245fa\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(24.298605 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"74.456802\" xlink:href=\"#mc2ccd245fa\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <g transform=\"translate(64.913052 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"121.433748\" xlink:href=\"#mc2ccd245fa\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 400 -->\n      <g transform=\"translate(111.889998 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"168.410695\" xlink:href=\"#mc2ccd245fa\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 600 -->\n      <g transform=\"translate(158.866945 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m01daac12c8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m01daac12c8\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0 -->\n      <g transform=\"translate(7.2 243.557344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m01daac12c8\" y=\"215.598125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1 -->\n      <g transform=\"translate(7.2 219.397344)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m01daac12c8\" y=\"191.438125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 2 -->\n      <g transform=\"translate(7.2 195.237344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m01daac12c8\" y=\"167.278125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 3 -->\n      <g transform=\"translate(7.2 171.077344)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m01daac12c8\" y=\"143.118125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 4 -->\n      <g transform=\"translate(7.2 146.917344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m01daac12c8\" y=\"118.958125\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 5 -->\n      <g transform=\"translate(7.2 122.757344)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m01daac12c8\" y=\"94.798125\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 6 -->\n      <g transform=\"translate(7.2 98.597344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m01daac12c8\" y=\"70.638125\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 7 -->\n      <g transform=\"translate(7.2 74.437344)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m01daac12c8\" y=\"46.478125\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 8 -->\n      <g transform=\"translate(7.2 50.277344)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m01daac12c8\" y=\"22.318125\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 9 -->\n      <g transform=\"translate(7.2 26.117344)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-57\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p7b5b11da7d)\" d=\"M 27.479855 18.682153 \nL 28.889164 24.241269 \nL 31.238011 31.983636 \nL 33.586858 38.876767 \nL 35.935706 45.077154 \nL 38.049668 50.201698 \nL 41.338055 57.536785 \nL 43.217132 61.449995 \nL 48.384597 71.194675 \nL 51.672983 76.678448 \nL 54.02183 80.285377 \nL 57.545101 85.141265 \nL 59.189294 87.092945 \nL 62.712565 91.274915 \nL 64.591643 93.312963 \nL 69.759107 97.953167 \nL 72.342839 100.047677 \nL 77.745188 103.717611 \nL 78.449842 104.066022 \nL 81.268459 105.792317 \nL 82.207998 106.12786 \nL 82.442883 106.241151 \nL 83.147537 106.124208 \nL 85.731269 107.462151 \nL 92.073157 109.710078 \nL 92.777811 109.914841 \nL 93.952235 110.222286 \nL 94.656889 110.371636 \nL 95.361543 110.413328 \nL 97.005736 111.00952 \nL 98.884814 111.570194 \nL 99.354584 111.575159 \nL 99.824353 111.572797 \nL 100.763892 111.767676 \nL 101.468546 112.077217 \nL 103.347624 112.405157 \nL 104.756932 112.437068 \nL 106.401125 112.782899 \nL 107.810434 112.970312 \nL 110.159281 113.339426 \nL 110.629051 113.142876 \nL 114.387206 113.52851 \nL 115.326745 113.572886 \nL 116.501169 113.735923 \nL 117.205823 113.779113 \nL 117.440708 113.706937 \nL 117.910477 113.801335 \nL 119.084901 113.469468 \nL 119.789555 113.534661 \nL 120.259325 113.530549 \nL 123.312826 113.936758 \nL 123.782596 113.954372 \nL 124.252365 113.965754 \nL 124.722135 114.008046 \nL 125.426789 114.104252 \nL 133.647754 114.214698 \nL 134.587293 114.141475 \nL 135.291948 114.058378 \nL 136.701256 114.223511 \nL 138.110564 114.146532 \nL 138.815219 113.95125 \nL 139.284988 113.986629 \nL 139.989642 114.124355 \nL 141.86872 114.334096 \nL 142.33849 114.081235 \nL 143.043144 114.050556 \nL 147.036184 114.474057 \nL 147.505954 114.420222 \nL 147.740838 114.296263 \nL 149.150147 114.324107 \nL 150.089686 114.324372 \nL 151.029225 114.345628 \nL 151.264109 114.245389 \nL 152.203648 114.319995 \nL 152.908303 114.320455 \nL 154.78738 114.435279 \nL 157.605997 114.330847 \nL 160.189729 114.349084 \nL 160.894383 114.30367 \nL 164.18277 114.484667 \nL 164.887424 114.349279 \nL 165.357193 114.027918 \nL 165.826963 113.945801 \nL 165.826963 113.945801 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 20.5625 239.758125 \nL 20.5625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 172.744318 239.758125 \nL 172.744318 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 20.5625 239.758125 \nL 172.744318 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 20.5625 22.318125 \nL 172.744318 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_15\">\n    <!-- Norm of parameters -->\n    <g transform=\"translate(35.888409 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"74.804688\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"135.986328\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"175.349609\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"272.761719\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"304.548828\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"365.730469\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"400.935547\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"432.722656\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"496.199219\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"557.478516\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"598.591797\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"659.871094\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"757.283203\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"818.806641\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"858.015625\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"919.539062\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"960.652344\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 203.180682 239.758125 \nL 355.3625 239.758125 \nL 355.3625 22.318125 \nL 203.180682 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_5\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"210.098037\" xlink:href=\"#mc2ccd245fa\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0 -->\n      <g transform=\"translate(206.916787 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"257.074984\" xlink:href=\"#mc2ccd245fa\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 200 -->\n      <g transform=\"translate(247.531234 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"304.05193\" xlink:href=\"#mc2ccd245fa\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 400 -->\n      <g transform=\"translate(294.50818 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"351.028877\" xlink:href=\"#mc2ccd245fa\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 600 -->\n      <g transform=\"translate(341.485127 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_11\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#m01daac12c8\" y=\"239.758125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#m01daac12c8\" y=\"215.598125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#m01daac12c8\" y=\"191.438125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#m01daac12c8\" y=\"167.278125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#m01daac12c8\" y=\"143.118125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#m01daac12c8\" y=\"118.958125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#m01daac12c8\" y=\"94.798125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#m01daac12c8\" y=\"70.638125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#m01daac12c8\" y=\"46.478125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"203.180682\" xlink:href=\"#m01daac12c8\" y=\"22.318125\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_30\">\n    <path clip-path=\"url(#pd464c7cca1)\" d=\"M 210.098037 157.473499 \nL 210.567807 188.096487 \nL 211.037576 198.957178 \nL 211.272461 209.767459 \nL 211.74223 204.312199 \nL 212.212 210.840859 \nL 212.681769 215.390749 \nL 213.151539 213.774347 \nL 213.386423 210.901586 \nL 213.621308 206.528433 \nL 213.856193 212.023021 \nL 214.091078 211.518322 \nL 214.560847 216.905767 \nL 214.795732 212.848612 \nL 215.030617 197.741084 \nL 215.500386 210.247335 \nL 215.735271 208.516515 \nL 215.970156 214.911259 \nL 216.20504 217.616636 \nL 216.439925 218.600283 \nL 217.144579 209.776474 \nL 217.379464 192.309827 \nL 218.084118 210.663454 \nL 218.553888 217.366727 \nL 219.023657 215.867764 \nL 219.258542 213.004333 \nL 219.493426 194.472842 \nL 219.728311 197.51499 \nL 220.198081 217.217995 \nL 220.432965 218.31728 \nL 220.66785 188.857386 \nL 220.902735 206.316487 \nL 221.13762 207.122756 \nL 221.372504 205.387285 \nL 221.607389 200.949407 \nL 221.842274 207.329115 \nL 222.077159 207.075741 \nL 222.312043 206.050885 \nL 222.546928 202.140647 \nL 222.781813 205.969698 \nL 223.016697 211.688581 \nL 223.251582 211.495341 \nL 223.486467 210.586988 \nL 223.721352 215.566015 \nL 223.956236 211.63004 \nL 224.191121 203.66988 \nL 224.426006 206.054851 \nL 224.660891 204.960834 \nL 224.895775 204.650513 \nL 225.13066 203.860231 \nL 225.365545 206.772741 \nL 225.60043 211.642592 \nL 225.835314 212.943068 \nL 226.070199 209.421254 \nL 226.305084 209.143262 \nL 226.539968 212.509861 \nL 226.774853 212.995661 \nL 227.479507 198.397656 \nL 227.714392 197.043711 \nL 228.184162 205.759603 \nL 228.419046 206.012153 \nL 228.653931 211.532198 \nL 228.888816 205.976817 \nL 229.123701 207.335229 \nL 229.358585 207.4226 \nL 229.59347 206.58232 \nL 229.828355 203.845819 \nL 230.063239 204.955388 \nL 230.298124 208.899703 \nL 230.533009 209.122994 \nL 230.767894 202.272959 \nL 231.002778 203.870787 \nL 231.237663 198.139032 \nL 231.472548 197.53308 \nL 231.707433 198.895585 \nL 231.942317 197.29231 \nL 232.412087 204.551409 \nL 232.646972 208.873725 \nL 232.881856 205.15137 \nL 233.116741 196.58164 \nL 233.351626 192.530548 \nL 233.58651 202.744335 \nL 233.821395 208.088354 \nL 234.05628 208.324289 \nL 234.291165 210.095217 \nL 235.230704 196.637799 \nL 235.465588 202.187336 \nL 235.700473 204.927209 \nL 235.935358 211.338419 \nL 236.170242 212.314285 \nL 236.405127 199.648321 \nL 236.640012 201.462039 \nL 236.874897 211.070556 \nL 237.344666 207.570596 \nL 237.579551 179.793613 \nL 237.814436 175.615601 \nL 238.04932 190.645586 \nL 238.284205 212.808253 \nL 238.753975 207.307387 \nL 238.988859 213.923346 \nL 239.223744 213.789301 \nL 239.458629 209.494031 \nL 240.163283 216.80603 \nL 240.867937 201.143367 \nL 241.337707 180.723832 \nL 241.807476 204.210526 \nL 242.042361 202.688579 \nL 242.277246 199.343851 \nL 242.51213 201.841422 \nL 242.9819 211.132746 \nL 243.451669 205.094471 \nL 243.686554 206.166587 \nL 243.921439 209.613596 \nL 244.156323 208.952997 \nL 244.391208 202.652532 \nL 244.626093 203.626126 \nL 244.860978 203.732612 \nL 245.095862 192.657978 \nL 245.330747 193.196182 \nL 245.800517 214.060427 \nL 246.035401 212.756178 \nL 246.270286 212.509397 \nL 246.505171 213.483889 \nL 246.740055 213.717644 \nL 247.209825 201.663798 \nL 247.44471 203.001096 \nL 247.679594 205.655542 \nL 247.914479 200.910673 \nL 248.149364 200.166897 \nL 248.384249 200.918443 \nL 248.854018 208.079509 \nL 249.558672 202.050468 \nL 249.793557 204.520652 \nL 250.028442 211.229557 \nL 250.263326 211.251797 \nL 250.498211 208.06074 \nL 250.967981 185.114188 \nL 251.202865 189.279285 \nL 251.43775 198.212982 \nL 251.672635 201.723451 \nL 251.90752 194.060073 \nL 252.142404 191.675687 \nL 252.377289 201.32602 \nL 253.081943 213.025165 \nL 253.551713 201.770868 \nL 253.786597 206.227895 \nL 254.021482 207.172368 \nL 254.491252 193.45061 \nL 254.961021 177.160467 \nL 255.195906 178.197008 \nL 255.430791 186.414206 \nL 255.665675 202.221022 \nL 255.90056 205.456263 \nL 256.135445 205.020041 \nL 256.370329 198.772288 \nL 256.840099 204.271489 \nL 257.074984 193.743233 \nL 257.544753 211.767268 \nL 257.779638 199.023202 \nL 258.014523 199.92032 \nL 258.484292 204.004619 \nL 258.719177 198.384644 \nL 259.188946 201.954195 \nL 259.423831 204.448174 \nL 259.658716 197.934275 \nL 259.8936 198.456943 \nL 260.128485 195.518528 \nL 260.36337 182.691266 \nL 260.598255 181.332385 \nL 261.068024 195.105697 \nL 261.772678 211.797149 \nL 262.007563 212.795011 \nL 262.242448 210.787313 \nL 262.477333 212.754612 \nL 262.947102 202.229193 \nL 263.181987 209.347141 \nL 263.416871 210.452712 \nL 263.651756 201.242397 \nL 263.886641 181.475831 \nL 264.121526 175.849436 \nL 264.82618 199.197917 \nL 265.061065 198.231247 \nL 265.295949 151.915339 \nL 265.530834 131.329579 \nL 266.000604 210.209418 \nL 266.235488 209.122001 \nL 266.470373 211.708598 \nL 266.705258 212.77203 \nL 266.940142 207.006383 \nL 267.644797 203.495505 \nL 267.879681 205.074359 \nL 268.114566 200.600717 \nL 268.349451 199.666313 \nL 268.81922 176.396026 \nL 269.28899 200.63891 \nL 269.523875 193.275794 \nL 269.758759 197.865651 \nL 269.993644 192.29127 \nL 270.228529 195.132042 \nL 270.463413 178.259841 \nL 270.698298 192.373301 \nL 270.933183 184.423218 \nL 271.402952 203.907229 \nL 271.637837 205.626238 \nL 271.872722 209.037084 \nL 272.107607 210.565205 \nL 272.342491 198.806284 \nL 272.577376 198.540356 \nL 273.047145 210.831171 \nL 273.28203 208.402365 \nL 273.516915 201.600626 \nL 274.221569 212.358607 \nL 274.456454 207.856181 \nL 274.691339 211.641667 \nL 274.926223 206.611458 \nL 275.161108 198.955488 \nL 275.395993 183.601187 \nL 275.865762 196.415404 \nL 276.100647 195.481628 \nL 276.335532 205.789902 \nL 276.570416 207.280749 \nL 277.040186 191.498397 \nL 277.275071 189.24901 \nL 277.509955 185.045354 \nL 277.74484 175.022768 \nL 277.979725 176.383613 \nL 278.21461 195.468267 \nL 278.684379 209.197563 \nL 279.154149 205.9808 \nL 279.389033 197.970501 \nL 279.623918 203.565629 \nL 279.858803 203.899479 \nL 280.093687 205.935136 \nL 280.328572 209.3725 \nL 280.563457 195.065624 \nL 280.798342 193.280229 \nL 281.268111 210.142834 \nL 281.502996 198.500182 \nL 281.737881 173.055777 \nL 281.972765 188.218586 \nL 282.20765 193.499819 \nL 282.442535 184.970033 \nL 282.67742 181.69219 \nL 282.912304 196.213593 \nL 283.382074 204.184516 \nL 283.616958 199.400039 \nL 283.851843 206.654475 \nL 284.086728 209.493306 \nL 284.321613 205.673347 \nL 284.556497 209.092693 \nL 284.791382 204.750829 \nL 285.261152 190.493926 \nL 285.496036 179.756051 \nL 285.730921 191.088902 \nL 285.965806 195.152554 \nL 286.200691 182.806718 \nL 286.67046 203.09918 \nL 287.140229 172.396373 \nL 287.375114 180.605253 \nL 287.609999 199.561647 \nL 287.844884 199.288476 \nL 288.079768 198.59045 \nL 288.314653 201.259778 \nL 288.549538 207.598962 \nL 288.784423 208.961769 \nL 289.254192 193.365157 \nL 289.723962 205.312091 \nL 289.958846 206.206476 \nL 290.193731 201.146371 \nL 290.428616 205.985786 \nL 290.6635 203.680712 \nL 290.898385 192.34141 \nL 291.368155 187.269122 \nL 291.837924 209.213285 \nL 292.072809 203.905446 \nL 292.307694 201.434133 \nL 292.542578 193.553671 \nL 292.777463 198.15239 \nL 293.012348 170.297232 \nL 293.247232 171.994093 \nL 293.482117 191.742116 \nL 293.951887 203.499675 \nL 294.186771 199.003516 \nL 294.421656 188.769318 \nL 294.656541 188.118486 \nL 295.361195 207.351513 \nL 295.59608 197.541997 \nL 295.830965 193.662651 \nL 296.065849 192.057156 \nL 296.300734 185.584991 \nL 296.535619 186.23674 \nL 296.770503 197.447484 \nL 297.240273 206.20404 \nL 297.475158 200.280796 \nL 297.710042 200.842556 \nL 297.944927 184.593155 \nL 298.179812 184.688412 \nL 298.414697 193.571577 \nL 298.649581 212.465467 \nL 298.884466 215.315259 \nL 299.354236 202.987545 \nL 299.58912 205.075168 \nL 300.05889 169.745936 \nL 300.528659 197.073422 \nL 300.763544 187.188059 \nL 300.998429 152.11345 \nL 301.233313 167.095677 \nL 301.468198 191.171561 \nL 301.937968 203.563835 \nL 302.172852 196.954302 \nL 302.407737 195.607422 \nL 302.642622 176.312406 \nL 302.877507 173.730307 \nL 303.112391 199.805093 \nL 303.347276 210.352729 \nL 303.582161 210.506374 \nL 303.817045 211.22219 \nL 304.05193 206.113147 \nL 304.286815 209.059992 \nL 304.5217 204.634635 \nL 304.756584 192.748416 \nL 305.461239 209.938816 \nL 305.931008 179.852488 \nL 306.400778 201.719375 \nL 306.635662 202.267383 \nL 306.870547 185.215993 \nL 307.340316 202.426882 \nL 307.575201 195.27236 \nL 307.810086 192.486126 \nL 308.279855 204.832094 \nL 308.51474 198.977434 \nL 308.749625 185.917487 \nL 308.98451 182.65424 \nL 309.454279 203.277821 \nL 309.689164 206.183173 \nL 309.924048 205.380018 \nL 310.158933 192.569243 \nL 310.393818 193.266102 \nL 310.628703 181.406202 \nL 310.863587 177.499317 \nL 311.568242 203.974165 \nL 311.803126 210.342977 \nL 312.038011 202.232701 \nL 312.272896 199.20677 \nL 312.507781 199.102007 \nL 312.742665 195.835776 \nL 312.97755 182.750043 \nL 313.212435 182.956926 \nL 313.682204 206.438851 \nL 313.917089 208.502938 \nL 314.151974 203.954846 \nL 314.386858 203.025703 \nL 314.621743 205.650868 \nL 315.326397 182.027479 \nL 315.796167 202.936363 \nL 316.031052 194.400477 \nL 316.265936 190.04686 \nL 316.500821 175.091948 \nL 316.735706 189.50747 \nL 316.97059 185.610676 \nL 317.44036 169.654285 \nL 317.675245 181.222636 \nL 317.910129 198.644129 \nL 318.145014 205.447864 \nL 318.379899 201.743499 \nL 318.614784 202.972191 \nL 318.849668 209.096604 \nL 319.084553 204.570725 \nL 319.319438 197.129118 \nL 319.554323 197.19161 \nL 319.789207 198.361557 \nL 320.258977 179.771597 \nL 320.493861 188.40975 \nL 320.963631 158.646816 \nL 321.668285 205.578428 \nL 321.90317 208.354654 \nL 322.138055 203.831972 \nL 322.842709 211.297375 \nL 323.077594 204.034086 \nL 323.312478 200.424495 \nL 323.547363 199.115803 \nL 323.782248 198.828735 \nL 324.017132 192.254563 \nL 324.252017 197.31963 \nL 324.486902 198.348847 \nL 324.721787 155.381714 \nL 324.956671 163.205715 \nL 325.191556 190.987626 \nL 325.426441 199.440257 \nL 325.661326 200.139251 \nL 325.89621 192.046612 \nL 326.36598 209.781266 \nL 326.600865 210.191124 \nL 326.835749 211.532374 \nL 327.070634 203.618542 \nL 327.305519 211.101131 \nL 327.540403 211.083729 \nL 327.775288 205.518951 \nL 328.010173 203.663987 \nL 328.245058 202.833379 \nL 328.479942 197.680829 \nL 328.714827 182.543581 \nL 328.949712 192.520353 \nL 329.184597 194.18233 \nL 329.419481 203.940125 \nL 329.654366 201.64239 \nL 329.889251 195.931291 \nL 330.124135 171.339689 \nL 330.35902 163.355906 \nL 331.063674 208.213186 \nL 331.298559 201.48806 \nL 331.533444 187.195207 \nL 331.768329 188.622203 \nL 332.003213 196.682665 \nL 332.238098 199.33541 \nL 332.472983 192.684812 \nL 332.707868 196.733424 \nL 332.942752 193.202518 \nL 333.177637 187.893642 \nL 333.412522 190.352991 \nL 333.647406 198.991593 \nL 333.882291 173.338654 \nL 334.117176 194.901208 \nL 334.352061 199.719007 \nL 334.586945 200.225677 \nL 334.82183 193.80114 \nL 335.056715 198.41477 \nL 335.2916 190.91336 \nL 335.526484 208.279466 \nL 335.761369 212.382342 \nL 335.996254 209.858136 \nL 336.231139 208.742237 \nL 336.466023 197.275271 \nL 336.700908 194.015987 \nL 336.935793 193.431785 \nL 337.170677 182.552175 \nL 337.405562 188.148888 \nL 337.640447 173.992678 \nL 337.875332 173.592506 \nL 338.345101 197.078857 \nL 338.579986 194.711654 \nL 338.814871 188.791518 \nL 339.049755 187.255292 \nL 339.519525 190.729656 \nL 339.75441 174.600016 \nL 340.459064 211.098997 \nL 340.693948 208.224096 \nL 340.928833 212.029006 \nL 341.163718 211.048025 \nL 341.398603 199.43151 \nL 341.633487 200.262208 \nL 342.103257 189.098922 \nL 342.338142 187.346816 \nL 342.573026 189.901271 \nL 342.807911 186.549922 \nL 343.042796 188.886256 \nL 343.277681 194.005406 \nL 343.512565 204.45377 \nL 343.74745 190.249348 \nL 343.982335 188.120813 \nL 344.452104 204.818373 \nL 344.686989 212.293223 \nL 345.391643 205.333153 \nL 345.626528 192.616525 \nL 345.861413 191.887149 \nL 346.096297 201.398308 \nL 346.331182 191.235533 \nL 346.566067 202.455588 \nL 347.035836 195.676198 \nL 347.270721 188.656263 \nL 347.505606 172.000106 \nL 347.74049 142.91426 \nL 347.975375 150.149886 \nL 348.21026 174.900358 \nL 348.445145 170.166315 \nL 348.445145 170.166315 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 203.180682 239.758125 \nL 203.180682 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 355.3625 239.758125 \nL 355.3625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 203.180682 239.758125 \nL 355.3625 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 203.180682 22.318125 \nL 355.3625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_20\">\n    <!-- Norm of gradients -->\n    <g transform=\"translate(224.907841 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"74.804688\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"135.986328\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"175.349609\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"272.761719\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"304.548828\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"365.730469\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"400.935547\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"432.722656\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"496.199219\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"537.3125\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"598.591797\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"662.068359\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"689.851562\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"751.375\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"814.753906\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"853.962891\" xlink:href=\"#DejaVuSans-115\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p7b5b11da7d\">\n   <rect height=\"217.44\" width=\"152.181818\" x=\"20.5625\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"pd464c7cca1\">\n   <rect height=\"217.44\" width=\"152.181818\" x=\"203.180682\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plot_trajectory(trajectory_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HOMEWORK - Early stopping\n",
    "\n",
    "Early stopping is yet another example of regularization technique which relies a lot on practical and experimental observations rather than any supporting theory.\n",
    "\n",
    "It is based upon the concept of **validation**, which is an assessment mode **additional** to *testing*. Actually, what insofar whe have described as testing is technically a validation.\n",
    "* a validation dataset may be obtained as result of a random splitting of the original training dataset\n",
    "* a testing dataset should be obtained instead from a model deployed \"in the wild\" and should consist of data unseen (from both the model and its architect) during the training and designing phase.\n",
    "\n",
    "In a normal academic setting it's very hard to obtain a proper testing dataset, so usually the meaning of testing and validation get mixed up a little bit.\n",
    "\n",
    "Anyway, early stopping requires us to assess the model at each epoch to get a proxy for the testing performance(s) (**validation step**). That should gives us an idea of how the model **learns to generalize** (if it ever does...) during training.\n",
    "\n",
    "The *theoretical trend* ('90 s), which is pretty much absent in modern Deep Learning due to a lot of modern factors, is the following (figures from [4](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf)):\n",
    "\n",
    "![](img/generalization.jpg)\n",
    "\n",
    "Already in that period, different stuff was observed:\n",
    "\n",
    "![](img/generalization_ugly.jpg)\n",
    "\n",
    "In some of my experiments, this happens (blue=training, orange=validation):\n",
    "\n",
    "![](img/generalization_fmnist1.jpg)\n",
    "\n",
    "![](img/generalization_fmnist2.jpg)\n",
    "\n",
    "(red=training, blue=validation)\n",
    "\n",
    "![](img/plot_acc_09.png)\n",
    "\n",
    "As we observe the curves for training and validation performance, we may notice some trends:\n",
    "* there usually is an intersection between the two curves which marks the moment when the network starts to **overfit** the training data.\n",
    "    * it might happen that, after that moment, the validation performance stays roughly the same (_white noise_), or that it drops and never recovers\n",
    "    * it might happen, instead, that the validation performance stays a few points below the training performance but keeps on growing\n",
    "    * it might happen, eventually, that the validation performance peaks a few epochs after and then it decreases\n",
    "    * other situations may apply depending on the dataset, the optimizer, the presence of regularization, and a lot of other factors.\n",
    "    \n",
    "A trick which is very often applied is to track the validation performance during training and retain the model with the highest validation performance.\n",
    "**Note**: it may not be the best strategy as the validation dataset may not be representative of the data manifold (!).\n",
    "In the main reference for early stopping ([4](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf)), it is indicated as $E_{\\text{opt}}$.\n",
    "\n",
    "**Homework**: implement \"early stopping\" in the $E_{\\text{opt}}$ using the test data as validation (since we don't know yet how to create additional `DataLoaders` and operate random splitting).\n",
    "*Suggestion*: try training for more than 5 epochs, maybe 20-30 total would be fine. Use Colab GPUs in case you want to accelerate training.\n",
    "\n",
    "**Homework for the bravest ones**: read [4](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf) and try implementing at least one of the techniques there specified (besides $E_{\\text{opt}}$, of course). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "[1](https://arxiv.org/abs/1502.03167) Ioffe, S., & Szegedy, C. (2015, June). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (pp. 448-456). PMLR.\n",
    "\n",
    "[2](https://arxiv.org/abs/1805.11604) Santurkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). How does batch normalization help optimization?. arXiv preprint arXiv:1805.11604.\n",
    "\n",
    "[3](https://arxiv.org/abs/2002.10365) Frankle, J., Schwab, D. J., & Morcos, A. S. (2020). The early phase of neural network training. arXiv preprint arXiv:2002.10365.\n",
    "\n",
    "[4](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf) Prechelt, L. (1998). Early stopping-but when?. In Neural Networks: Tricks of the trade (pp. 55-69). Springer, Berlin, Heidelberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homework recap\n",
    "\n",
    "1. Implement L1 norm regularization as a custom loss function\n",
    "2. Implement early stopping in the $E_{\\text{opt}}$ specification\n",
    "3. Implement early stopping in one of the additional specifications as of [4](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}