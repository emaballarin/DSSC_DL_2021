{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "0b44c525ca95e5dbf893da2282eb3ec3f420cb9fa59d94f9af90ca833dc1a37c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Learning course - LAB 5\n",
    "\n",
    "## Pruning in Artificial Neural Networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recap from previous Labs\n",
    "\n",
    "* Essentially, we now have a bag of _tricks_ to utilize in order to ensure our Multilayer Perceptrons (MLPs) train well while preserving a good generalization\n",
    "* We know how to construct a simple MLP with a custom number of layers and neurons\n",
    "* We know how to train the MLP using popular variants of Stochastic Gradient Descent (SGD) such as SGD with momentum and Adam\n",
    "* We learned some tricks to enforce a better generalization capability of our MLP, namely Weight Decay and Dropout\n",
    "* We saw how to use proper learning rate schedules for better convergence of our network\n",
    "\n",
    "### Agenda for today\n",
    "\n",
    "* Today, we are going more into the detail of Deep Learning (DL) for a more technical lab session devoted to a more \"experimental\" branch: **network pruning**\n",
    "* We will see how to implement a specific pruning algorithm, Magnitude Pruning (MP), in an MLP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## What is pruning?\n",
    "\n",
    "Pruning is referred to the specific act of **removing parameters from a Machine learning model**.\n",
    "\n",
    "The term originated from tree-based models that, when grown too deep, present pronounced tendencies towards overfitting, so their branches need to be cut (*pruned*) with proper criteria in order to ensure a better generalization.\n",
    "\n",
    "The main difference between pruning tree-based models and Artificial Neural Networks (ANNs) lies in the fact that, while in tree-based models we operate pruning on _leaf parameters_, in ANNs we usually prune without regard for the position of the parameter, which is a connection (_synapse_) between neurons.\n",
    "\n",
    "![](https://www.cs.cmu.edu/~bhiksha/courses/10-601/decisiontrees/DTprune.png)\n",
    "\n",
    "*In decision trees, models are pruned _downside up_ starting from the leaves. Picture from [Carnegie Mellon University](https://www.cs.cmu.edu/~bhiksha/courses/10-601/decisiontrees/).*\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/2/23/Before_after_pruning.png)\n",
    "\n",
    "*In ANNs, generically, pruning is operated regardless of the position of the neuron/connection within the network. Picture from [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Before_after_pruning.png).*\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Why pruning?\n",
    "\n",
    "Pruning (also called _Weight Sparsification_) is a collection of techniques falling within the category of **Network Miniaturization/Compression**, a bulk of practices aimed at reducing the size, or memory impact, or energetic impact, of a well-performing ANN, while still allowing the _reduced_ model to perform on-par with the original network.\n",
    "\n",
    "Some other examples of Network Compression include, but not limited to:\n",
    "* Knowledge Distillation\n",
    "* Weight Quantization\n",
    "* Weight Sharing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## How to prune?\n",
    "\n",
    "Pruning in ANNs is a theme that has been around since the early works of Yann LeCun [2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.7223&rep=rep1&type=pdf) in the late 80s. A huge number of techniques has been proposed since then [3](https://arxiv.org/abs/2102.00554).\n",
    "\n",
    "The findings in these early papers are essentialy different from those in the most recent ones (i.e., from 2015 on) mainly because the networks back then were not deep: large ANNs tend instead to react differently to pruning.\n",
    "\n",
    "Regarding how to prune, we can distinguish between two main categories: **Structured Pruning** and **Unstructured Pruning**.\n",
    "\n",
    "### Structured pruning\n",
    "\n",
    "To render this classification as neat as possible, structured pruning acts on multiple synapses logically connected in some way. In the case of MLPs, we may think of the act of _removing a neuron_ from a given layer as a structured pruning technique, as we **remove all the connections incoming towards that neuron**.\n",
    "\n",
    "![](img/struct_prune.png)\n",
    "\n",
    "In the image above, we prune the first neuron of the second layer. This equates to removing all of its incoming connections. The effect of this on the parameters of the ANN is that now we have a so called *regular sparsity*: the original parameters $\\Theta\\in\\mathbb{R}^{4\\times 3}$ are now $\\tilde\\theta\\in\\mathbb{R}^{4\\times 2}$.\n",
    "\n",
    "### Unstructured pruning\n",
    "\n",
    "In this case, instead, we utilize a pruning technique which has no regard for the geometry or the structure of the layers themselves.\n",
    "\n",
    "![](img/unstr_prune.png)\n",
    "\n",
    "In the image above, we removed four connections. This leads to an *irregular* form of sparsity in the parameters, which can't be taken advantage of directly, as happened instead in the structured case.\n",
    "There exist algorithms for sparse linear algebra which can be used in these cases (implemented for example in CuSPARSE) or some specific processor architectures are designed to take advantage of sparsity in matrices (e.g., NVIDIA Tesla A100 GPU).\n",
    "\n",
    "In this lab we will deal with unstructured sparsity, but an extension to structured sparsity can be easily derived."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## (Least) Magnitude pruning\n",
    "\n",
    "MP is one of the most straight-forward pruning techniques. Essentially, we remove a fixed fraction of parameters exhibiting a low absolute value.\n",
    "\n",
    "To get more technical:\n",
    "\n",
    "* Let us call $\\Theta$ the *structure* holding all of the parameters in the network.\n",
    "\n",
    "* Let us also introduce $p\\in(0,1)$, the **pruning rate**. This is the fixed fraction of parameters with small magnitude that we will prune.\n",
    "\n",
    "1. We obtain $\\vec\\Theta = \\text{vec}\\vert\\Theta\\vert$, the **empirical distribution of the parameters in magnitude**\n",
    "\n",
    "2. We sort $\\vec\\Theta$, obtaining $\\vec\\Theta_\\text{sort}$\n",
    "\n",
    "3. We obtain $\\theta_p$ the $p$-th percentile of $\\vec\\Theta_\\text{sort}$, which will act as a threshold for determining which parameters are going to be pruned.\n",
    "\n",
    "4. For each of the parameter $\\theta$ in the original $\\Theta$, we operate the pruning: $\\tilde\\theta = \\theta \\cdot \\mathbb{1}[\\vert\\theta\\vert\\geq\\theta_p]$ (NB: $\\mathbb{1}$ is the *indicator function*, which evaluates to 1 if the condition inside the brackets is verified, to 0 otherwise).\n",
    "\n",
    "5. We replace $\\Theta$ with $\\tilde\\Theta$, the structure composed of the various $\\tilde\\theta$s.\n",
    "\n",
    "#### Masking\n",
    "\n",
    "The same procedure described above can be obtained with masking. Essentially, instead of directly building $\\tilde\\Theta$ as element-wise application of $\\tilde\\theta = \\theta \\cdot \\mathbb{1}[\\vert\\theta\\vert\\geq\\theta_p]$, we rather do this:\n",
    "\n",
    "4. $m = \\mathbb{1}[\\vert\\theta\\vert\\geq\\theta_p]$. $m$ is a boolean telling us whether the corresponding parameters needs to be pruned or not. We have as many $m$s as there are $\\theta$s.\n",
    "\n",
    "5. We compose the $m$s in a structure $M$, identical in shape to $\\Theta$.\n",
    "\n",
    "6. Now, $\\tilde\\Theta = M\\odot \\Theta$, where $\\odot$ is the Hadamard product.\n",
    "\n",
    "This is the way in which pruning (analogously to Dropout) will be operated by us on PyTorch.\n",
    "\n",
    "#### Global vs Local\n",
    "\n",
    "The current version of MP operates on a global logic: we pool all of the parameters and prune w.r.t. one single threshold $\\theta_p$.\n",
    "If we wanted to complicate things just a little bit, we could instead pool parameters in different buckets and prune buckets separately (local approach):\n",
    "\n",
    "0. Divide the parameters in buckets $\\Theta_b, b\\in\\{1,...,B\\},~~\\cup_b\\Theta_b=\\Theta,~\\Theta_b\\cap\\Theta_{b^\\prime} = \\varnothing$.\n",
    "\n",
    "All the following points 1.-6. are operated separately for each bucket, so we will have a thresold for each bucket, a mask for each bucket...\n",
    "\n",
    "This strategy might be operated if we wanted to operate pruning separately for some specific layers. For example, [4](https://arxiv.org/abs/1803.03635) specifies that the reached better results when operating lower pruning rates on output layers. Nonetheless, this practice has fallen off, since in Deep ANNs, a global approach has been empirically proven more effective."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Perchance, did we already meet magnitude pruning somewhere???\n",
    "\n",
    "Indeed, L1-norm regularization is a form of *implicit pruning* operated during the training of the network. Although in theory it seems to work, in practice it doesn't work as well as MP, and some works (see [3](https://arxiv.org/abs/2102.00554)) have shown that L1-norm regularization does not work well with normalization layers, which play a fundamental role for faster convergence of SGD."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scripts import mnist, train_utils, architectures, train"
   ]
  },
  {
   "source": [
    "For the point 4., we'll be using `torch.where`. Let's see first what it is and how does it work:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'tuple'>\n(tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4]), tensor([6, 7, 8, 9, 3, 4, 5, 6, 0, 1, 2, 4, 7, 3, 6, 7, 9, 1, 7, 9]))\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((5,10)) # random tensor\n",
    "y = torch.where(x<.5)\n",
    "print(type(y))\n",
    "print(y)"
   ]
  },
  {
   "source": [
    "It returns a tuple of length `d`, where `d` is the number of dimensions of the tensor `x`.\n",
    "\n",
    "But we can use it differently:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 1, 1, 0, 1],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "torch.where(x<.5, 1, 0)"
   ]
  },
  {
   "source": [
    "by specifying two following arguments, we actually create a tensor with the same structure as `x`, where the element of `x` abiding to the condition in `torch.where` take on the value reported as the second argument; those who do not, assume the third argument instead. I hope it is straightforward how this method will be integrated in the pruning fct."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will create the function for obaining a mask from a single tensor of parameters\n",
    "def binarize(tensor, threshold):\n",
    "    return torch.where(..., ..., ...) # fill in the blanks & be wary of the MAGNITUDE\n",
    "\n",
    "def magnitude_pruning(model, pruning_rate):\n",
    "    # 1. vectorize distribution of abs(parameter)\n",
    "    flat = ...\n",
    "    # 2. sort this distribution\n",
    "    flat = flat.sort()[0]\n",
    "    # 3. obtain the threshold\n",
    "    position = ...\n",
    "    thresh = ...\n",
    "    # 4. binarize the parameters & 5. compose these booleans into the mask\n",
    "    mask = ...\n",
    "    # 6. obtain the new structure of parameters\n",
    "    for param, m in zip(parameters, mask):\n",
    "        param.data *= m\n",
    "    # 7. what do we need to return?\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4]),\n",
       " tensor([0, 1, 4, 6, 7, 8, 9, 1, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 4, 5, 7, 9, 1, 2,\n",
       "         3, 4, 6, 7, 8, 9, 1, 3, 4, 5, 7, 9]))"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "torch.where((x<.5)|(x>.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = next(net.parameters())\n",
    "kk = next(net.parameters())\n",
    "w = torch.cat((k.flatten(),kk.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def magnitude_pruning(model, p):\n",
    "    flat = torch.cat([param.detach().cpu().flatten().abs() for param in model.parameters()])\n",
    "    flat = flat.sort()[0]\n",
    "    position = int(p * flat.shape[0])\n",
    "    thresh = flat[position].item()\n",
    "    mask = [torch.where(param.abs()>=thresh, 1, 0) for param in model.parameters()]\n",
    "    for param, m in zip(model.parameters(), mask):\n",
    "        param.data *= m\n",
    "    return mask"
   ]
  },
  {
   "source": [
    "Let us see our algorithm at work:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MLPCustom(\n  (layers): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=784, out_features=16, bias=True)\n    (2): ReLU()\n    (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): Linear(in_features=16, out_features=32, bias=True)\n    (5): ReLU()\n    (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Linear(in_features=32, out_features=64, bias=True)\n    (8): ReLU()\n    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): Linear(in_features=64, out_features=10, bias=True)\n    (11): ReLU()\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    {\"n_in\": 784, \"n_out\": 16, \"batchnorm\": False},\n",
    "    {\"n_out\": 32, \"batchnorm\": True},\n",
    "    {\"n_out\": 64, \"batchnorm\": True},\n",
    "    {\"n_out\": 10, \"batchnorm\": True}\n",
    "]\n",
    "net = architectures.MLPCustom(layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = magnitude_pruning(net, .2)"
   ]
  },
  {
   "source": [
    "Did it work? Let's analyze the mask"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of ones in mask: 0.8000079622588928 \n\ntensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) \n tensor([-0.0169, -0.0196, -0.0118,  0.0160, -0.0095, -0.0098,  0.0144, -0.0000,\n        -0.0000,  0.0000,  0.0068, -0.0124, -0.0240, -0.0187, -0.0135, -0.0140,\n        -0.0167, -0.0137,  0.0092,  0.0329], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of ones in mask:\", sum([m.sum().item() for m in mask]) / sum([m.numel() for m in mask]), \"\\n\")\n",
    "print(mask[0][0,:20], \"\\n\", next(net.parameters())[0,:20])"
   ]
  },
  {
   "source": [
    "#### **Q**: What is wrong with this implementation?\n",
    "\n",
    "*Think about the type of layers we have in our MLP...*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct here the mistake\n",
    "def magnitude_pruning(model, pruning_rate):\n",
    "    # 1. vectorize distribution of abs(parameter)\n",
    "    flat = ...\n",
    "    # 2. sort this distribution\n",
    "    flat = flat.sort()[0]\n",
    "    # 3. obtain the threshold\n",
    "    position = ...\n",
    "    thresh = ...\n",
    "    # 4. binarize the parameters & 5. compose these booleans into the mask\n",
    "    mask = ...\n",
    "    # 6. obtain the new structure of parameters\n",
    "    for param, m in zip(parameters, mask):\n",
    "        param.data *= m\n",
    "    # 7. what do we need to return?\n",
    "    return None"
   ]
  },
  {
   "source": [
    "## When to prune?\n",
    "\n",
    "Usually, pruning is operated on a **fully trained model**.\n",
    "\n",
    "Let us see what happens when we prune a trained model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, testloader, _, _ = mnist.get_data()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss 7349.624454498291 - performance 0.9631333333333333\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7349.624454498291, 0.9631333333333333)"
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "# load pretrained model\n",
    "state_dict = torch.load(\"models_push/mlp_custom_mnist/mlp_custom_mnist.pt\")\n",
    "net.load_state_dict(state_dict)\n",
    "train.test_model(net, testloader, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TESTING - loss 8261.046932697296 - performance 0.9573333333333334\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8261.046932697296, 0.9573333333333334)"
      ]
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "train.test_model(net, testloader, loss_fn=loss_fn)"
   ]
  },
  {
   "source": [
    "We see that there's a drop in performance. This is a characteristic of pruning. Rarely a simple application of pruning leaves the performance untouched or better. Due to that, we must procede with a re-training of the ANN.\n",
    "\n",
    "## Retraining the network\n",
    "\n",
    "The retraining can be carried out in different ways:\n",
    "\n",
    "### Fine-tune (FT)\n",
    "\n",
    "This means that, after pruning, we retrain for a few epochs without touching the optimizer (i.e., we do not employ a LR schedule but leave the hyperparameters untouched w.r.t. their final configuration).\n",
    "\n",
    "### Learning Rate Rewind (LRR)\n",
    "\n",
    "After pruning, we re-train for the same number of epochs of the original training, resetting the optimizer hyperparameters as well\n",
    "\n",
    "### Weight Rewind (WR, a.k.a. _Lottery Ticket Hypothesis_)\n",
    "\n",
    "After pruning, we **reset** the surviving weights to their original configuration, then we re-train as in LRR. The idea behind the Lottery Ticket Hypothesis (LTH) is that, within a randomly initialized ANN, there lies a subnetwork (i.e., a subset of the ANN connections) who is actually responsible for the good performance of the dense ANN. It's hence assumed that this subnetwork has _won the lotter of initialization_.\n",
    "\n",
    "## Iterating the procedure\n",
    "\n",
    "Actually, all three procedures can be iterated (Iterative Magnitude Pruning, IMP). So, if we prune 25% of the ANN for each step, we can reach high pruning rates (e.g. 10 iterations of IMP $\\Rightarrow$ final sparsity of the ANN of about $0.75^{10}\\approx 0.0563$).\n",
    "\n",
    "Of course, IMP applied 10 times with a pruning rate of 0.25 yields far better result than a one-shot pruning + retraining with $p=0.95$.\n",
    "\n",
    "## Who wins and who doesn't?\n",
    "\n",
    "[5](https://arxiv.org/abs/2003.02389) does an excellent job in comparing these three paradigms, concluding that, as far as accuracy is involved, **LRR beats all of its competitors** on the long run. Despite that, a lot of recent publications still use a one-shot pruning followed by FT, this because LRR in the IMP setting can be very expensive in terms of time and energy. Moreover, we showed ([6](https://medvet.inginf.units.it/publications/2021-c-zmpa-speeding/)) that, at high pruning rates, we can \"cut the corners\" and re-train for less epochs while still getting good accuracy w.r.t. WR and LRR.\n",
    "\n",
    "## What about Weight Rewind?\n",
    "\n",
    "The LTH has sparked some debate in the ML community and has allowed for some interesting analysis, especially concerning the initial phase of training. In particular, the almost-defunct Uber AI has done some [additional research](https://eng.uber.com/deconstructing-lottery-tickets/) showing some interesting insights:\n",
    "* Random Pruning (RP) shows no benefit even after retraining. That confirms that re-training alone is not responsible for the effectiveness of the pruned network: you also need an _intelligent_ pruning algorithm;\n",
    "* Recalling the LTH, its application requires a parameters *reset* before retraining. As you can see from the image below, the sole application of the MP mask (_LT Mask_ in the picture) to a pruned and rewound network already increases the accuracy of the network by ~30%. This would seem to corroborate empirically the LTH.\n",
    "\n",
    "![](https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/image3-2-696x497.png)\n",
    "\n",
    "* Another heuristic is the so-called **supermask**, which keeps parameters which have mantained the sign during training and have a large final value. Parameters crossing the zero, despite large, are going to be pruned nonetheless. Of course, the supermask is more memory-intensive because we need to keep track of the evolution of the parameters in time.\n",
    "\n",
    "![](https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/image2-1-420x420.png)\n",
    "\n",
    "It's interesting to note how the supermask performs incredibly well after pruning and rewind (i.e., on untrained networks). The supermask is indicated as `large_final_same_sign`:\n",
    "\n",
    "![](https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/image1-3-685x420.png)\n",
    "\n",
    "*All images from [Uber AI](https://eng.uber.com/deconstructing-lottery-tickets/)*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Implementation of the retraining procedure\n",
    "\n",
    "If we retrain the network as-is, we will soon notice that we did a big mistake, because we have in no way communicated to PyTorch that the MLP does **not** have to be retrained on the pruned parameters.\n",
    "\n",
    "In order to enforce that, we must act on the training loop, namely `train_epoch`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.056313514709472656"
      ]
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, device, mask, params_type_to_prune):\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        y_hat = model(X)\n",
    "\n",
    "        loss = loss_fn(y_hat, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        ##### we must neutralize the gradient on the pruned params before the optimizer takes a step ####\n",
    "\n",
    "        if mask is not None:\n",
    "            for (name, param), m in zip(model.named_parameters(), mask):\n",
    "                if any([l in name for l in layers_to_prune]):\n",
    "                    param.grad *= m\n",
    "\n",
    "        ######\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = performance(y_hat, y)\n",
    "\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])"
   ]
  },
  {
   "source": [
    "let us integrate it with our `train_model` routine:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy, lr_scheduler=None, device=None, mask=None, params_type_to_prune=[\"weight\", \"bias\"]):\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "\n",
    "    if device is None:\n",
    "        device = use_gpu_if_possible()\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} --- learning rate {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, mask, params_type_to_prune)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg"
   ]
  },
  {
   "source": [
    "Now, we can effectively retrain the pruned ANN by simply passing the `mask` to the training routine."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Homework\n",
    "\n",
    "\"Close the loop\" for implementing IMP. Right now, the `magnitude_pruning` routine is thought for one-shot pruning. If you try pruning one more time, you'll notice that it will not work as there's no way to communicate to the future calls of `magnitude_pruning` to ignore the parameters which have already been pruned. Find a way to enhance the routine s.t. it can effectively prune networks in a sequential fashion (i.e., if we passed an MLP already pruned of 20% of its parameters, we want to prune *another* 20% of parameters).\n",
    "\n",
    "Hint:\n",
    "\n",
    "![](img/force.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.94450544 0.32553718 2.90253417 4.05758069]\n [1.8441957  0.18773161 2.39469929 2.06660618]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,2,3,4],[1,2,3,5]])\n",
    "y = np.random.chisquare(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqrtActivation(torch.nn.Module):\n",
    "    def __init__(self, n_in):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        w = torch.rand((n_in, ))\n",
    "        self.w = torch.nn.Parameter(w)\n",
    "        torch.nn.init.uniform_(self.w, 2.0, 8.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        s = x.sign()\n",
    "        return s * (x.abs() ** (1/self.w))\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableSqrt(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_, weight):\n",
    "        sign = input_.sign()\n",
    "        output = input_.abs() ** (1/weight)\n",
    "        return sign * output\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_, weight = ctx.saved_tensors\n",
    "        weight_1 = 1 / weight\n",
    "        sign_input = input_.sign()\n",
    "\n",
    "        grad_input = grad_output * weight_1 * (input_.abs() ** (weight_1 - 1))\n",
    "        grad_input *= sign_input\n",
    "        grad_weight = (input_.abs() ** weight_1) * input_.abs().log() * (-1 * weight_1 * weight_1)\n",
    "        \n",
    "        return grad_input, grad_weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = VariableSqrt.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'require_grad'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-260-b3c2c94e332d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'require_grad'"
     ]
    }
   ],
   "source": [
    "x = torch.rand((10,))\n",
    "x = x.require_grad()\n",
    "w = torch.Tensor([1,1,2,2,3,3,4,4,5,5], require_grad=True)\n",
    "y = sq(x, w)\n",
    "z = y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-257-40c0c9b0bbab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "mynet = FancyLayer(10)\n",
    "x = torch.rand((10,))\n",
    "h, s = mynet(x)\n",
    "z = h.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = np.linspace(0, 3, num=200)\n",
    "yyy = []\n",
    "yyy.append(xxx ** (1/2))\n",
    "yyy.append(xxx ** (1/2.5))\n",
    "yyy.append(xxx ** (1/3))\n",
    "yyy.append(xxx ** (1/4))\n",
    "yyy.append(xxx ** (1/8))\n",
    "xxxx = np.hstack((-np.flip(xxx), xxx))\n",
    "yyyy = [np.hstack((np.flip(y_), y_)) for y_ in yyy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.array([1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x138c8c96dc0>]"
      ]
     },
     "metadata": {},
     "execution_count": 248
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-04-01T20:07:42.730551</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"ma618e3ffdc\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#ma618e3ffdc\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- −3 -->\r\n      <g transform=\"translate(37.950213 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.59375 35.5 \r\nL 73.1875 35.5 \r\nL 73.1875 27.203125 \r\nL 10.59375 27.203125 \r\nz\r\n\" id=\"DejaVuSans-8722\"/>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.04858\" xlink:href=\"#ma618e3ffdc\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- −2 -->\r\n      <g transform=\"translate(88.677486 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.775852\" xlink:href=\"#ma618e3ffdc\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- −1 -->\r\n      <g transform=\"translate(139.404759 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"197.503125\" xlink:href=\"#ma618e3ffdc\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(194.321875 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.230398\" xlink:href=\"#ma618e3ffdc\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 1 -->\r\n      <g transform=\"translate(245.049148 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"298.95767\" xlink:href=\"#ma618e3ffdc\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(295.77642 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"349.684943\" xlink:href=\"#ma618e3ffdc\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 3 -->\r\n      <g transform=\"translate(346.503693 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"md01759f2ac\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md01759f2ac\" y=\"214.756364\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(7.2 218.555582)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md01759f2ac\" y=\"167.240796\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.5 -->\r\n      <g transform=\"translate(7.2 171.040015)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md01759f2ac\" y=\"119.725228\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(7.2 123.524447)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md01759f2ac\" y=\"72.209661\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 1.5 -->\r\n      <g transform=\"translate(7.2 76.008879)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md01759f2ac\" y=\"24.694093\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 2.0 -->\r\n      <g transform=\"translate(7.2 28.493312)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p1e6197eb6e)\" d=\"M 45.321307 17.083636 \r\nL 57.557031 27.826468 \r\nL 69.028022 38.186103 \r\nL 79.734281 48.13711 \r\nL 90.440539 58.39478 \r\nL 100.382065 68.230648 \r\nL 109.558858 77.612432 \r\nL 118.735651 87.326998 \r\nL 127.147712 96.569306 \r\nL 134.795039 105.296743 \r\nL 141.677634 113.460135 \r\nL 148.560229 121.967038 \r\nL 154.678091 129.870236 \r\nL 160.795953 138.160463 \r\nL 166.149082 145.801124 \r\nL 170.737479 152.704228 \r\nL 175.325875 160.015703 \r\nL 179.149539 166.50397 \r\nL 182.973203 173.462983 \r\nL 186.032134 179.48365 \r\nL 189.091065 186.072356 \r\nL 191.385263 191.559032 \r\nL 192.914728 195.607388 \r\nL 194.444194 200.142961 \r\nL 195.973659 205.550497 \r\nL 196.738392 208.957031 \r\nL 197.503125 214.756364 \r\nL 197.503125 214.756364 \r\nL 198.267858 208.957031 \r\nL 199.032591 205.550497 \r\nL 200.562056 200.142961 \r\nL 202.091522 195.607388 \r\nL 204.38572 189.664108 \r\nL 206.679918 184.35926 \r\nL 209.738849 177.932895 \r\nL 212.79778 172.026514 \r\nL 216.621444 165.172765 \r\nL 220.445108 158.76442 \r\nL 225.033504 151.527841 \r\nL 229.621901 144.684409 \r\nL 234.97503 137.100291 \r\nL 241.092892 128.862671 \r\nL 247.210754 121.002982 \r\nL 254.093349 112.537155 \r\nL 260.975944 104.408627 \r\nL 268.623271 95.714422 \r\nL 277.035331 86.503542 \r\nL 285.447392 77.612432 \r\nL 294.624185 68.230648 \r\nL 304.565711 58.39478 \r\nL 315.271969 48.13711 \r\nL 325.978228 38.186103 \r\nL 337.449219 27.826468 \r\nL 349.684943 17.083636 \r\nL 349.684943 17.083636 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p1e6197eb6e)\" d=\"M 45.321307 50.157609 \r\nL 56.792298 56.482604 \r\nL 67.498557 62.623 \r\nL 78.204815 69.021887 \r\nL 88.146341 75.226225 \r\nL 97.323134 81.208897 \r\nL 105.735194 86.938772 \r\nL 114.147255 92.937861 \r\nL 121.794582 98.660271 \r\nL 128.677177 104.063082 \r\nL 135.559772 109.743496 \r\nL 141.677634 115.064105 \r\nL 147.795496 120.685163 \r\nL 153.148625 125.894789 \r\nL 158.501755 131.429489 \r\nL 163.090151 136.484394 \r\nL 167.678548 141.889125 \r\nL 171.502211 146.720255 \r\nL 175.325875 151.921742 \r\nL 178.384806 156.415882 \r\nL 181.443737 161.286429 \r\nL 184.502668 166.64757 \r\nL 186.796866 171.098345 \r\nL 189.091065 176.057666 \r\nL 190.62053 179.752075 \r\nL 192.149996 183.885482 \r\nL 193.679461 188.665707 \r\nL 195.208927 194.546628 \r\nL 195.973659 198.255184 \r\nL 196.738392 203.088267 \r\nL 197.503125 214.756364 \r\nL 197.503125 214.756364 \r\nL 198.267858 203.088267 \r\nL 199.032591 198.255184 \r\nL 199.797323 194.546628 \r\nL 201.326789 188.665707 \r\nL 202.856254 183.885482 \r\nL 204.38572 179.752075 \r\nL 206.679918 174.336892 \r\nL 208.974116 169.566021 \r\nL 211.268315 165.252823 \r\nL 214.327246 160.028141 \r\nL 217.386177 155.260513 \r\nL 221.20984 149.791153 \r\nL 225.033504 144.747785 \r\nL 228.857168 140.044093 \r\nL 233.445564 134.763925 \r\nL 238.033961 129.81134 \r\nL 243.38709 124.375678 \r\nL 248.74022 119.248879 \r\nL 254.858082 113.707685 \r\nL 260.975944 108.454947 \r\nL 267.858538 102.839915 \r\nL 275.505866 96.914367 \r\nL 283.153194 91.272839 \r\nL 291.565254 85.350915 \r\nL 300.742047 79.185335 \r\nL 309.91884 73.288214 \r\nL 319.860366 67.165322 \r\nL 330.566624 60.843602 \r\nL 342.037616 54.346559 \r\nL 349.684943 50.157609 \r\nL 349.684943 50.157609 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p1e6197eb6e)\" d=\"M 45.321307 67.282717 \r\nL 57.557031 72.145167 \r\nL 69.028022 76.941233 \r\nL 79.734281 81.655315 \r\nL 89.675807 86.268877 \r\nL 98.8526 90.759965 \r\nL 107.26466 95.102702 \r\nL 115.67672 99.695746 \r\nL 123.324048 104.124079 \r\nL 130.206643 108.350329 \r\nL 137.089237 112.844668 \r\nL 143.207099 117.105401 \r\nL 148.560229 121.076731 \r\nL 153.913358 125.318147 \r\nL 158.501755 129.210058 \r\nL 163.090151 133.387511 \r\nL 166.913815 137.132155 \r\nL 170.737479 141.169481 \r\nL 174.561142 145.569811 \r\nL 177.620073 149.418861 \r\nL 180.679004 153.642145 \r\nL 182.973203 157.122912 \r\nL 185.267401 160.951544 \r\nL 187.561599 165.239796 \r\nL 189.855797 170.172963 \r\nL 191.385263 173.979935 \r\nL 192.914728 178.412283 \r\nL 194.444194 183.85361 \r\nL 195.208927 187.212701 \r\nL 195.973659 191.336455 \r\nL 196.738392 197.007392 \r\nL 197.503125 214.756364 \r\nL 197.503125 214.756364 \r\nL 198.267858 197.007392 \r\nL 199.032591 191.336455 \r\nL 199.797323 187.212701 \r\nL 201.326789 180.968464 \r\nL 202.856254 176.100765 \r\nL 204.38572 172.012853 \r\nL 205.915185 168.440447 \r\nL 208.209384 163.749995 \r\nL 210.503582 159.630836 \r\nL 212.79778 155.928214 \r\nL 215.856711 151.477644 \r\nL 218.915642 147.453057 \r\nL 221.974573 143.760478 \r\nL 225.798237 139.515481 \r\nL 229.621901 135.602357 \r\nL 234.210297 131.259592 \r\nL 238.798694 127.231651 \r\nL 244.151823 122.858562 \r\nL 249.504952 118.777246 \r\nL 255.622814 114.410696 \r\nL 261.740676 110.312025 \r\nL 268.623271 105.972027 \r\nL 276.270599 101.435969 \r\nL 283.917926 97.15711 \r\nL 292.329987 92.705217 \r\nL 301.50678 88.111158 \r\nL 311.448305 83.401035 \r\nL 321.389831 78.931526 \r\nL 332.09609 74.35276 \r\nL 343.567081 69.683382 \r\nL 349.684943 67.282717 \r\nL 349.684943 67.282717 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#p1e6197eb6e)\" d=\"M 45.321307 77.69775 \r\nL 58.321764 81.717322 \r\nL 70.557488 85.736076 \r\nL 82.028479 89.745537 \r\nL 92.734738 93.734965 \r\nL 102.676263 97.690788 \r\nL 111.853056 101.595917 \r\nL 120.265117 105.428943 \r\nL 127.912444 109.163207 \r\nL 134.795039 112.765815 \r\nL 140.912901 116.196699 \r\nL 147.030763 119.884677 \r\nL 152.383892 123.364827 \r\nL 157.737022 127.13236 \r\nL 162.325418 130.641143 \r\nL 166.149082 133.806412 \r\nL 169.972746 137.240699 \r\nL 173.79641 141.009672 \r\nL 176.855341 144.328708 \r\nL 179.914272 147.994069 \r\nL 182.20847 151.033007 \r\nL 184.502668 154.393252 \r\nL 186.796866 158.176141 \r\nL 189.091065 162.546462 \r\nL 190.62053 165.924555 \r\nL 192.149996 169.848611 \r\nL 193.679461 174.613165 \r\nL 194.444194 177.490719 \r\nL 195.208927 180.898278 \r\nL 195.973659 185.178602 \r\nL 196.738392 191.280478 \r\nL 197.503125 214.756364 \r\nL 197.503125 214.756364 \r\nL 198.267858 191.280478 \r\nL 199.032591 185.178602 \r\nL 199.797323 180.898278 \r\nL 200.562056 177.490719 \r\nL 202.091522 172.097849 \r\nL 203.620987 167.804593 \r\nL 205.150453 164.179102 \r\nL 206.679918 161.010003 \r\nL 208.974116 156.859852 \r\nL 211.268315 153.23214 \r\nL 213.562513 149.988176 \r\nL 216.621444 146.112459 \r\nL 219.680375 142.631007 \r\nL 222.739306 139.456656 \r\nL 226.56297 135.831015 \r\nL 230.386633 132.510993 \r\nL 234.97503 128.850923 \r\nL 239.563427 125.478688 \r\nL 244.916556 121.841355 \r\nL 250.269685 118.468473 \r\nL 256.387547 114.882415 \r\nL 263.270142 111.13369 \r\nL 270.152737 107.638185 \r\nL 277.800064 104.004321 \r\nL 286.212125 100.264496 \r\nL 295.388918 96.445316 \r\nL 305.330443 92.568422 \r\nL 316.036702 88.651295 \r\nL 327.507693 84.707982 \r\nL 339.743417 80.749732 \r\nL 349.684943 77.69775 \r\nL 349.684943 77.69775 \r\n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#p1e6197eb6e)\" d=\"M 45.321307 89.688356 \r\nL 59.851229 92.786907 \r\nL 72.851686 95.774748 \r\nL 85.08741 98.808621 \r\nL 96.558401 101.886904 \r\nL 106.499927 104.774847 \r\nL 115.67672 107.658969 \r\nL 124.088781 110.524436 \r\nL 131.736108 113.351783 \r\nL 138.618703 116.115778 \r\nL 144.736565 118.784211 \r\nL 150.854427 121.695855 \r\nL 156.207556 124.488866 \r\nL 160.795953 127.108109 \r\nL 165.384349 129.985758 \r\nL 169.208013 132.629849 \r\nL 173.031677 135.557225 \r\nL 176.090608 138.157475 \r\nL 179.149539 141.053261 \r\nL 181.443737 143.473065 \r\nL 183.737935 146.167898 \r\nL 186.032134 149.224008 \r\nL 188.326332 152.779689 \r\nL 189.855797 155.541196 \r\nL 191.385263 158.754116 \r\nL 192.914728 162.6404 \r\nL 193.679461 164.962542 \r\nL 194.444194 167.664274 \r\nL 195.208927 170.932236 \r\nL 195.973659 175.156794 \r\nL 196.738392 181.457228 \r\nL 197.503125 214.756364 \r\nL 197.503125 214.756364 \r\nL 198.267858 181.457228 \r\nL 199.032591 175.156794 \r\nL 199.797323 170.932236 \r\nL 200.562056 167.664274 \r\nL 202.091522 162.6404 \r\nL 203.620987 158.754116 \r\nL 205.150453 155.541196 \r\nL 206.679918 152.779689 \r\nL 208.974116 149.224008 \r\nL 211.268315 146.167898 \r\nL 213.562513 143.473065 \r\nL 216.621444 140.297232 \r\nL 219.680375 137.482529 \r\nL 222.739306 134.945603 \r\nL 226.56297 132.080477 \r\nL 230.386633 129.485615 \r\nL 234.97503 126.655131 \r\nL 239.563427 124.073832 \r\nL 244.916556 121.316782 \r\nL 251.034418 118.43836 \r\nL 257.15228 115.797064 \r\nL 264.034875 113.058279 \r\nL 271.682202 110.254052 \r\nL 280.094263 107.409613 \r\nL 289.271056 104.544518 \r\nL 299.212581 101.673741 \r\nL 309.91884 98.808621 \r\nL 322.154564 95.774748 \r\nL 335.155021 92.786907 \r\nL 349.684943 89.688356 \r\nL 349.684943 89.688356 \r\n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#p1e6197eb6e)\" d=\"M 45.321307 105.73639 \r\nL 65.204358 107.62782 \r\nL 82.793212 109.521208 \r\nL 98.087867 111.386877 \r\nL 111.088324 113.181968 \r\nL 122.559315 114.97425 \r\nL 132.500841 116.733631 \r\nL 141.677634 118.580787 \r\nL 149.324961 120.335709 \r\nL 156.207556 122.137668 \r\nL 162.325418 123.975529 \r\nL 167.678548 125.829594 \r\nL 172.266944 127.667288 \r\nL 176.090608 129.437671 \r\nL 179.149539 131.065925 \r\nL 182.20847 132.951676 \r\nL 184.502668 134.596761 \r\nL 186.796866 136.518777 \r\nL 188.326332 138.011893 \r\nL 189.855797 139.741135 \r\nL 191.385263 141.804618 \r\nL 192.914728 144.381374 \r\nL 193.679461 145.967095 \r\nL 194.444194 147.859318 \r\nL 195.208927 150.222213 \r\nL 195.973659 153.411502 \r\nL 196.738392 158.502878 \r\nL 197.503125 214.756364 \r\nL 197.503125 214.756364 \r\nL 198.267858 158.502878 \r\nL 199.032591 153.411502 \r\nL 199.797323 150.222213 \r\nL 200.562056 147.859318 \r\nL 201.326789 145.967095 \r\nL 202.856254 143.012181 \r\nL 204.38572 140.722613 \r\nL 205.915185 138.842075 \r\nL 207.444651 137.240184 \r\nL 209.738849 135.201921 \r\nL 212.033047 133.474502 \r\nL 215.091978 131.509972 \r\nL 218.150909 129.824645 \r\nL 221.974573 128.001629 \r\nL 225.798237 126.412853 \r\nL 230.386633 124.73761 \r\nL 235.739763 123.0244 \r\nL 241.857625 121.306658 \r\nL 248.74022 119.606364 \r\nL 256.387547 117.937322 \r\nL 265.56434 116.16856 \r\nL 276.270599 114.351652 \r\nL 287.74159 112.630747 \r\nL 301.50678 110.80222 \r\nL 316.801435 109.004017 \r\nL 334.390288 107.170287 \r\nL 349.684943 105.73639 \r\nL 349.684943 105.73639 \r\n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p1e6197eb6e\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(xxxx,yyyy[0])\n",
    "plt.plot(xxxx,yyyy[1])\n",
    "plt.plot(xxxx,yyyy[2])\n",
    "plt.plot(xxxx,yyyy[3])\n",
    "plt.plot(xxxx,yyyy[4])\n",
    "plt.plot(xxxx,yyyy[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "metadata": {},
     "execution_count": 222
    }
   ],
   "source": [
    "2**0.5\n"
   ]
  },
  {
   "source": [
    "### References\n",
    "\n",
    "[1](https://arxiv.org/abs/2006.03669) O'Neill, James. \"An Overview of Neural Network Compression.\"\n",
    "\n",
    "[2](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.7223&rep=rep1&type=pdf) Yann LeCun, John Denker, Sara Solla. \"Optimal Brain Damage.\"\n",
    "\n",
    "[3](https://arxiv.org/abs/2102.00554) Hoefler, Torsten, et al. \"Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks.\"\n",
    "\n",
    "[4](https://arxiv.org/abs/1803.03635) Frankle, Jonathan, and Michael Carbin. \"The lottery ticket hypothesis: Finding sparse, trainable neural networks.\"\n",
    "\n",
    "[5](https://arxiv.org/abs/2003.02389) Renda, Alex, Jonathan Frankle, and Michael Carbin. \"Comparing rewinding and fine-tuning in neural network pruning.\"\n",
    "\n",
    "[6](https://medvet.inginf.units.it/publications/2021-c-zmpa-speeding/) Zullich, Marco, Eric Medvet, Felice Andrea Pellegrino, and Ansuini, Alessio. \"Speeding-up pruning for Artificial Neural Networks: Introducing Accelerated Iterative Magnitude Pruning.\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}