{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap from previous Lab\n",
    "\n",
    "* We \"closed the loop\" on our first MultiLayer Perceptron (MLP), exploring how the training routine is implemented in PyTorch (PT):\n",
    "\n",
    "    * we saw how to use built-in loss functions in PT and we learned how to construct custom losses based upon tensor methods\n",
    "    * moreover, we also saw how to use vanilla Stochastic Gradient Descent (SGD) in conjunction with backpropagation to enable the parameters updating in our MLP\n",
    "\n",
    "### Agenda for today\n",
    "\n",
    "* The main topic of our lecture is **regularization**\n",
    "* First of all, though, we will implement a framework for monitoring the parameters during training (the so called *trajectory*), a simple research exercize\n",
    "* On to regularization, we will see how to utilize various way to infuse regularization into our MLP training, still with an eye on the trajectory:\n",
    "\n",
    "  * L2 regularization (aka *weight decay*)\n",
    "  * dropout\n",
    "  * regularization layers\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining parameters mid-training â€“ the trajectory\n",
    "\n",
    "We already covered how to save the \"snapshot\" of the parameters via the `state_dict` during the previous Lab.\n",
    "We can use the same method to recover the trajectory during our training, although a more useful alternative is `model.named_parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from scripts import mnist\n",
    "from scripts.train_utils import accuracy, AverageMeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us quickly recover the stuff we implemented during Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flat = nn.Flatten()\n",
    "        self.h1 = nn.Linear(28*28, 16)\n",
    "        self.h2 = nn.Linear(16, 32)\n",
    "        self.h3 = nn.Linear(32, 24)\n",
    "        self.out = nn.Linear(24, 10)\n",
    "    \n",
    "    def forward(self, X, activ_hidden=nn.functional.relu):\n",
    "        out = self.flat(X)\n",
    "        out = activ_hidden(self.h1(out))\n",
    "        out = activ_hidden(self.h2(out))\n",
    "        out = activ_hidden(self.h3(out))\n",
    "        out = self.out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what `model.named_parameters()` is and how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, \"\\t\", param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add a small piece of code to our `train_epoch` from last Lab to implement the tracking of the trajectory: more specifically, we're interested in the L2-norm of the parameters during each training iteration.\n",
    "\n",
    "NB:\n",
    "* Each **epoch** is composed of **training iterations**:\n",
    "  * a training iteration coincides with the forward/backward pass on a single batch\n",
    "  * an epoch is completed when the whole of the dataset has been seen from the network during training. After an epoch has ended, we reshuffle the batches and begin a new epoch\n",
    "\n",
    "As we can see above, parameters are stored in a (lazy) list of tuples. If we want to calculate the norm, we can't do it on such structure.\n",
    "What we need to do is:\n",
    "* \"flatten\" all the list in a single tensor (a vector)\n",
    "  * first, we must create this tensor, and to do so we have to know the number of parameters that we will need to fit into it\n",
    "* calculate the norm of this vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the norm of the parameters, we're interested in the **norm of the gradients**.\n",
    "\n",
    "Gradients of a layer may be accessed via `tensor.grad` where `tensor` is the tensor associated to the parameters of a given layer.\n",
    "\n",
    "Let's try to call it now on our `named_parameters`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, \"\\t\", param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all the gradients are `None`\n",
    "\n",
    "**Q**: who knows why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_and_gradients_norm(named_parameters):\n",
    "    square_norms_params = []\n",
    "    square_norms_grads = []\n",
    "\n",
    "    for _, param in named_parameters:\n",
    "\n",
    "        # Q: what is this and why did I write it here?\n",
    "        if param.requires_grad:\n",
    "            square_norms_params.append((param ** 2).sum())\n",
    "            square_norms_grads.append((param.grad ** 2).sum())\n",
    "    \n",
    "    norm_params = sum(square_norms_params).sqrt().item()\n",
    "    norm_grads = sum(square_norms_grads).sqrt().item()\n",
    "\n",
    "    return norm_params, norm_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, trajectory, device): # note: I've added a generic performance to replace accuracy and the device\n",
    "    for X, y in dataloader:\n",
    "        # TRANSFER X AND y TO GPU IF SPECIFIED\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # ... like last time\n",
    "        optimizer.zero_grad() \n",
    "        y_hat = model(X)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = performance(y_hat, y)\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])\n",
    "\n",
    "        if trajectory is not None:\n",
    "\n",
    "            params_norm, gradients_norm = get_params_and_gradients_norm(model.named_parameters())\n",
    "            trajectory[\"parameters\"].append(params_norm)\n",
    "            trajectory[\"gradients\"].append(gradients_norm)\n",
    "\n",
    "def train_model(model, dataloader, loss_fn, optimizer, num_epochs, checkpoint_loc=None, checkpoint_name=\"checkpoint.pt\", performance=accuracy, trajectory=None, device=None): # note: I've added a generic performance to replace accuracy and an object where to store the trajectory and the device on which to run our training\n",
    "\n",
    "    # establish device\n",
    "    if device is None:\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Training on {device}\")\n",
    "\n",
    "    # create the folder for the checkpoints (if it's not None)\n",
    "    if checkpoint_loc is not None:\n",
    "        os.makedirs(checkpoint_loc, exist_ok=True)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # epoch loop\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "        performance_meter = AverageMeter()\n",
    "\n",
    "        train_epoch(model, dataloader, loss_fn, optimizer, loss_meter, performance_meter, performance, trajectory, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum} - average: {loss_meter.avg}; Performance: {performance_meter.avg}\")\n",
    "\n",
    "        # produce checkpoint dictionary -- but only if the name and folder of the checkpoint are not None\n",
    "        if checkpoint_name is not None and checkpoint_loc is not None:\n",
    "            checkpoint_dict = {\n",
    "                \"parameters\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(checkpoint_dict, os.path.join(checkpoint_loc, checkpoint_name))\n",
    "\n",
    "    return loss_meter.sum, performance_meter.avg, trajectory\n",
    "\n",
    "def test_model(model, dataloader, performance=accuracy, loss_fn=None, device=None):\n",
    "    # establish device\n",
    "    if device is None:\n",
    "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # create an AverageMeter for the loss if passed\n",
    "    if loss_fn is not None:\n",
    "        loss_meter = AverageMeter()\n",
    "    \n",
    "    performance_meter = AverageMeter()\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_hat = model(X)\n",
    "            loss = loss_fn(y_hat, y) if loss_fn is not None else None\n",
    "            acc = performance(y_hat, y)\n",
    "            if loss_fn is not None:\n",
    "                loss_meter.update(loss.item(), X.shape[0])\n",
    "            performance_meter.update(acc, X.shape[0])\n",
    "    # get final performances\n",
    "    fin_loss = loss_meter.sum if loss_fn is not None else None\n",
    "    fin_perf = performance_meter.avg\n",
    "    print(f\"TESTING - loss {fin_loss if fin_loss is not None else '--'} - performance {fin_perf}\")\n",
    "    return fin_loss, fin_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size_train = 256\n",
    "minibatch_size_test = 512\n",
    "\n",
    "trainloader, testloader, trainset, testset = mnist.get_data(batch_size_train=minibatch_size_test, batch_size_test=minibatch_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.1\n",
    "num_epochs = 5\n",
    "\n",
    "model = MLP()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate)\n",
    "\n",
    "trajectory = {\"parameters\": [], \"gradients\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, trajectory = train_model(model, trainloader, loss_fn, optimizer, num_epochs, trajectory=trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(trajectory, ylim=(0,9)):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, sharey=True)\n",
    "    ax1.set_ylim(*ylim)\n",
    "    ax1.plot(trajectory[\"parameters\"])\n",
    "    ax1.set_title(\"Norm of parameters\")\n",
    "    ax2.plot(trajectory[\"gradients\"])\n",
    "    ax2.set_title(\"Norm of gradients\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a similar trend to what [3](https://arxiv.org/abs/2002.10365) observed:\n",
    "\n",
    "1. in the very first iterations, gradients are very large and parameters quickly grow\n",
    "2. gradients quickly diminishes, while parameters growth tends to slow down\n",
    "3. gradients reach a \"stationary state\" (with some noise added), while parameters grow even slower\n",
    "\n",
    "5 epochs of training is very little. If you want (_not a homework_), you may try increasing the number of epochs and see what happens.\n",
    "\n",
    "of course, the analysis in [3](https://arxiv.org/abs/2002.10365) is much finer than ours and considers much more quantities and experiments than ours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Regularization in DL comes in the form of different tools. We can have:\n",
    "\n",
    "1. Penalty terms in loss functions (e.g. L1 and L2 norm regularization) which introduce bias in our parameters by actively reducing the magnitude of some weights:\n",
    "    * L1 norm regularization is also called LASSO regularization\n",
    "    * L2 norm regularization is also called Ridge regularization or **weight decay**\n",
    "    * they were originally implemented in linear regression models as a way to infuse *inductive bias* in models originally thought to rely on the complete unbiasedness on training data\n",
    "2. Normalization layers which normalize the incoming information s.t. their mean is zero and standard deviation one. It comes in different flavors:\n",
    "    * batch normalization or batchnorm (the most common technique)\n",
    "    * group normalization or groupnorm\n",
    "    * there are more possibilities, for additional info on these, please check [this lecture by Aaron Defazio, NYU](https://atcold.github.io/pytorch-Deep-Learning/en/week05/05-2/)\n",
    "3. Dropout, a technique [patented by Google](https://patents.google.com/patent/US9406017B2/en) which consists in randomly *dropping* some neurons from a given layer to prevent overfitting.\n",
    "4. Early stopping, which we'll see later on during this Lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight decay or L2 norm/Ridge regularization\n",
    "\n",
    "Weight Decay (WD) is a simple technique which *appends* a penalty term to the loss function equation. The term is based upon the L2 norm of the weights.\n",
    "\n",
    "Given our original loss function $\\mathcal{L}_0 (\\hat{y}, y)$ and our parameter vector $\\Theta$, our new loss will be:\n",
    "\n",
    "$\\mathcal{L}_0 (\\hat{y}, y) + \\lambda \\cdot \\vert\\vert \\Theta \\vert\\vert_2$\n",
    "\n",
    "the parameter $\\lambda$ (also called weight decay) controls the strenght of the regularization. $\\lambda$ too high means that the model will not concentrate well enough on the original objective ($\\mathcal{L}_0$), hence it will not perform well. Usually, good values form $\\lambda$ fall into the interval $[5\\cdot 10^{-4}, 1\\cdot 10^{-4}]$.\n",
    "\n",
    "In PT, instead of inserting our penalty term in the loss function, we specify the weight decay parameter in our optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 5e-2\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 norm regularization\n",
    "\n",
    "L1 norm regularization is analogous to weight decay. The equation is:\n",
    "\n",
    "$\\mathcal{L}_0 (\\hat{y}, y) + \\lambda \\cdot \\vert\\vert \\Theta \\vert\\vert_1$\n",
    "\n",
    "where $\\vert\\vert x \\vert\\vert_1 = \\sum_{j=1}^d \\vert x_j \\vert$\n",
    "\n",
    "unlike weight decay, to my knowledge PT does not provide a built-in for L1 reg. You need to define a custom loss function for this task (**homework**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batchnorm\n",
    "\n",
    "Batch Normalization is not really a regularization technique. It operates in such a way that the mean and standard deviation of the incoming batches of data are approximately 0 and 1 respectively.\n",
    "\n",
    "The ultimate goal of batchnorm is not to normalize each batch, but estimate one vector for mean (a running mean) and one for std (a running std) for the whole dataset and to normalize w.r.t. these. So, these become new parameters of the network. They are not adjusted via backprop but they get adjusted each time the layer *sees* another batch of data.\n",
    "\n",
    "![](https://miro.medium.com/max/474/1*QQ2Q5rVBtLv7b3yGhO0flg.png)\n",
    "\n",
    "*picture from [towardsdatascience.com](https://towardsdatascience.com/batch-normalisation-explained-5f4bd9de5feb)*\n",
    "\n",
    "When the network is evaluated on test data, the running mean and std must not be adjusted, hence PT has implemented a \"switch\", which we saw during the previous Lab, to tell the network when to adjust and not adjust these two parameters. The switch is triggered via `model.train()` and `model.eval()` (or equivalently `model.train(False)`).\n",
    "\n",
    "In PT, the batch normalization is found as a regular layer under within the `torch.nn` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_BN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=16), # we specify the dimensionality of the incoming data\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=32),\n",
    "            nn.Linear(32, 24),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=24),\n",
    "            nn.Linear(24, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** (for the most skilled students): why didn't we apply batchnorm for the first layer?\n",
    "\n",
    "By peeking at the PT docs, we can see that actually the batchnorm layers have much more hyperparameters which we can play with if we wanted to:\n",
    "\n",
    "![](img/bn_docs.jpg)\n",
    "\n",
    "*from [PyTorch docs](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)*\n",
    "\n",
    "In addition to what we say till now, there is still some debate in the DL community on whether batchnorm or other normalization techniques help optimization. The claims in the original paper [1](https://arxiv.org/abs/1502.03167) of \"reducing internal covariate shift\" was confuted in successive works such as [2](https://arxiv.org/abs/1805.11604.pdf), which claims that it \"makes the optimization landscape significantly smoother\". Another things to consider is that, since the data is distributed in a small intervall around 0, there's also a better numerical stability added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n",
    "\n",
    "Dropout acts by removing (i.e. *zeroing-out*) a random subset of the neurons in a given layer for each forward pass.\n",
    "\n",
    "It has one hyperparameter ($p$), which is the fraction of neurons to be dropped out.\n",
    "\n",
    "During training, each time a layer with backprop produces an output, a fraction $p$ of that output gets discarded. This helps in such a way that co-dependence between neurons gets *forgotten* by the network. To say it in simple terms, it forces each neuron to be independent from the output of other neurons within the same layer.\n",
    "\n",
    "For the same reason as in batchnorm, since dropout has to apply only during training, we must be careful in activating the switch `model.eval()` when testing our network.\n",
    "\n",
    "In PT, we find Dropout as a module of `torch.nn`. Instead of placing if *before* the layer (as in batchnorm), we place it *after* the layer (the reason being, the layer produces an output, a portion $p$ of that output gets discarded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_BN_Drop(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=16),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=.2), # we add a dropout here. it's referred to the previous layer (with 32 neurons)\n",
    "\n",
    "            nn.BatchNorm1d(num_features=32),\n",
    "            nn.Linear(32, 24),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(num_features=24),\n",
    "            nn.Linear(24, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "\n",
    "trajectory_reg = {\"parameters\": [], \"gradients\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc, trajectory_reg = train_model(model, trainloader, loss_fn, optimizer, num_epochs, trajectory=trajectory_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectory(trajectory_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HOMEWORK - Early stopping\n",
    "\n",
    "Early stopping is yet another example of regularization technique which relies a lot on practical and experimental observations rather than any supporting theory.\n",
    "\n",
    "It is based upon the concept of **validation**, which is an assessment mode additional to *testing*. Actually, what insofar whe have described as testing is technically a validation.\n",
    "* a validation dataset may be obtained as result of a random splitting of the original training dataset\n",
    "* a testing dataset should be obtained instead from a model deployed \"in the wild\" and should consist of data unseen (from both the model and its architect) during the training and designing phase.\n",
    "\n",
    "In a normal academic setting it's very hard to obtain a proper training dataset, so usually the meaning of testing and validation get mixed up a little bit.\n",
    "\n",
    "Anyway, early stopping requires us to assess the model at each epoch to get a proxy for the testing performance(s) (**validation step**). That should gives us an idea of how the model **learns to generalize** (if it ever does...) during training.\n",
    "\n",
    "The *theoretical trend* ('90 s), which is pretty much absent in modern Deep Learning due to a lot of modern factors, is the following (figures from [4](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf)):\n",
    "\n",
    "![](img/generalization.jpg)\n",
    "\n",
    "Already in that period, different stuff was observed:\n",
    "\n",
    "![](img/generalization_ugly.jpg)\n",
    "\n",
    "In some of my experiments, this happens (blue=training, orange=validation):\n",
    "\n",
    "![](img/generalization_fmnist1.jpg)\n",
    "\n",
    "![](img/generalization_fmnist2.jpg)\n",
    "\n",
    "(red=training, blue=validation)\n",
    "\n",
    "![](img/plot_acc_09.png)\n",
    "\n",
    "As we observe the curves for training and validation performance, we may notice some trends:\n",
    "* there usually is an intersection between the two curves which marks the moment when the network starts to **overfit** the training data.\n",
    "    * it might happen that, after that moment, the validation performance stays roughly the same (_white noise_), or that it drops and never recovers\n",
    "    * it might happen, instead, that the validation performance stays a few points below the training performance but keeps on growing\n",
    "    * it might happen, eventually, that the validation performance peaks a few epochs after and then it decreases\n",
    "    * other situations may apply depending on the dataset, the optimizer, the presence of regularization, and a lot of other factors.\n",
    "    \n",
    "A trick which is very often applied is to track the validation performance during training and retain the model with the highest validation performance.\n",
    "**Note**: it may not be the best strategy as the validation dataset may not be representative of the data manifold (!).\n",
    "In the main reference for early stopping ([4](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf)), it is indicated as $E_{\\text{opt}}$.\n",
    "\n",
    "**Homework**: implement \"early stopping\" in the $E_{\\text{opt}}$ using the test data as validation (since we don't know yet how to create additional `DataLoaders` and operate random splitting).\n",
    "\n",
    "**Homework for the bravest ones**: read [4](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf) and try implementing at least one of the techniques there specified (besides $E_{\\text{opt}}$, of course). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "[1](https://arxiv.org/abs/1502.03167) Ioffe, S., & Szegedy, C. (2015, June). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning (pp. 448-456). PMLR.\n",
    "\n",
    "[2](https://arxiv.org/abs/1805.11604) Santurkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). How does batch normalization help optimization?. arXiv preprint arXiv:1805.11604.\n",
    "\n",
    "[3](https://arxiv.org/abs/2002.10365) Frankle, J., Schwab, D. J., & Morcos, A. S. (2020). The early phase of neural network training. arXiv preprint arXiv:2002.10365.\n",
    "\n",
    "[4](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf) Prechelt, L. (1998). Early stopping-but when?. In Neural Networks: Tricks of the trade (pp. 55-69). Springer, Berlin, Heidelberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homework recap\n",
    "\n",
    "1. Implement L1 norm regularization as a custom loss function\n",
    "2. Implement early stopping in the $E_{\\text{opt}}$ specification\n",
    "3. Implement early stopping in one of the additional specifications as of [4](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}