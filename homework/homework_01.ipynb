{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Deep Learning Homework \\#01\n", "### Deep Learning Course $\\in$ DSSC @ UniTS (Spring 2021)  \n", "\n", "#### Submitted by [Emanuele Ballarin](mailto:emanuele@ballarin.cc)  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Request 1:\n", "\n", "Taking inspiration from the notebook [`01-intro-to-pt.ipynb`](https://github.com/ansuini/DSSC_DL_2021/blob/main/labs/01-intro-to-pt.ipynb), build a class for the Multilayer Perceptron (MLP) whose scheme is drawn in the last figure of the notebook. As written there, no layer should have bias units and the activation for each hidden layer should be the Rectified Linear Unit (ReLU) function, also called ramp function. The activation leading to the output layer, instead, should be the softmax function, which prof. Ansuini explained during the last lecture. You can find some notions on it also on the notebook."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Preliminaries:\n", "\n", "Just to set things clear, we obtain specifications of the desired model from the scheme drawn in the [notebook](https://github.com/ansuini/DSSC_DL_2021/blob/main/labs/01-intro-to-pt.ipynb) (via direct counting) and its accompaining text, and summarize them below.\n", "\n", "The desired model should:\n", "- Be a *MultiLayer Perceptron* (*MLP*, a.k.a. *Fully-Connected*, a.k.a. *Dense* block);\n", "- Be composed of *biasless* units;\n", "- Take as input $5$ scalars;\n", "- Return as output $4$ scalars;\n", "- Have *hidden layers* or sizes (in *input-to-output* order): $11$, $16$, $13$, $8$;\n", "- Have *ReLU* *activation function* for *hidden layers*;\n", "- Have the *SoftMax* function as *output function*."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### The imports *of the day*:"]}, {"cell_type": "code", "execution_count": 1, "metadata": {"execution": {"iopub.execute_input": "2021-03-17T19:51:00.262962Z", "iopub.status.busy": "2021-03-17T19:51:00.262634Z", "iopub.status.idle": "2021-03-17T19:51:00.856584Z", "shell.execute_reply": "2021-03-17T19:51:00.855919Z", "shell.execute_reply.started": "2021-03-17T19:51:00.262889Z"}, "tags": []}, "outputs": [], "source": ["# The usual stuff\n", "import numpy as np  # Just to force-load MKL (if available)\n", "import torch as th\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"execution": {"iopub.execute_input": "2021-03-17T19:51:00.857784Z", "iopub.status.busy": "2021-03-17T19:51:00.857503Z", "iopub.status.idle": "2021-03-17T19:51:00.864230Z", "shell.execute_reply": "2021-03-17T19:51:00.863283Z", "shell.execute_reply.started": "2021-03-17T19:51:00.857753Z"}}, "outputs": [], "source": ["# The extra stuff (I am not forcing you to do it; uncomment if willing!)\n", "\n", "#!pip install --upgrade --no-deps --force --force-reinstall git+https://github.com/TylerYep/torchinfo.git\n", "import torchinfo as thinfo\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"execution": {"iopub.execute_input": "2021-03-17T19:51:00.865673Z", "iopub.status.busy": "2021-03-17T19:51:00.865379Z", "iopub.status.idle": "2021-03-17T19:51:00.962914Z", "shell.execute_reply": "2021-03-17T19:51:00.961514Z", "shell.execute_reply.started": "2021-03-17T19:51:00.865641Z"}, "tags": []}, "outputs": [], "source": ["# The crazy stuff (I am not forcing you to do it; uncomment if willing!)\n", "\n", "#!pip install --upgrade --no-deps --force --force-reinstall git+https://github.com/emaballarin/ebtorch.git\n", "from ebtorch.nn import FCBlock\n", "\n", "# No problem if it fails: it just won't run the crazy stuff... :)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### The *standard* solution:"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"execution": {"iopub.execute_input": "2021-03-17T19:51:00.964216Z", "iopub.status.busy": "2021-03-17T19:51:00.963968Z", "iopub.status.idle": "2021-03-17T19:51:01.027585Z", "shell.execute_reply": "2021-03-17T19:51:01.026653Z", "shell.execute_reply.started": "2021-03-17T19:51:00.964191Z"}, "tags": []}, "outputs": [], "source": ["# Define:\n", "class myMLP(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        # Layers:\n", "        self.layer1 = nn.Linear(in_features=5, out_features=11, bias=False)\n", "        self.layer2 = nn.Linear(in_features=11, out_features=16, bias=False)\n", "        self.layer3 = nn.Linear(in_features=16, out_features=13, bias=False)\n", "        self.layer4 = nn.Linear(in_features=13, out_features=8, bias=False)\n", "        self.layer5 = nn.Linear(in_features=8, out_features=4, bias=False)\n", "        # Stateful functions:\n", "        # Since ReLU and SoftMax are stateless, no cruft here!\n", "\n", "    def forward(self, x):\n", "        # A \"layer with elementwise nonlinearity\" block\n", "        # <- from here...\n", "        x = self.layer1(x)\n", "        x = F.relu(x)\n", "        # <- ...to here.\n", "        x = self.layer2(x)\n", "        x = F.relu(x)\n", "        x = self.layer3(x)\n", "        x = F.relu(x)\n", "        x = self.layer4(x)\n", "        x = F.relu(x)\n", "        # The \"pre-output layer\": a linear layer with SoftMax afterwards\n", "        x = self.layer5(x)\n", "        x = F.softmax(x, dim=1)\n", "        return x\n", "\n", "\n", "# Instantiate:\n", "mymodel_class = myMLP()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### The *not a class, but I am lazy* solution (a.k.a. `nn.Sequential`)"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"execution": {"iopub.execute_input": "2021-03-17T19:51:01.029432Z", "iopub.status.busy": "2021-03-17T19:51:01.029055Z", "iopub.status.idle": "2021-03-17T19:51:01.131216Z", "shell.execute_reply": "2021-03-17T19:51:01.130304Z", "shell.execute_reply.started": "2021-03-17T19:51:01.029394Z"}}, "outputs": [], "source": ["# Define and Instantiate:\n", "mymodel_seq = nn.Sequential(\n", "    nn.Linear(in_features=5, out_features=11, bias=False),  # H1\n", "    nn.ReLU(),\n", "    nn.Linear(in_features=11, out_features=16, bias=False),  # H2\n", "    nn.ReLU(),\n", "    nn.Linear(in_features=16, out_features=13, bias=False),  # H3\n", "    nn.ReLU(),\n", "    nn.Linear(in_features=13, out_features=8, bias=False),  # H4\n", "    nn.ReLU(),\n", "    nn.Linear(in_features=8, out_features=4, bias=False),  # Pre-output\n", "    nn.Softmax(dim=1),\n", ")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### The *crazy* solution (a.k.a. *parameterized FC block*)"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"execution": {"iopub.execute_input": "2021-03-17T19:51:01.132835Z", "iopub.status.busy": "2021-03-17T19:51:01.132469Z", "iopub.status.idle": "2021-03-17T19:51:01.216397Z", "shell.execute_reply": "2021-03-17T19:51:01.215772Z", "shell.execute_reply.started": "2021-03-17T19:51:01.132799Z"}, "tags": []}, "outputs": [], "source": ["# Define and Instantiate (with a proper class under the hood):\n", "# (cfr.: https://github.com/emaballarin/ebtorch/blob/main/ebtorch/nn/architectures.py#L32)\n", "mymodel_fpfcb = FCBlock(\n", "    fin=5,\n", "    hsizes=[11, 16, 13, 8],\n", "    fout=4,\n", "    hactiv=F.relu,\n", "    oactiv=lambda x: F.softmax(x, dim=1),\n", "    bias=False,\n", ")\n", "\n", "# WLOG, the instantiation call also automatically supports the use of lists for `hactiv` and `bias`\n", "# (with integrated size-checking) to enable per-layer specifications\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Request 2:\n", "\n", "After having defined the class, create an instance of it and print a summary using a method of your choice."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Solution:\n", "\n", "Classes have been instantiated right after their definition (or, as in some methods of proposed solution, just directly)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["As far as *model summarization* is concerned, we will use [`torchinfo`](https://github.com/TylerYep/torchinfo), the *maintained*, *properly-coded* (was) fork of [`torchsummary`](https://github.com/sksq96/pytorch-summary)."]}, {"cell_type": "code", "execution_count": 7, "metadata": {"execution": {"iopub.execute_input": "2021-03-17T19:51:01.218745Z", "iopub.status.busy": "2021-03-17T19:51:01.218490Z", "iopub.status.idle": "2021-03-17T19:51:01.356317Z", "shell.execute_reply": "2021-03-17T19:51:01.355791Z", "shell.execute_reply.started": "2021-03-17T19:51:01.218718Z"}, "tags": []}, "outputs": [{"data": {"text/plain": ["=================================================================\n", "Layer (type:depth-idx)                   Param #\n", "=================================================================\n", "\u251c\u2500Linear: 1-1                            55\n", "\u251c\u2500Linear: 1-2                            176\n", "\u251c\u2500Linear: 1-3                            208\n", "\u251c\u2500Linear: 1-4                            104\n", "\u251c\u2500Linear: 1-5                            32\n", "=================================================================\n", "Total params: 575\n", "Trainable params: 575\n", "Non-trainable params: 0\n", "================================================================="]}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": ["thinfo.summary(mymodel_class)\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"execution": {"iopub.execute_input": "2021-03-17T19:51:01.357392Z", "iopub.status.busy": "2021-03-17T19:51:01.357200Z", "iopub.status.idle": "2021-03-17T19:51:01.429063Z", "shell.execute_reply": "2021-03-17T19:51:01.427871Z", "shell.execute_reply.started": "2021-03-17T19:51:01.357373Z"}, "tags": []}, "outputs": [{"data": {"text/plain": ["=================================================================\n", "Layer (type:depth-idx)                   Param #\n", "=================================================================\n", "\u251c\u2500Linear: 1-1                            55\n", "\u251c\u2500ReLU: 1-2                              --\n", "\u251c\u2500Linear: 1-3                            176\n", "\u251c\u2500ReLU: 1-4                              --\n", "\u251c\u2500Linear: 1-5                            208\n", "\u251c\u2500ReLU: 1-6                              --\n", "\u251c\u2500Linear: 1-7                            104\n", "\u251c\u2500ReLU: 1-8                              --\n", "\u251c\u2500Linear: 1-9                            32\n", "\u251c\u2500Softmax: 1-10                          --\n", "=================================================================\n", "Total params: 575\n", "Trainable params: 575\n", "Non-trainable params: 0\n", "================================================================="]}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": ["thinfo.summary(mymodel_seq)\n"]}, {"cell_type": "code", "execution_count": 9, "metadata": {"execution": {"iopub.execute_input": "2021-03-17T19:51:01.430830Z", "iopub.status.busy": "2021-03-17T19:51:01.430564Z", "iopub.status.idle": "2021-03-17T19:51:01.518995Z", "shell.execute_reply": "2021-03-17T19:51:01.518073Z", "shell.execute_reply.started": "2021-03-17T19:51:01.430803Z"}, "tags": []}, "outputs": [{"data": {"text/plain": ["=================================================================\n", "Layer (type:depth-idx)                   Param #\n", "=================================================================\n", "\u251c\u2500ModuleList: 1-1                        --\n", "|    \u2514\u2500Linear: 2-1                       55\n", "|    \u2514\u2500Linear: 2-2                       176\n", "|    \u2514\u2500Linear: 2-3                       208\n", "|    \u2514\u2500Linear: 2-4                       104\n", "|    \u2514\u2500Linear: 2-5                       32\n", "=================================================================\n", "Total params: 575\n", "Trainable params: 575\n", "Non-trainable params: 0\n", "================================================================="]}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": ["thinfo.summary(mymodel_fpfcb)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### A brief comment:\n", "\n", "As we can see from the output of `torchinfo`, all the three *instantiated models* exhibit the same learnable structures. Exact differences in output can be explained as follows:\n", "\n", "- The summary of the *class-instantiated* model just lists *`Linear` layers*, since they are the only portion of the model defined as *class members*. The nonlinear transformations are obtained as *pure function* calls in the `forward(x)`;\n", "\n", "- The summary of the `Sequential` model lists both *`Linear` layers* and *activation functions*, since - by definition - it uses *function-objects* to implement nonlinearities;\n", "\n", "- As far as the summary of the model obtained via `ebtorch.nn.FCBlock` is concerned, the same as in the case of the *class-instantiated* model applies. Additionally, all *stateful* class members (e.g. *`Linear` layers*) are wrapped inside a `ModuleList`, whose elements are created programmatically from *call-arguments*."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Request 3:\n", "\n", "- Provide detailed calculations (layer-by-layer) on the exact number of parameters in the network.\n", "- Provide the same calculation in the case that the bias units are present in all layers (except input)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Preliminaries:\n", "\n", "In order to concisely compute the number of learnable parameters *layer-by-layer* for our specific model, we need to note first that:\n", "\n", "- In the case of scalar-neurons *FC* layers, the number of learnable parameters is the number of scalars parameterizing the affine transformation of the output vector from the previous layer (i.e. that vector whose coordinates are the outputs of single scalar-neurons in the previous layer);\n", "\n", "- Such affine transformation is parameterized by an $n \\times n'$ matrix ($\\mathbf{W}$) and an $n'$-dimensional vector ($\\mathbf{b}$), with $n$ and $n'$ respectively the layer-specific *input* and *desired output* sizes. Just to put it in another way: $n'$ is the number of units of the layer, whereas $n$ is the number of units of the previous (again, in the scalar-neurons *FC* layer case);\n", "\n", "- Of course, if the *layer* to be considered is *biasless*, the vector is fixed as $\\mathbf{b} = \\mathbf{0}$, and only the matrix $\\mathbf{W}$ accounts for the number of learnable parameters. In the case of *layers with bias*, instead, the $\\mathbf{b}$ vector has to be considered too;\n", "\n", "- Risking overzealousness, we explicitly acknowledge that - as in our specific case:\n", "    - neither the *ReLU* nor the *SoftMax* activation functions carry learnable parameters, as it is the case e.g. of [PReLU](https://arxiv.org/abs/1502.01852)s instead;\n", "    - the use of the term *scalar neuron* explicitly dispels ambiguity with recently proposed *ANN*s with [complex-valued](https://arxiv.org/abs/2101.12249) or even [quaternion-valued](https://sci-hub.do/10.1007/s10462-019-09752-1) neurons."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Abstract solution:\n", "\n", "This directly leads to the number of *per-layer* learnable parameters being:\n", "- $n \\times n'$ (*biasless* case)\n", "- $(n + 1) \\times n'$ (*biased* case)\n", "\n", "with the sole exception of the *input layer* (i.e. the one composed by *input units*), which carry no learnable parameters by definition.\n", "\n", "It also immediately follows that the whole-network number of *learnable parameters* is:\n", "- $\\sum_{i=1}^K {{n'}_{i-1} \\times {n'}_i}$ (*biasless* case)\n", "- $\\sum_{i=1}^K {({n'}_{i-1}+1) \\times {n'}_i}$ (*biased* case)\n", "\n", "where, as always, ${n'}_j$ denotes the number of units in the $j^{\\text{th}}$ layer (being the $0^{\\text{th}}$ the *input* and the $K^{\\text{th}}$ the *output* layers).\n"]}, {"cell_type": "markdown", "metadata": {"execution": {"iopub.execute_input": "2021-03-17T18:15:42.174938Z", "iopub.status.busy": "2021-03-17T18:15:42.174685Z", "iopub.status.idle": "2021-03-17T18:15:42.183589Z", "shell.execute_reply": "2021-03-17T18:15:42.182603Z", "shell.execute_reply.started": "2021-03-17T18:15:42.174916Z"}, "tags": []}, "source": ["#### Putting the numbers in:\n", "\n", "Legenda:  \n", "*layer # . parameters in the *biasless* case | parameters in the *biased* case*\n", "\n", "0. $\\text{        }$ None $\\text{        }$ $|$ $\\text{        }$ None $\\text{        }$  (*input layer*)\n", "1. $\\text{        }5 \\times 11 = 55 \\text{        }$ $|$ $\\text{        }(5+1) \\times 11 = 66$\n", "2. $\\text{        }11 \\times 16 = 176 \\text{        }$ $|$ $\\text{        }(11+1) \\times 16 = 192$\n", "3. $\\text{        }16 \\times 13 = 208 \\text{        }$ $|$ $\\text{        }(16+1) \\times 13 = 221$\n", "5. $\\text{        }13 \\times 8 = 104 \\text{        }$ $|$ $\\text{        }(13+1) \\times 8 = 112$\n", "6. $\\text{        }8 \\times 4 = 32 \\text{        }$ $|$ $\\text{        }(8+1) \\times 4 = 36$\n", "\n", "Which finally give the *whole-network* results:\n", "- $55+176+208+104+32 = 575$ (*biasless* case)\n", "- $66+192+221+112+36 = 627$ (*biased* case)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Request 4:\n", "\n", "For each layer within the MLP, calculate the L2 norm and L1 norm of its parameters."]}, {"cell_type": "code", "execution_count": 10, "metadata": {"execution": {"iopub.execute_input": "2021-03-17T19:51:01.520498Z", "iopub.status.busy": "2021-03-17T19:51:01.520106Z", "iopub.status.idle": "2021-03-17T19:51:01.648955Z", "shell.execute_reply": "2021-03-17T19:51:01.648074Z", "shell.execute_reply.started": "2021-03-17T19:51:01.520468Z"}, "tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["MODEL:  myMLP(\n", "  (layer1): Linear(in_features=5, out_features=11, bias=False)\n", "  (layer2): Linear(in_features=11, out_features=16, bias=False)\n", "  (layer3): Linear(in_features=16, out_features=13, bias=False)\n", "  (layer4): Linear(in_features=13, out_features=8, bias=False)\n", "  (layer5): Linear(in_features=8, out_features=4, bias=False)\n", ") \n", "\n", "Layer  1 : L2 norm (Frobenius):  2.0606470108032227\n", "           L1 norm (E.W. Eqv.):  13.390910148620605\n", "\n", "\n", "Layer  2 : L2 norm (Frobenius):  2.22224497795105\n", "           L1 norm (E.W. Eqv.):  25.169963836669922\n", "\n", "\n", "Layer  3 : L2 norm (Frobenius):  2.1237316131591797\n", "           L1 norm (E.W. Eqv.):  26.496627807617188\n", "\n", "\n", "Layer  4 : L2 norm (Frobenius):  1.6654473543167114\n", "           L1 norm (E.W. Eqv.):  14.795565605163574\n", "\n", "\n", "Layer  5 : L2 norm (Frobenius):  1.061855673789978\n", "           L1 norm (E.W. Eqv.):  4.946002006530762\n", "\n", "\n", "\n", "\n", "\n", "MODEL:  Sequential(\n", "  (0): Linear(in_features=5, out_features=11, bias=False)\n", "  (1): ReLU()\n", "  (2): Linear(in_features=11, out_features=16, bias=False)\n", "  (3): ReLU()\n", "  (4): Linear(in_features=16, out_features=13, bias=False)\n", "  (5): ReLU()\n", "  (6): Linear(in_features=13, out_features=8, bias=False)\n", "  (7): ReLU()\n", "  (8): Linear(in_features=8, out_features=4, bias=False)\n", "  (9): Softmax(dim=1)\n", ") \n", "\n", "Layer  1 : L2 norm (Frobenius):  2.000171661376953\n", "           L1 norm (E.W. Eqv.):  13.264612197875977\n", "\n", "\n", "Layer  2 : L2 norm (Frobenius):  2.33101487159729\n", "           L1 norm (E.W. Eqv.):  26.669950485229492\n", "\n", "\n", "Layer  3 : L2 norm (Frobenius):  2.086151123046875\n", "           L1 norm (E.W. Eqv.):  25.90290641784668\n", "\n", "\n", "Layer  4 : L2 norm (Frobenius):  1.5620944499969482\n", "           L1 norm (E.W. Eqv.):  13.450577735900879\n", "\n", "\n", "Layer  5 : L2 norm (Frobenius):  1.0676379203796387\n", "           L1 norm (E.W. Eqv.):  5.4537882804870605\n", "\n", "\n", "\n", "\n", "\n", "MODEL:  FCBlock(\n", "  (linears): ModuleList(\n", "    (0): Linear(in_features=5, out_features=11, bias=False)\n", "    (1): Linear(in_features=11, out_features=16, bias=False)\n", "    (2): Linear(in_features=16, out_features=13, bias=False)\n", "    (3): Linear(in_features=13, out_features=8, bias=False)\n", "    (4): Linear(in_features=8, out_features=4, bias=False)\n", "  )\n", ") \n", "\n", "Layer  1 : L2 norm (Frobenius):  1.8526719808578491\n", "           L1 norm (E.W. Eqv.):  11.377017974853516\n", "\n", "\n", "Layer  2 : L2 norm (Frobenius):  2.3248534202575684\n", "           L1 norm (E.W. Eqv.):  26.8232364654541\n", "\n", "\n", "Layer  3 : L2 norm (Frobenius):  2.0438599586486816\n", "           L1 norm (E.W. Eqv.):  26.046585083007812\n", "\n", "\n", "Layer  4 : L2 norm (Frobenius):  1.5632193088531494\n", "           L1 norm (E.W. Eqv.):  13.634401321411133\n", "\n", "\n", "Layer  5 : L2 norm (Frobenius):  1.4122995138168335\n", "           L1 norm (E.W. Eqv.):  7.410478591918945\n", "\n", "\n", "\n", "\n", "\n"]}], "source": ["with th.no_grad():\n", "\n", "    for model in [mymodel_class, mymodel_seq, mymodel_fpfcb]:\n", "        print(\"MODEL: \", model, \"\\n\")\n", "        for lay_n, lay_params in enumerate(model.parameters()):\n", "            print(\n", "                \"Layer \",\n", "                lay_n + 1,\n", "                \": L2 norm (Frobenius): \",\n", "                th.linalg.norm(lay_params, ord=\"fro\").item(),\n", "            )\n", "            print(\n", "                \"      \",\n", "                \"  \",\n", "                \" L1 norm (E.W. Eqv.): \",\n", "                th.linalg.norm(lay_params.flatten(), ord=1).item(),\n", "            )\n", "            print(\"\\n\")\n", "        print(\"\\n\\n\")\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3.8.8 64-bit ('RDDL': conda)", "name": "python388jvsc74a57bd04665a7ab5f73937e17a5e61da1e18d3b6b9921930ee46fb6de3bfba5784f1ea7"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.8-final"}}, "nbformat": 4, "nbformat_minor": 5}